Paper number,Title,Abstract,Authors Names,Presenters Name,Type of Presentation,Attendance Type,Room Location,Session,Session Date,Session time,category_primary,category_secondary,keywords
3005-CL,Multimodal OXYmorons: A Comprehensive Introduction and Computational Analysis Using a Dataset of Oxymoronic Memes in Italian and Spanish,"This paper introduces the concept of multimodal oxymorons. Multimodal oxymorons extend the traditional oxymoron theory by constructing and communicating meaning through the interplay of multiple modalities (such as visual and textual) rather than relying solely on language. The paper argues that multimodal oxymorons are central mechanisms of meaning-making in contemporary communication, as evidenced by the use of memes as an example. While textual oxymorons have long been the subject of analysis in order to ascertain their role in shaping thought and meaning, multimodal oxymorons demonstrate how human cognitive process transcends linguistic boundaries, integrating different modalities (e.g., visual) in order to convey complex ideas. To encourage further study, we present a curated multilingual dataset of Multimodal OXYmoron (MOXY), which can be used as a foundation for further analysis and experimentation. Furthermore, we propose a methodical approach for the identification of multimodal oxymorons along with a pipeline for automated generation. Through illustrative examples and a detailed methodology, this work establishes a comprehensive framework for understanding, identifying, and generating multimodal oxymorons, paving the way for advancements in computational linguistics, artificial intelligence, and figurative language studies.","Giulia Rizzi, Francesca Masini, Paolo Rosso, Elisabetta Fersini, Eliana Di Palma",Eliana Di Palma,Oral,In-person,Pavillon  DE RABAT,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multimodal & Speech/Audio,"Linguistics, Syntax & Semantics",multimodal; meaning; modalities; order; analysis; textual; computational; visual; experimentation; evidenced
3052-CL,How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of Target Language Text?,"Large language models (LLMs) have shown remarkable capabilities in many languages beyond English. Yet, LLMs require more inference steps when generating non-English text due to their reliance on English-centric tokenizers and vocabulary, resulting in higher usage costs to non-English speakers. Vocabulary expansion with target language tokens is a widely used cross-lingual vocabulary adaptation approach to remedy this issue. Despite its effectiveness in inference speedup, previous work on vocabulary expansion has focused on high-resource settings assuming access to a substantial amount of target language data to effectively initialize the embeddings of the new tokens and adapt the LLM to the target language. However, vocabulary expansion in low-resource settings has yet to be explored. In this article, we investigate vocabulary expansion in low-resource settings by considering embedding initialization methods and continual pre-training strategies. Through extensive experiments across typologically diverse languages, tasks and models, we establish a set of strategies to perform vocabulary expansion for faster inference, while striving to maintain competitive downstream performance to baselines. This is achieved with only 30K sentences (~0.01GB text data) from the target language.","Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras",Atsuki Yamaguchi,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multilinguality & Low-Resource NLP,"Efficiency, Scaling & NLP Systems",vocabulary; vocabulary expansion; target language; expansion; target; resource settings; english; non english; low resource settings; settings
3081-CL,Training and Evaluating with Human Label Variation: An Empirical Study,"Human label variation (HLV) challenges the standard assumption that a labelled instance has a single ground truth, instead embracing the natural variation in human annotation to train and evaluate models. While various training methods and metrics for HLV have been proposed, it is still unclear which methods and metrics perform best in what settings. We propose new evaluation metrics for HLV leveraging fuzzy set theory. Since these new proposed metrics are differentiable, we then in turn experiment with employing these metrics as training objectives. We conduct an extensive study over 6 HLV datasets testing 14 training methods and 6 evaluation metrics. We find that training on either disaggregated annotations or soft labels performs best across metrics, outperforming training using the proposed training objectives with differentiable metrics. We also show that our proposed soft micro F1 score is one of the best metrics for HLV data.","Kemal Kurniawan, Meladel Mistica, Timothy Baldwin, Jey Han Lau",Kemal Kurniawan,Oral,In-person,SALLE  LA PALMERAIE,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",,hlv; metrics; training; proposed; variation; label variation; human label; human label variation; differentiable; training methods
3155-CL,The Quest for the Right Mediator: Surveying Mechanistic Interpretability for NLP Through the Lens of Causal Mediation Analysis,"Interpretability provides a toolset for understanding how and why language models behave in certain ways. However, there is little unity in the field: most studies employ ad-hoc evaluations and do not share theoretical foundations, making it difficult to measure progress and compare the pros and cons of different techniques. Furthermore, while mechanistic understanding is frequently discussed, the basic causal units underlying these mechanisms are often not explicitly defined. In this article, we propose a perspective on interpretability research grounded in causal mediation analysis. Specifically, we describe the history and current state of interpretability taxonomized according to the types of causal units (mediators) employed, as well as methods used to search over mediators. We discuss the pros and cons of each mediator, providing insights as to when particular kinds of mediators and search methods are most appropriate. We argue that this framing yields a more cohesive narrative of the field and helps researchers select appropriate methods based on their research objective. Our analysis yields actionable recommendations for future work, including the discovery of new mediators and the development of standardized evaluations tailored to these goals.","Aaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David Bau, Yonatan Belinkov",Aaron Mueller,Oral,In-person,SALLE  LE RIAD,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Interpretability & Model Analysis,,mediators; causal; interpretability; causal mediation; mediation; units; mechanistic; field; appropriate; analysis
3233-CL,LLMs and Cultural Values: The Impact of Prompt Language and Explicit Cultural Framing,"Large Language Models (LLMs) are rapidly being adopted by users across the globe, who interact with them in a diverse range of languages. At the same time, there are well-documented imbalances in the training data and optimisation objectives of this technology, raising doubts as to whether LLMs can accurately represent the cultural diversity of their broad user base. In this study, we look at LLMs and cultural values in particular, and examine how prompt language and cultural framing influence model responses and their alignment with human values in different countries.We do so by probing 10 LLMs with 63 items from the Hofstede Values Survey Module and World Values Survey, translated into 11 languages, and formulated as prompts with and without different explicit cultural perspectives.; Our study confirms that both prompt language and cultural perspective produce variation in LLM outputs, but with an important caveat: While targeted prompting can, to a certain extent, steer LLM responses in the direction of the predominant values of the corresponding countries, it does not overcome the modelsâ systematic bias toward the values associated with a restricted set of countries in our dataset: the Netherlands, Germany, the United States, and Japan. All tested models, regardless of their origin, exhibit remarkably similar patterns: They produce fairly neutral responses on most topics, with selective progressive stances on issues such as social tolerance. Alignment with cultural values of human respondents is improved more with an explicit cultural perspective than with a targeted prompt language. Unexpectedly, combining both approaches is no more effective than cultural framing with an English prompt. These findings reveal that LLMs occupy an uncomfortable middle ground: They are responsive enough to changes in prompts to produce variation, but they are also too firmly anchored to specific cultural defaults to adequately represent cultural diversity.","Ayla Rigouts Terryn, Bram Bulte",Ayla Rigouts Terryn,Oral,In-person,Pavillon  DE RABAT,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness",,cultural; values; prompt language; prompt; countries; framing; cultural diversity; values survey; llms cultural; produce
7743-TACL,Modelling analogies and analogical reasoning: connecting cognitive science theory and NLP research.,"Analogical reasoning is an essential aspect of human cognition. In this paper, we summarize key theory about the processes underlying analogical reasoning from the cognitive science literature and relate it to current research in natural language processing. While these processes can be easily linked to concepts in NLP, they are generally not viewed through a cognitive lens. Furthermore, we show how these notions are relevant for several major challenges in NLP research, not directly related to analogy solving. This may guide researchers to better optimize relational understanding in text, as opposed to relying heavily on entity-level similarity.","Molly Petersen, Claire Stevenson, Lonneke van der Plas",Lonneke van der Plas,Oral,In-person,Pavillon  DE RABAT,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Reasoning, Planning & Agents",,analogical; cognitive; nlp research; cognitive science; nlp; processes; research; theory; science; reasoning
8207-TACL,Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings,"Reinforcement learning (RL) has been proven to be an effective and robust method for training neural machine translation systems, especially when paired with powerful reward models that accurately assess translation quality.
However, most research has focused on RL methods that use sentence-level feedback, leading to inefficient learning signals
due to the reward sparsity problem – the model receives a single score for the entire sentence.
To address this, we propose a novel approach that leverages fine-grained, token-level quality assessments along with error severity levels using RL methods. Specifically, we use xCOMET, a state-of-the-art quality estimation system, as our token-level reward model.
We conduct experiments on small and large translation datasets with standard encoder-decoder and large language models-based machine translation systems, comparing the impact of sentence-level versus fine-grained reward signals on translation quality.
Our results show that training with token-level rewards improves translation quality across language pairs over baselines according to both automatic and human evaluation. Furthermore, token-level reward optimization improves training stability, evidenced by a steady increase in mean rewards over training epochs.","Miguel Moura Ramos, Tomás Duarte de Almeida, Daniel Rocha Vareta, Filipe Parrado de Azevedo, Sweta Agrawal, Patrick Santos Fernandes, André Filipe Torres Martins",Miguel Moura Ramos,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Machine Translation,"LLM Evaluation, Benchmarks & Metrics",translation; reward; token level; level; translation quality; quality; token; machine translation; rl methods; level reward
8231-TACL,Aligned Probing: Relating Toxic Behavior and Model Internals,"We introduce aligned probing, a novel interpretability framework that aligns the behavior of language models (LMs), based on their outputs, and their internal representations (internals). Using this framework, we examine over 20 OLMo, Llama, and Mistral models, bridging behavioral and internal perspectives for toxicity for the first time. Our results show that LMs strongly encode information about the toxicity level of inputs and subsequent outputs, particularly in lower layers. Focusing on how unique LMs differ offers both correlative and causal evidence that they generate less toxic output when strongly encoding information about the input toxicity. We also highlight the heterogeneity of toxicity, as model behavior and internals vary across unique attributes such as Threat. Finally, four case studies analyzing detoxification, multi-prompt evaluations, model quantization, and pre-training dynamics underline the practical impact of aligned probing with further concrete insights. Our findings contribute to a more holistic understanding of LMs, both within and beyond the context of toxicity.","Andreas Waldis, Vagrant Gautam, Anne Lauscher, Dietrich Klakow, Iryna Gurevych",Andreas Waldis,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Interpretability & Model Analysis,"Trustworthy, Safety, Privacy & Fairness",toxicity; lms; probing; toxic; aligned; behavior; strongly; unique; internal; outputs
8325-TACL,CorefInst: Leveraging  LLMs for Multilingual Coreference Resolution,"Coreference Resolution (CR) is a crucial yet challenging task in natural language understanding, often constrained by task-specific architectures and encoder-based language models that demand extensive training and lack adaptability. This study introduces the first multilingual CR methodology, which leverages decoder-only LLMs to handle both overt and zero mentions. The article explores how to model the CR task for LLMs via five different instruction sets using a controlled inference method. The approach is evaluated across three LLMs; Llama 3.1, Gemma~2, and Mistral 0.3. The results indicate that LLMs, when instruction-tuned with a suitable instruction set, can surpass state-of-the-art task-specific architectures. Specifically, our best model, a fully fine-tuned Llama 3.1 for multilingual CR, outperforms the leading multilingual CR model (i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages in the CorefUD v1.2 dataset collection. ","Tuğba Pamay Arslan, Emircan Erol, Gülşen Eryiğit",Tuğba Pamay Arslan,Oral,Virtual,ZOOM,Virtual TBA,,,Information Extraction & Structured Prediction,Multilinguality & Low-Resource NLP,specific architectures; coreference; multilingual; instruction; resolution; task specific; architectures; llms; llama; tuned
8559-TACL,Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking,"Current automated fact-checking (AFC) approaches typically evaluate evidence either implicitly via the predicted verdicts
or through exact matches with predefined closed knowledge sources, such as Wikipedia. However, these methods are
limited due to reliance on evaluation metrics originally designed for other purposes and constraints from closed knowledge
sources. In this work, we introduce Ev2R, a weighted scorer that combines the strengths of reference-based evaluation and verdict-
level proxy scoring. Ev2R jointly assesses how well the evidence aligns with gold references and how reliably it supports the verdict, addressing the shortcomings of prior methods. We evaluate Ev2R against three categories of evidence evaluation approaches: reference-based, proxy-reference, and reference-less baselines. Assessments against human ratings and adversarial tests demonstrate that Ev2R consistently outperforms existing scoring approaches in accuracy and robustness. It achieves stronger correlation with human judgments and greater robustness to adversarial perturbations, establishing it as a reliable metric for evidence evaluation in AFC.",Mubashara Akhtar; Michael Schlichtkrull; Andreas Vlachos,Andreas Vlachos,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics","Retrieval, Grounding & External Knowledge (RAG)",reference; evidence; knowledge sources; verdict; automated fact; automated fact checking; fact checking; proxy; checking; scoring
8647-TACL,PiKGL: Leveraging Pruned Knowledge Graphs for Explainable Stance Detection,"Stance detection on social media plays a vital role in understanding public opinion on contentious topics. While prior work leverages external knowledge sources like Wikipedia to enrich limited target information, it primarily introduces conceptual content, neglecting the interpretability potential of knowledge and often leading to the incorporation of irrelevant or redundant information that hinders stance prediction performance. To address these limitations, we introduce PiKGL, a Pruned interpretable Knowledge Graph Learning framework for explainable stance detection. Specifically, we first extract event triples and topic entities from text and query the GDELT knowledge base to retrieve real-world knowledge, which is then used to construct an interpretable knowledge graph, enriching its content with the retrieved insights. To ensure precision and minimize noise, we introduce a topology-aware retrieval pruning strategy that incorporates target-specific commonsense knowledge, effectively filtering redundant nodes and enhancing graph relevance. Finally, the pruned knowledge graph is injected into a large language model to jointly model textual, target, and commonsense signals for improved stance comprehension. Experimental results conducted on three public datasets demonstrate our PiKGL achieves state-of-the-art performance on stance detection.","Bingbing Wang, Jingjie Lin, Zhixin Bai, Xintong Song, Qianlong Wang, Min Yang, Xi Zeng, Li Jing, Ruifeng Xu",Jingjie Lin,Oral,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",Interpretability & Model Analysis,stance; knowledge; pruned; graph; knowledge graph; detection; interpretable knowledge; target; explainable; commonsense
9193-TACL,Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework,"Large Language Models (LLMs) have great potential to accelerate and support scholarly peer review and are increasingly used as fully automatic review generators (ARGs). However, potential biases and systematic errors may pose significant risks to scientific integrity; understanding the specific capabilities and limitations of state-of-the-art ARGs is essential. We focus on a core reviewing skill that underpins high-quality peer review: detecting faulty research logic. This involves evaluating the internal consistency between a paper’s results, interpretations, and claims. We present a fully automated counterfactual evaluation framework that isolates and tests this skill under controlled conditions. Testing a range of ARG approaches, we find that, contrary to expectation, flaws in research logic have no significant effect on their output reviews. Based on our findings, we derive three actionable recommendations for future work and release our counterfactual dataset and evaluation framework publicly.","Nils Dycke, Iryna Gurevych",Nils Dycke,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",Domain NLP (Biomedical/Clinical/Legal/Scientific),counterfactual; review; evaluation framework; skill; peer review; peer; logic; fully; research; automatic
14-MAIN,LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts,"We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications. The code, data, and models will be made publicly available upon completion of the review process.",Yang Liu; Jiaye Yang; Weikang Li; Jiahui Liang; Yang Li; Lingyong Yan,Yang Liu,Oral,In-person,SALLE  LA PALMERAIE,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems",,definition; lexicon; semantic; expert; modeling; experts; clustering; routing; empirically demonstrate; methods widely
15-MAIN,Teams of LLM Agents can Exploit Zero-Day Vulnerabilities,"LLM agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that LLM agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities). In this work, we show that teams of LLM agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce HPTSA, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 14 real-world vulnerabilities and show that our team of agents improve over prior agent frameworks by up to 4.3X.",Yuxuan Zhu; Antony Kellermann; Akul Gupta; Philip Li; Richard Fang; Rohan Bindu; Daniel Kang,Philip Li,Oral,In-person,SALLE  LE RIAD,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",vulnerabilities; agents; real world vulnerabilities; world vulnerabilities; llm agents; planning; day; exploit; agent; teams
37-MAIN,Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?,"Variation in human annotation (i.e., disagreements) is common in NLP, often reflecting important information like task subjectivity and sample ambiguity. Modeling this variation is important for applications that are sensitive to such information. Although RLVR-style reasoning (Reinforcement Learning with Verifiable Rewards) has improved Large Language Model (LLM) performance on many tasks, it remains unclear whether such reasoning enables LLMs to capture informative variation in human annotation. In this work, we evaluate the influence of different reasoning settings on LLM disagreement modeling. We systematically evaluate each reasoning setting across model sizes, distribution expression methods, and steering methods, resulting in 60 experimental setups across 3 tasks. Surprisingly, our results show that RLVR-style reasoning degrades performance in disagreement modeling, while naive Chain-of-Thought (CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback). These findings underscore the potential risk of replacing human annotators with reasoning LLMs, especially when disagreements are important.",Jingwei Ni; Yu Fan; VilГ©m Zouhar; Donya Rooein; Alexander Miserlis Hoyle; Mrinmaya Sachan; Markus Leippold; Dirk Hovy; Elliott Ash,Jingwei Ni,Oral,In-person,SALLE  LA PALMERAIE,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Reasoning, Planning & Agents","Dialogue, Conversational & Interactive NLP",reasoning; disagreement; variation; style reasoning; variation human annotation; variation human; rlvr; disagreements; important; human
38-MAIN,Early-Exit and Instant Confidence Translation Quality Estimation,"Quality estimation is omnipresent in machine translation, for both evaluation and generation. Unfortunately, quality estimation models are often opaque and computationally expensive, making them impractical to be part of large-scale pipelines. In this work, we tackle two connected challenges: (1) reducing the cost of quality estimation at scale, and (2) developing an inexpensive uncertainty estimation method for quality estimation. To address the latter, we introduce Instant Confidence COMET, an uncertainty-aware quality estimation model that matches the performance of previous approaches at a fraction of their costs. We extend this to Early-Exit COMET, a quality estimation model that can compute quality scores and associated confidences already at early model layers, allowing us to early-exit computations and reduce evaluation costs. We also apply our model to machine translation reranking. We combine Early-Exit COMET with an upper confidence bound bandit algorithm to find the best candidate from a large pool without having to run the full evaluation model on all candidates. In both cases (evaluation and reranking) our methods reduce the required compute by 50% with very little degradation in performance. Finally, we show how Instant Confidence COMET can be used to decide which translations a human evaluator should score rather than relying on the COMET score.",VilГ©m Zouhar; Maike ZГјfle; Beni Egressy; Julius Cheng; Mrinmaya Sachan; Jan Niehues,Vilém Zouhar,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Machine Translation,"LLM Evaluation, Benchmarks & Metrics",quality estimation; estimation; comet; early; quality; confidence; quality estimation model; estimation model; translation; reranking
50-MAIN,GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval,"Decomposition-based multi-hop retrieval methods rely on many autoregressive steps to break down complex queries, which breaks end-to-end differentiability and is computationally expensive. Decomposition-free methods tackle this, but current approaches struggle with longer multi-hop problems and generalization to out-of-distribution data. To address these challenges, we introduce GRITHopper-7B, a novel multi-hop dense retrieval model that achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks. GRITHopper-7B combines generative and representational instruction tuning by integrating causal language modeling with dense retrieval training. Through controlled studies, we find that incorporating additional context after the retrieval process, referred to as post-retrieval language modeling, enhances dense retrieval performance. By including elements such as final answers during training, the model learns to better contextualize and retrieve relevant information. GRITHopper-7B offers a robust, scalable, and generalizable solution for multi-hop dense retrieval, and we release it to the community for future research and applications requiring complex reasoning and retrieval capabilities.",Justus-Jonas Erker; Nils Reimers; Iryna Gurevych,Justus-Jonas Erker,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)","Reasoning, Planning & Agents",dense retrieval; retrieval; dense; hop; multi hop; decomposition; multi; distribution; language modeling; free
53-MAIN,SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models,"Understanding long-context visual information remains a fundamental challenge for vision-language models, particularly in agentic tasks such as GUI control and web navigation. While web pages and GUI environments are inherently structured documents, current VLMs typically neglect decision-oriented document understanding in their training objectives. Existing approaches primarily extend visual embeddings to process long, high-resolution inputs, but these methods are memory-intensive and impractical for locally deployable solutions. To address these issues, we propose SCoPE VLM, a document navigation expert that leverages a novel Chain of Scroll mechanism to selectively and recursively navigate documents, focusing exclusively on relevant segments. We introduce a dedicated data generation pipeline to construct informative Chain of Scroll trajectories and Episodic Group Relative Policy Optimization, a tailored reinforcement learning method to reduce the gap between training and inference. Our method substantially reduces memory usage and effectively models human-like reading behaviors. To the best of our knowledge, SCoPE VLM is the first framework to explicitly model agentic reading patterns in multi-page document question answering, advancing the capabilities of multimodal agents.",Gyubeum Lim; Yemo Koo; Vijay Krishna Madisetti,Gyubeum Lim,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multimodal & Speech/Audio,"Efficiency, Scaling & NLP Systems",scope; document; vlm; navigation; gui; reading; agentic; web; chain; documents
54-MAIN,Beyond Sample-Level Feedback: Using Reference-Level Feedback to Guide Data Synthesis,"High-quality instruction-tuning data is crucial for developing Large Language Models (LLMs) that can effectively navigate real-world tasks and follow human instructions. While synthetic data generation offers a scalable approach for creating such datasets, it imposes a quality ceiling where models trained on such data cannot outperform the LLM generating it. To address this limitation, we introduce Reference-Level Feedback, a paradigm that breaks through this limitation by extracting desirable characteristics from carefully curated reference samples to guide the synthesis of new instruction-response pairs. Using this approach, we synthesize REFED, a dataset of 10K instruction-response pairs. Fine-tuning Llama-3.1-8B-Instruct and Mistral-7B-Instruct on REFED demonstrate state-of-the-art performance among similarly sized models, notably reaching a 43.96% length-controlled win-rate on AlpacaEval 2.0. Extensive experiments demonstrate that Reference-Level Feedback consistently outperforms traditional sample-level feedback methods, generalizes across model architectures, and produces high-quality and diverse data at low cost.",Shuhaib Mehri; Xiusi Chen; Heng Ji; Dilek Hakkani-TГјr,Shuhaib Mehri,Oral,In-person,SALLE  LA PALMERAIE,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Dialogue, Conversational & Interactive NLP",Summarization & Generation,level feedback; reference; feedback; level; response pairs; instruction response; instruction; synthesis; instruct; sample
56-MAIN,Investigating the Multilingual Calibration Effects of Language Model Instruction Tuning,"Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.",Jerry Huang; Peng Lu; QIUHAO Zeng; Yusuke Iwasawa; Yutaka Matsuo; Sarath Chandar; Edison Marrese-Taylor; Irene Li,Jerry Huang,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Multilinguality & Low-Resource NLP,calibration; multilingual; sft; languages; instruction tuning; resource; tuning; effects; maintaining; reliability
58-MAIN,T2-RAGBench: Text-and-Table Aware Retrieval-Augmented Generation,"Since many real-world documents combine textual and tabular data, robust Retrieval Augmented Generation (RAG) systems are essential for effectively accessing and analyzing such content to support complex reasoning tasks. Therefore, this paper introduces $\textbf{$T^2$-RAGBench}$, a benchmark comprising $\textbf{23,088}$ question-context-answer triples, designed to evaluate RAG methods on real-world text-and-table data. Unlike typical QA datasets that operate under Oracle-Context settings, $\textbf{$T^2$-RAGBench}$ challenges models to first retrieve the correct context before conducting numerical reasoning. Existing QA datasets containing text-and-table data typically contain context-dependent questions, which may yield multiple correct answers depending on the provided context. To address this, we transform SOTA datasets into a context-independent format, validated by experts as 91.3% context-independent questions, enabling reliable RAG evaluation. Our comprehensive evaluation identifies $\textit{Hybrid BM25}$ , a technique that combines dense and sparse vectors, as the most effective approach for text-and-table data. However, results demonstrate that $\textbf{$T^2$-RAGBench}$ remains challenging even for SOTA LLMs and RAG methods. Further ablation studies examine the impact of embedding models and corpus size on retrieval performance. $\textbf{$T^2$-RAGBench}$ provides a realistic and rigorous benchmark for existing RAG methods on text-and-table data. Code and dataset are available online: https://anonymous.4open.science/r/g4kmu-paper-D5F8/README.md.",Jan Strich; Enes Kutay Isgorur; Maximilian Trescher; Chris Biemann; Martin Semmann,Jan Strich,Oral,In-person,Pavillon  DE RABAT,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Retrieval, Grounding & External Knowledge (RAG)","LLM Evaluation, Benchmarks & Metrics",table; textbf; context; rag; rag methods; text; qa datasets; sota; independent; retrieval
59-MAIN,The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models,"Current large language models (LLMs) have demonstrated emerging capabilities in social intelligence tasks, including implicature resolution and theory-of-mind reasoning, both of which require substantial pragmatic understanding. However, how LLMs acquire this pragmatic competence throughout the training process remains poorly understood. In this work, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to evaluate whether LLMs at different training stages can accurately infer nuanced speaker intentions. Each instance pairs two equally plausible yet pragmatically divergent continuations and requires the model to (i) infer the speakerвЂ™s intended meaning and (ii) explain when and why a speaker would choose one utterance over its alternative, thus directly probing pragmatic competence through contrastive reasoning. We systematically evaluate 22 LLMs across three key training stages: after pre-training, supervised fine-tuning (SFT), and preference optimization, to examine the development of pragmatic competence. Our results show that even base models exhibit notable sensitivity to pragmatic cues, which improves consistently with increases in model and data scale. Additionally, SFT and RLHF contribute further gains, particularly in cognitive-pragmatic scenarios. These findings highlight pragmatic competence as an emergent and compositional property of LLM training and offer new insights for aligning models with human communicative norms.",Kefan Yu; Qingcheng Zeng; Weihao Xuan; Wanxin Li; Jingyi Wu; Rob Voigt,Kefan Yu,Oral,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Reasoning, Planning & Agents",pragmatic; competence; training; infer; mind; speaker; stages; sft; llms; evaluate 22
60-MAIN,Hierarchical Text Classification with LLM-Refined Taxonomies,"Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the modelвЂ™s inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.",Jonas Golde; Nicolaas Paul Jedema; RaviKiran Krishnan; Phong Le,Jonas Golde,Oral,In-person,SALLE  WALILI,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Linguistics, Syntax & Semantics",,taxonomies; refined; human curated; nodes; lms; leaf; parent; curated; text classification; classification
67-MAIN,"Divide, Reweight, and Conquer: A Logit Arithmetic Approach for In-Context Learning","In-Context Learning (ICL) emerges as a key feature for Large Language Models (LLMs), allowing them to adapt to new tasks by leverageing task-specific examples without updating model parameters. However, ICL faces challenges with increasing numbers of examples due to performance degradation and quadratic computational costs. In this paper, we propose Logit Arithmetic Reweighting Approach (LARA), a novel framework that enhances ICL by using logit-based ensembling of multiple demonstrations. Our approach divides long input demonstrations into parallelizable shorter inputs to significantly reduce memory requirements, and then effectively aggregate the information by reweighting logits of each group via a non-gradient optimization approach. We further introduce Binary LARA (B-LARA), a variant that constrains weights to binary values to simplify the search space and reduces memory usage by filtering out less informative demonstration groups. Experiments on BBH and MMLU demonstrate that LARA and B-LARA outperform all baseline methods in both accuracy and memory efficiency. We also conduct extensive analysis to show that LARA generalizes well to scenarios of varying numbers of examples from limited to many-shot demonstrations.",Chengsong Huang; Langlin Huang; Jiaxin Huang,Chengsong Huang,Oral,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",Interpretability & Model Analysis,lara; logit; icl; demonstrations; reweighting; examples; memory; numbers; binary; arithmetic
70-MAIN,Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models,"Conversational large language models are trained to refuse to answer harmful questions. However, emergent jailbreaking techniques can still elicit unsafe outputs, presenting an ongoing challenge for model alignment. This paper aims to deepen our understanding of how different jailbreak types circumvent safeguards by analyzing model activations on different jailbreak inputs. We find that it is possible to extract a jailbreak vector from a single class of jailbreaks that works to mitigate jailbreak effectiveness from other, semantically-dissimilar classes. This suggests that diverse jailbreaks may exploit a common internal mechanism. We investigate a potential common mechanism of harmfulness feature suppression, and find evidence that effective jailbreaks noticeably reduce a modelвЂ™s perception of prompt harmfulness. These insights pave the way for developing more robust jailbreak countermeasures and lay the groundwork for a deeper, mechanistic understanding of jailbreak dynamics in language models.",Sarah Ball; Frauke Kreuter; Nina Panickssery,Sarah Ball,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,jailbreak; jailbreaks; harmfulness; dynamics; understanding; common; mechanism; circumvent; analyzing model; deepen understanding
71-MAIN,Out of Style: RAGвЂ™s Fragility to Linguistic Variation,"Despite the impressive performance of Retrieval-augmented Generation (RAG) systems across various NLP benchmarks, their robustness in handling real-world user-LLM interaction queries remains largely underexplored. This presents a critical gap for practical deployment, where user queries exhibit greater linguistic variations and can trigger cascading errors across interdependent RAG components. In this work, we systematically analyze how varying four linguistic dimensions (formality, readability, politeness, and grammatical correctness) impact RAG performance. We evaluate two retrieval models and nine LLMs, ranging from 3 to 72 billion parameters, across four information-seeking Question Answering (QA) datasets. Our results reveal that linguistic reformulations significantly impact both retrieval and generation stages, leading to a relative performance drop of up to 40.41% in Recall@5 scores for less formal queries and 38.86% in answer match scores for queries containing grammatical errors. Notably, RAG systems exhibit greater sensitivity to such variations compared to LLM-only generations, highlighting their vulnerability to error propagation due to linguistic shifts. These findings highlight the need for improved robustness techniques to enhance reliability in diverse user interactions.",Tianyu Cao; Neel Bhandari; Akhila Yerukola; Akari Asai; Maarten Sap,Tianyu Cao,Oral,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,rag; linguistic; queries; exhibit greater; grammatical; user; rag systems; greater; variations; retrieval
73-MAIN,Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs,"Public opinion surveys show cross-cultural differences in political opinions between socio-cultural contexts. However, there is no clear evidence whether these differences translate to cross-lingual differences in multilingual large language models (MLLMs). We analyze whether opinions transfer between languages or whether there are separate opinions for each language in MLLMs of various sizes across five Western languages. We evaluate MLLMs' opinions by prompting them to report their (dis)agreement with political statements from voting advice applications. To better understand the interaction between languages in the models, we evaluate them both before and after aligning them with more left or right views using direct preference optimization and English alignment data only. Our findings reveal that unaligned models show only very few significant cross-lingual differences in the political opinions they reflect. The political alignment shifts opinions almost uniformly across all five languages. We conclude that in Western language contexts, political opinions transfer between languages, demonstrating the challenges in achieving explicit socio-linguistic, cultural, and political alignment of MLLMs.",Franziska Weeber; Tanise Ceron; Sebastian PadГі,Franziska Weeber,Oral,In-person,Pavillon  DE RABAT,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multilinguality & Low-Resource NLP,"Trustworthy, Safety, Privacy & Fairness",opinions; political; western; mllms; languages; differences; political alignment; unaligned; transfer languages; socio
78-MAIN,H-MEM: Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents,"Long-term memory is one of the key factors influencing the reasoning capabilities of Large Language Model Agents (LLM Agents). Incorporating a memory mechanism that effectively integrates past interactions can significantly enhance decision-making and contextual coherence of LLM Agents. While recent works have made progress in memory storage and retrieval, such as encoding memory into dense vectors for similarity-based search or organizing knowledge in the form of graph, these approaches often fall short in structured memory organization and efficient retrieval. To address these limitations, we propose a Hierarchical Memory Architecture that organizes and updates memory in a multi-level fashion based on the degree of semantic abstraction. Each memory vector at a higher level is embedded with a positional index encoding pointing to its semantically related sub-memories in the next layer. During the reasoning phase, an index-based routing mechanism enables efficient, layer-by-layer retrieval without performing exhaustive similarity computations. We evaluate our method on five task settings from the LoCoMo dataset. Experimental results show that our approach consistently outperforms five baseline methods, demonstrating its effectiveness in long-term dialogue scenarios.",Haoran Sun; Shaoning Zeng; Bob Zhang,Haoran Sun,Oral,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",memory; long term; llm agents; term; agents; layer; index; long; encoding; retrieval
82-MAIN,MULSUM: A Multimodal Summarization System with Vis-Aligner and Diversity-Aware Image Selection,"The abundance of multimodal news in digital form has intensified demand for systems that condense articles and images into concise, faithful digests. Yet most approaches simply conduct unimodal text summarization and attach the most-similar images with the text summary, which leads to redundancy both in processing visual content as well as in selection of images to complement the summary. We propose MULSUM, a two-step framework: (i) a Cross-Vis Aligner that projects image-level embeddings into a shared space and conditions a pre-trained LLM decoder to generate a visually informed text summary, and (ii) a Diversity-Aware Image Selector that, after the summary is produced, maximizes images-relevance to the summary while enforcing pairwise image diversity, yielding a compact, complementary visual set. Experimental results on the benchmarks MSMO (Multimodal Summarization with Multimodal Output) corpus show that MULSUM consistently outperforms strong baselines on automatic metrics such as ROUGE, while qualitative inspection shows that selected images act as explanatory evidence rather than ornamental add-ons. Human evaluation results shows that our diverse set of selected images was 13% more helpful than mere similarity-based image selection.",Abid Ali; Diego Molla; Usman Naseem,Abid Ali,Oral,In-person,Pavillon  DE RABAT,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Summarization & Generation,Multimodal & Speech/Audio,summary; images; image; multimodal; summarization; vis; text summary; selection; diversity; selected
83-MAIN,How Quantization Shapes Bias in Large Language Models,"This work presents a comprehensive evaluation of how quantization affects model bias, with particular attention to its impact on individual demographic subgroups. We focus on weight and activation quantization strategies and examine their effects across a broad range of bias types, including stereotypes, fairness, toxicity, and sentiment. We employ both probability- and generated text-based metrics across 13 benchmarks and evaluate models that differ in architecture family and reasoning ability. Our findings show that quantization has a nuanced impact on bias: while it can reduce model toxicity and does not significantly impact sentiment, it tends to slightly increase stereotypes and unfairness in generative tasks, especially under aggressive compression. These trends are generally consistent across demographic categories and subgroups, and model types, although their magnitude depends on the specific setting. Overall, our results highlight the importance of carefully balancing efficiency and ethical considerations when applying quantization in practice.",Federico Marcuzzi; Xuefei Ning; Roy Schwartz; Iryna Gurevych,Federico Marcuzzi,Oral,In-person,SALLE  WALILI,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Efficiency, Scaling & NLP Systems","Trustworthy, Safety, Privacy & Fairness",quantization; bias; stereotypes; impact; toxicity; sentiment; demographic; types; model types; consistent demographic
85-MAIN,"If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments in Large Language Models","Conditional acceptability refers to how plausible a conditional statement is perceived to be. It plays an important role in communication and reasoning, as it influences how individuals interpret implications, assess arguments, and make decisions based on hypothetical scenarios. When humans evaluate how acceptable a conditional ""If A, then B"" is, their judgments are influenced by two main factors: the $\textit{conditional probability}$ of $B$ given $A$, and the $\textit{semantic relevance}$ of the antecedent $A$ given the consequent $B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has examined how large language models (LLMs) draw inferences about conditional statements, it remains unclear how these models judge the $\textit{acceptability}$ of such statements. To address this gap, we present a comprehensive study of LLMs' conditional acceptability judgments across different model families, sizes, and prompting strategies. Using linear mixed-effects models and ANOVA tests, we find that models are sensitive to both conditional probability and semantic relevance$\textemdash{}$though to varying degrees depending on architecture and prompting style. A comparison with human data reveals that while LLMs incorporate probabilistic and semantic cues, they do so less consistently than humans. Notably, larger models do not necessarily align more closely with human judgments.",Jasmin Orth; Philipp Mondorf; Barbara Plank,Jasmin Orth,Oral,In-person,SALLE  LE LIXUS,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents",,conditional; judgments; textit; acceptable; statements; probability; semantic; humans; relevance; given
88-MAIN,The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts,"When language models correctly parse ""The cat that the dog chased meowed,'' are they analyzing syntax or simply familiar with dogs chasing cats? Despite extensive benchmarking, we lack methods to distinguish structural understanding from semantic pattern matching. We introduce **CenterBench**, a dataset of 9,720 comprehension questions on center-embedded sentences (like ""The cat [that the dog chased] meowed'') where relative clauses nest recursively, creating processing demands from simple to deeply nested structures. Each sentence has a syntactically identical but semantically implausible counterpart (e.g., mailmen prescribe medicine, doctors deliver mail) and six comprehension questions testing surface understanding, syntactic dependencies, and causal reasoning. Testing six models reveals that performance gaps between plausible and implausible sentences widen systematically with complexity, with models showing median gaps up to 26.8 percentage points, quantifying when they abandon structural analysis for semantic associations. Notably, semantic plausibility harms performance on questions about resulting actions, where following causal relationships matters more than semantic coherence. Reasoning models improve accuracy while their traces expose semantic shortcuts, overthinking, and answer refusal. Unlike models whose plausibility advantage systematically widens with complexity, humans show variable semantic effects. CenterBench provides the first framework to identify when models shift from structural analysis to pattern matching.",Sangmitra Madhusudan; Kaige Chen; Ali Emami,Sangmitra Madhusudan,Oral,In-person,SALLE  LA PALMERAIE,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Linguistics, Syntax & Semantics",Interpretability & Model Analysis,semantic; abandon; shortcuts; implausible; pattern matching; structural; plausibility; pattern; questions; comprehension
89-MAIN,Automated Screening of Antibacterial Nanoparticle Literature: Dataset Curation and Model Evaluation,"Antimicrobial resistance is a growing global health threat, driving interest in nanoparticle-based alternatives to conventional antibiotics. Inorganic nanoparticles with intrinsic antibacterial properties show significant promise; however, efficiently identifying relevant studies from the rapidly expanding literature remains a major challenge. This step is crucial for enabling computational approaches that aim to model and predict nanoparticle efficacy based on physicochemical and structural features. In this study, we explore the effectiveness of traditional machine learning and deep learning methods for classifying scientific abstracts in the domain of nanoparticle-based antimicrobial research. We introduce a novel, manually annotated corpus of 7,910 articles, curated to distinguish intrinsic antibacterial nanoparticles from studies focusing on drug carriers or surface-bound applications. Our comparative evaluation shows that a fine-tuned BERT model achieves the highest overall F1 score of 0.79 on the test set, while simpler models such as SVMs with TF-IDF features remain highly competitive, highlighting their utility in low-resource settings. The dataset and best-performing models are made publicly available to support future research in nanoparticle-based therapeutics and automated literature mining.",Alperen Ozturk; Ећaziye BetГјl Г–zateЕџ; Sophia Bahar Root; Angela Violi; Nicholas Kotov; J. Scott VanEpps; Emine Sumeyra Turali Emre,M. Alperen Öztürk and Şaziye Betül Özateş,Oral,In-person,SALLE  LE LIXUS,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multilinguality & Low-Resource NLP,"LLM Evaluation, Benchmarks & Metrics",literature; intrinsic; based; features; automated; studies; idf; dataset curation; identifying relevant; rapidly expanding
91-MAIN,Intention Knowledge Graph Construction for User Intention Relation Modeling,"Understanding user intentions is challenging for online platforms. Recent work on intention knowledge graphs addresses this but often lacks focus on connecting intentions, which is crucial for modeling user behavior and predicting future actions. This paper introduces a framework to automatically generate an intention knowledge graph, capturing connections between user intentions. Using the Amazon m2 dataset, we construct an intention graph with 351 million edges, demonstrating high plausibility and acceptance. Our model effectively predicts new session intentions and enhances product recommendations, outperforming previous state-of-the-art methods and showcasing the approach's practical utility.",Jiaxin Bai; Zhaobo Wang; Junfei Cheng; Dan Yu; Zerui Huang; Weiqi Wang; Xin Liu; Chen Luo; Yanming Zhu; Bo Li; Yangqiu Song,Jiaxin Bai,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Dialogue, Conversational & Interactive NLP",,intention; intentions; user; graph; knowledge graph; knowledge; modeling; relation modeling; outperforming previous; model effectively
92-MAIN,"Analogical Structure, Minimal Contextual Cues and Contrastive Distractors: Input Design for Sample-Efficient Linguistic Rule Induction","Large language models achieve strong performance through training on vast datasets. Can analogical paradigm organization enable lightweight models to match this performance with minimal data? We develop a computational approach implementing three cognitive-inspired principles: analogical structure, contrastive learning, and minimal contextual cues. We test this approach with structured completion tasks where models identify correct sentence completions from analogical patterns with contrastive alternatives. Training lightweight models (BERT+CNN, 0.5M parameters) on only one hundred structured examples of English causative/inchoative alternations achieves F1=0.95, outperforming zero-shot GPT-o3 (F1=0.87). Ablation studies confirm that analogical organization and contrastive structure improve performance, consistently surpassing randomly shuffled baselines across architectures. Cross-phenomenon validation using unspecified object alternations replicates these efficiency gains, confirming approach robustness. Our results show that analogical paradigm organization enables competitive linguistic rule learning with orders of magnitude less data than conventional approaches require.",Chunyang Jiang; Paola Merlo,Chunyang Jiang,Oral,In-person,SALLE  LE LIXUS,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Efficiency, Scaling & NLP Systems","Linguistics, Syntax & Semantics",analogical; contrastive; organization; minimal; lightweight models; structure; rule; contextual; paradigm; cues
93-MAIN,JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models' Detection of Human risky health behavior Content in Jirai Community,"In this paper, we present the first cross-lingual dataset that captures a transnational cultural phenomenon, focusing on the Chinese and Japanese ""Jirai"" subculture and its association with risky health behaviors. Our dataset of more than 15,000 annotated social media posts forms the core of JiraiBench, a benchmark designed to evaluate LLMs on culturally specific content. This unique resource allowed us to uncover an unexpected cross-cultural transfer in which Japanese prompts better handle Chinese content, indicating that cultural context can be more influential than linguistic similarity. Further evidence suggests potential cross-lingual knowledge transfer in fine-tuned models. This work proves the indispensable role of developing culturally informed, cross-lingual datasets for creating effective content moderation tools that can protect vulnerable communities across linguistic borders.",Yunze Xiao; Tingyu He; Lionel Z. WANG; Yiming Ma; Xingyu Song; Xiaohang Xu; Mona T. Diab; Irene Li; Ka Chung Ng,Yunze Xiao,Oral,Virtual,ZOOM,Virtual TBA,,,Multilinguality & Low-Resource NLP,"Trustworthy, Safety, Privacy & Fairness",cross; risky; cultural; content; cross lingual; lingual; chinese; japanese; culturally; health
94-MAIN,Chandomitra: Towards Generating Structured Sanskrit Poetry from Natural Language Inputs,"Text Generation has achieved remarkable performance using large language models. It has also been recently well-studied that these large language models are capable of creative generation tasks but prominently for high-resource languages. This prompts a fundamental question: $\textit{Is there a way to utilize these (large) language models for structured poetry generation in a low-resource language, such as Sanskrit?}$ We present Chandomitra, an English input to structured Sanskrit Poetry translation dataset, specifically adhering to the Anushtubh meter. We benchmark various open and closed models, and scrutinize specialized techniques such as constrained decoding and instruction fine-tuning, for the proposed task. Our constrained decoding methodology achieves 99.86% syntactic accuracy in generating metrically valid Sanskrit poetry, outperforming GPT-4o (1-shot: 31.24%). Our best-performing instruction-tuned model, on the other hand, performs better in semantic coherence with the English input, at the expense of slightly lower syntactic accuracy. Human evaluation further reveals that instruction fine-tuned model is better able to capture the poetic aspects.",Manoj Balaji Jagadeeshan; Samarth Bhatia; Pretam Ray; Harshul Raj Surana; Akhil Rajeev P; PRIYA MISHRA; ANNARAO KULKARNI; Ganesh Ramakrishnan; Prathosh AP; Pawan Goyal,Pretam Ray,Oral,In-person,SALLE  LA PALMERAIE,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics",Summarization & Generation,poetry; constrained decoding; instruction fine; syntactic accuracy; instruction; syntactic; structured; constrained; decoding; generation
96-MAIN,Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity,"Large language models (LLMs) show promise in offering emotional support and generating empathetic responses for individuals in distress, but their ability to deliver culturally sensitive support remains underexplored due to lack of resources. In this work, we introduce CultureCare, the first dataset designed for this task, spanning four cultures and including 1729 distress messages, 1523 cultural signals, and 1041 support strategies with fine-grained emotional and cultural annotations. Leveraging CultureCare, we (i) develop and test four adaptation strategies for guiding three state-of-the-art LLMs toward culturally appropriate responses; (ii) conduct comprehensive evaluations using LLM judges, in-culture human annotators, and clinical psychologists; (iii) show that adapted LLMs outperform anonymous online peer responses, and that simple cultural role-play is insufficient for cultural sensitivity; and (iv) explore the application of LLMs in clinical training, where experts highlight their potential in fostering cultural competence in future therapists.",Chen Cecilia Liu; Hiba Arnaout; Nils KovaДЌiД‡; Dana Atzil-Slonim; Iryna Gurevych,Hiba Arnaout,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness",Domain NLP (Biomedical/Clinical/Legal/Scientific),cultural; emotional; distress; support; responses; culturally; sensitivity; clinical; llms; strategies
97-MAIN,Detecting Subtle Sense Shift with Polysemy-Aware Trends,"Language changes faster than dictionaries can be revised, yet automatic tools still struggle to spot the subtle, shortвЂ‘term shifts in meaning that precede a formal update. We present a languageвЂ‘independent pipeline that detects wordвЂ‘sense shifts in large, timeвЂ‘stamped web corpora. The method couples a robust reвЂ‘implementation of the Adaptive SkipвЂ‘Gram model, which induces multiple sense vectors per lemma without any external inventory, with a second stage that tracks each sense through time under three alternative frequency normalizations. Linear Regression and the robust Mann-Kendall/Theil-Sen estimator then test whether a senseвЂ™s frequency slope deviates significantly from zero, producing a ranked list of headwords whose semantics are drifting. We evaluate the system on the English (12 B tokens) and Czech (1 B tokens) Timestamped corpora for May 2023-May 2025. Expert annotation of the topвЂ‘100 candidates for each model variant shows that 50.7 % of Czech and 25.7 % of English headwords exhibit genuine sense shifts, despite webвЂ‘scale noise.",OndЕ™ej Herman; Pavel RychlГЅ,Ondřej Herman,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Linguistics, Syntax & Semantics",,sense; shifts; frequency; subtle; corpora; tokens; expert annotation; precede; lemma; estimator
98-MAIN,Leveraging LLM-GNN Integration for Open-World Question Answering over Knowledge Graphs,"Open-world Question Answering (OW-QA) over knowledge graphs (KGs) aims to answer questions over incomplete or evolving KGs. Traditional KGQA assumes a closed world where answers must exist in the KG, limiting real-world applicability. In contrast, open- world QA requires inferring missing knowledge based on graph structure and context. Large language models (LLMs) excel at language understanding but lack structured reasoning. Graph neural networks (GNNs) model graph topology but struggle with semantic interpretation. Existing systems integrate LLMs with GNNs or graph retrievers. Some support open-world QA but rely on structural embeddings without semantic grounding. Most assume observed paths or complete graphs, making them unreliable under missing links or multi-hop reasoning. We present GLOW, a hybrid system that combines a pre-trained GNN and an LLM for open-world KGQA. The GNN predicts top-k candidate answers from the graph structure. These, along with relevant KG facts, are serialized into a structured prompt (e.g., triples and candidates) to guide the LLMвЂ™s reasoning. This enables joint reasoning over symbolic and semantic signals, without relying on retrieval or fine-tuning. To evaluate generalization, we introduce GLOW-BENCH, a 1,000-question benchmark over incomplete KGs across diverse domains. GLOW outperforms existing LLMвЂ“GNN systems on standard benchmarks and GLOW-BENCH, achieving up to 53.3% and an average 38% improvement.",Hussein Abdallah; Ibrahim Abdelaziz; Panos Kalnis; Essam Mansour,Essam Mansour,Oral,In-person,SALLE  LA PALMERAIE,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)","Reasoning, Planning & Agents",open world; gnn; world; graph; kgs; open; graphs; gnns; llmвђ; graph structure
99-MAIN,Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models,"As Large Language Models (LLMs) become increasingly integrated into everyday life and information ecosystems, concerns about their implicit biases continue to persist. While prior work has primarily examined socio-demographic and leftвЂ“right political dimensions, little attention has been paid to how LLMs align with broader geopolitical value systems, particularly the democracyвЂ“authoritarianism spectrum. In this paper, we propose a novel methodology to assess such alignment, combining (1) the F-scale, a psychometric tool for measuring authoritarian tendencies, (2) FavScore, a newly introduced metric for evaluating model favorability toward world leaders, and (3) role-model probing to assess which figures are cited as general role models by LLMs. We find that LLMs generally favor democratic values and leaders, but exhibit increased favorability toward authoritarian figures when prompted in Mandarin. Further, models are found to often cite authoritarian figures as role models, even outside explicitly political contexts. These results shed light on ways LLMs may reflect and potentially reinforce global political ideologies, highlighting the importance of evaluating bias beyond conventional socio-political axes.",David Guzman Piedrahita; Irene Strauss; Rada Mihalcea; Zhijing Jin,"David Guzman Piedrahita, Irene Strauss",Oral,In-person,SALLE  WALILI,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,political; democratic; socio; role; probing; llms; biases; assess; axes; political biases
101-MAIN,PromptFE: Automated Feature Engineering by Prompting,"Automated feature engineering (AutoFE) liberates data scientists from the burden of manual feature construction. The rich semantic information of datasets has been underutilized in many existing AutoFE works. We present PromptFE, a novel AutoFE framework that leverages large language models (LLMs) to automatically construct features in a compact string format and generate semantic explanations based on dataset descriptions. By learning the performance of constructed features in context, the LLM iteratively improves feature construction. We demonstrate through experiments on real-world datasets the superior performance of PromptFE over state-of-the-art methods. We verify the impact of dataset semantic information and provide comprehensive study on the LLM-based feature construction process.",Yufeng Zou; Jean Utke; Diego Klabjan; Han Liu,Diego Klabjan,Oral,In-person,SALLE  WALILI,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Information Extraction & Structured Prediction,,feature; construction; feature engineering; semantic information; engineering; semantic; features; automated; data scientists; llms automatically
106-MAIN,Detecting (Un)answerability in Large Language Models with Linear Directions,"Large language models (LLMs) often respond confidently to questions even when they lack the necessary information, leading to hallucinated answers. In this work, we study the problem of (un)answerability detection in extractive question answering (QA), where the model should determine if a passage contains sufficient information to answer a given question. We propose a simple approach that identifies a direction in the modelвЂ™s activation space that captures unanswerability and uses it for classification. This direction is selected by applying activation additions during inference and measuring their impact on the modelвЂ™s abstention behavior. We show that projecting hidden activations onto this direction yields a reliable score for (un)answerability classification. Experiments on two open-weight LLMs and four QA benchmarks show that our method effectively detects unanswerable questions and generalizes better across datasets than existing prompt-based and classifier-based approaches. Causal interventions reveal that adding the direction increases abstention, while ablating it suppresses it, further indicating that it captures an unanswerability signal.",Maor Juliet Lavi; Tova Milo; Mor Geva,Maor Juliet Lavi,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Efficiency, Scaling & NLP Systems",,answerability; direction; abstention; activation; modelвђ; captures; classification; questions; information answer; contains sufficient
110-MAIN,Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning,"Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning ability of Large Language Models (LLMs). However, due to the sparsity of rewards in RORL, effective training is highly dependent on the selection of problems of appropriate difficulty. Although curriculum learning attempts to address this by adjusting difficulty, it often relies on static schedules, and even recent online filtering methods lack theoretical grounding and a systematic understanding of their effectiveness. In this work, we theoretically and empirically show that curating the batch with the problems that the training model achieves intermediate accuracy on the fly can maximize the effectiveness of RORL training, namely balanced online difficulty filtering. We first derive that the lower bound of the KL divergence between the initial and the optimal policy can be expressed with the variance of the sampled accuracy. Building on those insights, we show that balanced filtering can maximize the lower bound, leading to better performance. Experimental results across five challenging math reasoning benchmarks with 3B and 7B scale models show that balanced online filtering yields an additional 10% in AIME and 13% in AMC with scalability. Moreover, further analysis shows the gains in sample and training time efficiency, exceeding the plain GRPO within 60% training time and the training set volume.",Sanghwan Bae; Jiwoo Hong; Min Young Lee; Hanbyul Kim; jeongyeon nam; Donghyun Kwak,Sanghwan Bae,Oral,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)","Reasoning, Planning & Agents",filtering; online; difficulty; balanced; training; lower bound; reasoning oriented; training time; maximize; bound
111-MAIN,"BERT, are you paying attention? Attention regularization with human-annotated rationales","Attention regularization aims to supervise the attention patterns in language models like BERT. Various studies have shown that using human-annotated rationales, in the form of highlights that explain why a text has a specific label, can have positive effects on model generalizability. In this work, we address a current gap in the literature: Does attention regularization using human-annotated rationales truly make the model rely more on the tokens that are part of the human-annotated rationale? Our results suggest that oftentimes, attention regularization with randomly selected tokens yields similar improvements as attention regularization with human-annotated rationales. Nevertheless, we find that human-annotated rationales surpass randomly selected tokens when it comes to reducing model sensitivity to strong spurious correlations.",Elize Herrewijnen; Dong Nguyen; Floris Bex; Albert Gatt,Elize Herrewijnen,Oral,In-person,SALLE  LE LIXUS,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Interpretability & Model Analysis,,human annotated; rationales; regularization; attention; annotated; human; using human; randomly; tokens; selected
112-MAIN,Humans and transformer LMs: Abstraction drives language learning,"Categorization is a core component of human linguistic competence. We investigate how a transformer-based language model (LM) learns linguistic categories by comparing its behaviour over the course of training to behaviours which characterize abstract featureвЂ“based and concrete exemplarвЂ“based accounts of human language acquisition. We investigate how lexical semantic and syntactic categories emerge using novel divergence-based metrics that track learning trajectories using next-token distributions. In experiments with GPT-2 small, we find that (i) when LMs learn a construction, abstract class-level behaviour is evident at earlier steps than lexical itemвЂ“specific behaviour, and (ii) that different LM behaviours emerge abruptly in sequence at different points in training, revealing that abstraction plays a key role in how LMs learn. This result informs the models of human language acquisition that LMs may serve as an existence proof for.",Jasper Jian; Christopher D Manning,Jasper Jian,Oral,In-person,Pavillon  DE RABAT,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Linguistics, Syntax & Semantics",,lms; behaviour; human language; language acquisition; behaviours; acquisition; emerge; abstraction; abstract; learn
114-MAIN,BigTokDetect: A ClinicallyвЂ‘Informed VisionвЂ“Language Model Framework for Detecting ProвЂ‘Bigorexia Videos on TikTok,"Social media platforms increasingly struggle to detect harmful content that promotes muscleвЂ‘dysmorphic behaviors, particularly proвЂ‘bigorexia material that disproportionately affects adolescent males. Unlike traditional eatingвЂ‘disorder detection focused on the вЂњthin ideal,вЂќ proвЂ‘bigorexia content often masquerades as legitimate fitness guidance, combining visual displays, coded language, and motivational messaging in ways that evade textвЂ‘only detection systems. We address this challenge with BigTokDetect, a clinically informed framework for identifying proвЂ‘bigorexia videos on TikTok. At its core is BigTok, the first expertвЂ‘annotated multimodal dataset of more than 2,200 TikTok videos labeled by clinical psychologists and psychiatrists across five primary categoriesвЂ”body image, nutrition, exercise, supplements, and masculinity. Through a comprehensive evaluation of stateвЂ‘ofвЂ‘theвЂ‘art visionвЂ“language models, BigTokDetect achieves 82.9вЂЇpercent accuracy on primaryвЂ‘category classification and 69.0вЂЇpercent on subcategory detection after domainвЂ‘specific fineвЂ‘tuning. Ablation studies show that multimodal fusion outperforms textвЂ‘only approaches by 5вЂ“10вЂЇpercentage points, with video features providing the most discriminative signals. These results establish new benchmarks for multimodal harmfulвЂ‘content detection and offer a scalable, clinically grounded approach to content moderation in specialized mentalвЂ‘health domains.",Minh Duc Chu; Kshitij Pawar; Zihao He; Roxanna Sharifi; Ross M. Sonnenblick; Magdalayna Curry; Laura DAdamo; Lindsay Young; Stuart Murray; Kristina Lerman,Minh Duc Hoang Chu,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,,tiktok; videos; detection; textвђ; content; clinically; visionвђ language; visionвђ; multimodal; informed
115-MAIN,Do language models accommodate their users? A study of linguistic convergence,"While large language models (LLMs) are generally considered proficient in generating language, how similar their language usage is to that of humans remains understudied. In this paper, we test whether models exhibit linguistic convergence, a core pragmatic element of human language communication, asking: do models adapt, or converge, to the linguistic patterns of their user? To answer this, we systematically compare model completions of exisiting dialogues to the original human responses across sixteen language models, three dialogue corpora, and a variety of stylometric features. We find that models strongly converge to the conversation's style, often significantly overfitting relative to the human baseline. While convergence patterns are often feature-specific, we observe consistent shifts in convergence across modeling settings, with instruction-tuned and larger models converging less than their pretrained counterparts. Given the differences between human and model convergence patterns, we hypothesize that the underlying mechanisms for these behaviors are very different.",Terra Blevins; Susanne Schmalwieser; Benjamin Roth,Terra Blevins,Oral,In-person,SALLE  LE LIXUS,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Dialogue, Conversational & Interactive NLP","Linguistics, Syntax & Semantics",convergence; converge; patterns; linguistic; human; language; underlying mechanisms; human language; accommodate; stylometric
117-MAIN,Auditing Language Model Unlearning via Information Decomposition,"We expose a critical limitation in current approaches to machine unlearning in language models: despite the apparent success of unlearning algorithms, information about the forgotten data remains linearly decodable from internal representations. To systematically assess this discrepancy, we introduce an interpretable, information-theoretic framework for auditing unlearning using Partial Information Decomposition (PID). By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge. Our analysis reveals that redundant information, shared across both models, constitutes residual knowledge that persists post-unlearning and correlates with susceptibility to known adversarial reconstruction attacks. Leveraging these insights, we propose a representation-based risk score that can guide abstention on sensitive inputs at inference time, providing a practical mechanism to mitigate privacy leakage. Our work introduces a principled, representation-level audit for unlearning, offering theoretical insight and actionable tools for safer deployment of language models.",Anmol Goel; Alan Ritter; Iryna Gurevych,Anmol Goel,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,unlearning; information; residual; auditing; decomposition; representation; representations; apparent; formalizing; redundant information
119-MAIN,Logic Haystacks: Probing LLMsвЂ™ Long-Context Logical Reasoning (Without Easily Identifiable Unrelated Padding),"Large language models demonstrate promising long-context processing capabilities, with recent models touting context windows close to one million tokens. However, the evaluations supporting these claims often involve simple retrieval tasks or synthetic tasks padded with irrelevant text, which models may easily detect and discard. In this work, we generate lengthy, simplified English text with first-order logic representations spanning up to 2048 sentences (~25$k GPT-4 tokens). We formulate an evaluation task with evidence retrieval for contradiction detection. The long, homogeneous text is filled with distractors that are both hard to distinguish from relevant evidence and provably non-interfering. Our evaluation of evidence retrieval reveals that the effective context window is much smaller with such realistic distractors, already crumbling at 128 sentences.",Damien Sileo,Damien Sileo,Oral,In-person,SALLE  LE RIAD,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)",Interpretability & Model Analysis,evidence retrieval; distractors; context; evidence; easily; long; long context; logic; retrieval; sentences
120-MAIN,OD-Stega: LLM-Based Relatively Secure Steganography via Optimized Distributions,"We consider coverless steganography where a Large Language Model (LLM) is used to generate stego-texts in combination with arithmeticic coding. An efficient method should embed secret bits in as few language tokens as possible while keeping the stego-text as natural as possible. We show that this problem is equivalent to maximizing the entropy of a replacement probability distribution of the next token generation, subject to a constraint on the divergence between the new distribution and the original one produced by the LLM. A closed-form solution is provided under either the KL divergence or the total variation constraint. Several important practical issues are also tackled: 1) An often-overlooked tokenization mismatch issue is resolved with a simple prompt selection approach, 2) The combination of the optimized distribution and the vocabulary truncation technique is considered, and 3) The incorporation of the proposed approach with existing (potentially non arithemtic coding based) techniques, e.g., the Discop technique.",Yu-Shin Huang; Peter Just; Hanyun Yin; Krishna Narayanan; Ruihong Huang; Chao Tian,Yu-Shin Huang,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Summarization & Generation,,constraint; distribution; divergence; possible; combination; technique; coding; optimized; language tokens; probability distribution
121-MAIN,"When Does Auxiliary Modality Matter in Solving Geometric Problems? A Comprehensive Study of Textual, Formal, and Visual Modalities","Large Language Models (LLMs) face challenges in integrating linguistic and spatial reasoning, which limits their performance on geometry problems. While prior work has attempted to bridge this gap using diagram parsers with multimodal models, a systematic comparison of how various auxiliary modalities and their combinations affect performance has been lacking. To address this, we present a systematic study of four auxiliary modalitiesвЂ”formal diagram facts (CDL), natural language representations ($T_{CDL}$), diagram descriptions (DES), and image augmentations (IMG)вЂ”on a range of open- and closed-source multimodal LLMs. Our analysis reveals a compelling dichotomy in the effectiveness of these modalities. While formal representations like CDL and $T_{CDL}$ offer a modest performance lift, diagram descriptions (DES) cause a dramatic split: they significantly boost the accuracy of open-source LLMs which often struggle with visual parsing, while often misleading more capable closed-source models and causing a performance drop. This highlights a critical trade-off between augmenting input with helpful information and introducing misleading noise, demonstrating that the efficacy of auxiliary modalities is heavily dependent on the inherent capabilities of the underlying model.",Hyuk Namgoong; Jeesu Jung; Yerim Han; Sangkeun Jung,Hyuk Namgoong,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Linguistics, Syntax & Semantics",diagram; auxiliary; modalities; formal; misleading; descriptions; source; closed source; closed; problems
122-MAIN,IYKYK: Using language models to decode extremist cryptolects,"Extremist groups develop complex in-group language, also referred to as cryptolects, to exclude or mislead outsiders. We investigate the ability of current language technologies to detect and interpret the cryptolects of two online extremist platforms. Evaluating eight models across six tasks, our results indicate that general purpose LLMs cannot consistently detect or decode extremist language. However, performance can be significantly improved by domain adaptation and specialised prompting techniques. These results provide important insights to inform the development and deployment of automated moderation technologies. We further develop and release novel labelled and unlabelled datasets, including 19.4M posts from extremist platforms and lexicons validated by human experts.",Christine de Kock; Arij Riabi; Zeerak Talat; Michael Sejr Schlichtkrull; Pranava Madhyastha; Eduard Hovy,Zeerak Talat,Oral,In-person,SALLE  WALILI,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),,technologies; platforms; develop; detect; lexicons; release novel; investigate ability; current language; general purpose llms; models tasks
123-MAIN,Sparse Adapter Fusion for Continual Learning in NLP,"Continual learning in natural language processing plays a crucial role in adapting to evolving data and preventing catastrophic forgetting. Despite significant progress, existing methods still face challenges, such as inefficient parameter reuse across tasks, risking catastrophic forgetting when tasks are dissimilar, and the unnecessary introduction of new parameters for each task, which hampers knowledge sharing among similar tasks. To tackle these issues, we propose a Sparse Adapter Fusion Method (SAFM), which dynamically fuses old and new adapters to address these challenges. SAFM operates in two stages: the decision stage and the tuning stage. In the decision stage, SAFM determines whether to incorporate a new adapter, reuse an existing one, or add an empty adapter. The architecture search procedure, designed to prioritize reusing or adding empty adapters, minimizes parameter consumption and maximizes reuse. In the tuning stage, SAFM especially facilitates a layer-wise loss to encourage differentiation between adapters, effectively capturing knowledge within the same task. Experimental results consistently show that SAFM outperforms state-of-the-art (SOTA) methods, achieving comparable performance while utilizing less than 60% of the parameters.",Min Zeng; Xi Chen; Haiqin Yang; Yike Guo,Xi Chen,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",,adapter; reuse; stage; adapters; catastrophic; continual learning; catastrophic forgetting; forgetting; fusion; continual
125-MAIN,Rethinking Prompt Optimizers: From Prompt Merits to Optimization,"Prompt optimization (PO) provides a practical way to improve response quality when users lack the time or expertise to manually craft effective prompts. Existing methods typically rely on LLMs' self-generation ability to optimize prompts. However, due to limited downward compatibility, the instruction-heavy prompts generated by advanced LLMs can overwhelm lightweight inference models and degrade response quality, while also lacking interpretability due to implicit optimization. In this work, we rethink prompt optimization through the lens of explicit and interpretable design. We first identify a set of model-agnostic prompt quality merits and empirically validate their effectiveness in enhancing prompt and response quality. We then introduce MePO, a merit-guided, locally deployable prompt optimizer trained on our merit-guided prompt preference dataset generated by a lightweight LLM. MePO avoids online optimization, reduces privacy concerns, and, by learning clear, interpretable merits, generalizes effectively to both large-scale and lightweight inference models. Experiments demonstrate that MePO achieves better results across diverse tasks and model types, offering a scalable and robust solution for real-world deployment.",Zixiao Zhu; Hanzhang Zhou; Zijian Feng; Tianjiao Li; Chua Jia Jim Deryl; Lee Onn Mak; Gee Wah Ng; Kezhi Mao,Zijian Feng,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,prompt; optimization; response quality; inference models; merit; lightweight; response; prompts; prompt optimization; quality
131-MAIN,A Survey on Multilingual Mental Disorders Detection from Social Media Data,"The increasing prevalence of mental health disorders globally highlights the urgent need for effective digital screening methods that can be used in multilingual contexts. Most existing studies, however, focus on English data, overlooking critical mental health signals that may be present in non-English texts. To address this important gap, we present the first survey on the detection of mental health disorders using multilingual social media data. We investigate the cultural nuances that influence online language patterns and self-disclosure behaviors, and how these factors can impact the performance of NLP tools. Additionally, we provide a comprehensive list of multilingual data collections that can be used for developing NLP models for mental health screening. Our findings can inform the design of effective multilingual mental health screening tools that can meet the needs of diverse populations, ultimately improving mental health outcomes on a global scale.",Ana-Maria Bucur; Marcos Zampieri; Tharindu Ranasinghe; Fabio Crestani,Ana-Maria Bucur,Oral,In-person,SALLE  WALILI,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multilinguality & Low-Resource NLP,"Trustworthy, Safety, Privacy & Fairness",mental; mental health; health; screening; disorders; multilingual; media data; mental health screening; health screening; social media data
132-MAIN,Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns,"Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of instruction-following tasks, yet their grasp of nuanced social science concepts remains underexplored. This paper examines whether LLMs can identify and classify fine-grained forms of populism, a complex and contested concept in both academic and media debates. To this end, we curate and release novel datasets specifically designed to capture populist discourse. We evaluate a range of pre-trained (large) language models, both open-weight and proprietary, across multiple prompting paradigms. Our analysis reveals notable variation in performance, highlighting the limitations of LLMs in detecting populist discourse. We find that a fine-tuned RoBERTa classifier vastly outperforms all new-era instruction-tuned LLMs, unless fine-tuned. Additionally, we apply our best-performing model to analyze campaign speeches by Donald Trump, extracting valuable insights into his strategic use of populist rhetoric. Finally, we assess the generalizability of these models by benchmarking them on campaign speeches by European politicians, offering a lens into cross-context transferability in political discourse analysis. In this setting, we find that instruction-tuned LLMs exhibit greater robustness on out-of-domain data.",Ilias Chalkidis; Stephanie Brandl; Paris Aslanidis,Ilias Chalkidis,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Linguistics, Syntax & Semantics",Interpretability & Model Analysis,discourse; tuned; instruction tuned llms; political discourse; tuned llms; instruction; political; fine; forms; instruction tuned
134-MAIN,SCoNE: a Self-Correcting and Noise-Augmented Method for Complex Biological and Chemical Named Entity Recognition,"Generative methods have recently gained traction in biological and chemical named entity recognition for their ability to overcome tagging limitations and better capture entity-rich contexts. However, under a few-shot environment, they struggle with the scarcity of annotated data and the structural complexity of biological and chemical entitiesвЂ”particularly nested and discontinuous onesвЂ”leading to incorrect recognition and error propagation during generation. To address these challenges, we propose SCoNE, a Self-Correcting and Noise-Augmented Method for Complex Biological and Chemical Named Entity Recognition. Specifically, we introduce a Noise Augmentation Module to enhance training diversity and guide the model to better learn complex entity structures. Besides, we design a Confidence-based Self-Correction Module that identifies low-confidence outputs and revises them to improve generation robustness. Benefiting from these designs, our method outperforms the baselines by 1.80 and 2.73 F1-score on the CHEMDNER and microbial ecology dataset Florilege, highlighting its effectiveness in biological and chemical named entity recognition.",Xingyu Zhu; Claire NГ©dellec; Balazs Nagy; Laszlo Vidacs; Robert Bossy,Xingyu Zhu,Oral,In-person,SALLE  LE LIXUS,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Information Extraction & Structured Prediction,Summarization & Generation,chemical; biological; chemical named entity; chemical named; entity; named entity recognition; recognition; entity recognition; named entity; named
135-MAIN,A Benchmark for Audio Reasoning Capabilities of Multimodal Large Language Models,"The present benchmarks for testing the audio modality of multimodal large language models concentrate on testing various audio tasks such as speaker diarization or gender identification in isolation. Whether a multimodal model can answer the questions that require reasoning skills to combine audio tasks of different categories cannot be verified with their use. To address this issue, we propose Audio Reasoning Tasks (ART), a new benchmark for assessing the ability of multimodal models to solve problems that require reasoning over audio signal.",Iwona Christop; Mateusz CzyЕјnikiewicz; PaweЕ‚ SkГіrzewski; ЕЃukasz Bondaruk; Jakub Kubiak; Marcin Lewandowski; Marek Kubis,Iwona Christop,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multimodal & Speech/Audio,"Reasoning, Planning & Agents",audio; multimodal; require reasoning; multimodal large language; multimodal large; testing; reasoning; require; benchmark assessing; solve problems
138-MAIN,CrossThink: Scaling Self-Learning beyond Math Reasoning,"Prior work has successfully applied Reinforcement Learning (RL) to mathematical reasoningвЂ”where rules and correctness are well-defined. Yet, generalizing these methods to broader reasoning domains remains challenging due to limited data and the lack of verifiable rewards for unstructured domains. In this work, we propose CrossThink, a framework that systematically incorporates multi-domain corpora into RL training to improve generalization across diverse reasoning tasks. CrossThink addresses key challenges by (1) combining data from varied sources; (2) applying structured templates to control answer-space complexity; (3) filtering for verifiable answers; and (4) optimizing data blending strategies to utilize multi-source data effectively. This enables scalable and verifiable reward modeling beyond math and demonstrates improved accuracies on both math (MATH-500: +30.1%, AMC23: +27.5%) and non-math reasoning benchmarks (MMLU-Pro: +12.8%, GPQA-Diamond: +11.3%, AGIEval: +15.1%, SuperGPQA: +3.8%). Moreover, CrossThink exhibits significantly improved response efficiencyвЂ”using 28% fewer tokens for correct answersвЂ”highlighting more focused and effective reasoning. Through CrossThink, we demonstrate that integrating multi-domain, multi-format data in RL leads to more accurate, efficient, and generalizable LLMs.",Syeda Nahida Akter; Shrimai Prabhumoye; Matvei Novikov; Seungju Han; Ying Lin; Evelina Bakhturina; Eric Nyberg; Yejin Choi; Mostofa Patwary; Mohammad Shoeybi; Bryan Catanzaro,Syeda Nahida Akter,Oral,In-person,SALLE  LE RIAD,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",math; verifiable; math reasoning; reasoning; multi domain; multi; improved; agieval; systematically incorporates; amc23
139-MAIN,"Safety of Large Language Models Beyond English: A Systematic Literature Review of Risks, Biases, and Safeguards","As Large Language Models (LLMs) continue to evolve, ensuring their safety across multiple languages has become a critical concern. While LLMs demonstrate impressive capabilities in English, their safety mechanisms may not generalize effectively to other languages, leading to disparities in toxicity detection, bias mitigation, and harm prevention. This systematic review examines the multilingual safety of LLMs by synthesizing findings from recent studies that evaluate their robustness across diverse linguistic and cultural contexts beyond English language. Our review explores the methodologies used to assess multilingual safety, identifies challenges such as dataset availability and evaluation biases. Based on our analysis we highlight gaps in multilingual safety research and provide recommendations for future work. This review aims to contribute to the development of fair and effective safety mechanisms for LLMs across all languages. We provide the extracted data in an interactive Streamlit dashboard, enabling transparent access to the raw data and allowing for continuous updates.",Aleksandra KrasnodД™bska; Katarzyna Dziewulska; Karolina Seweryn; Maciej Chrabaszcz; Wojciech Kusa,Aleksandra Krasnodębska,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Trustworthy, Safety, Privacy & Fairness",,safety; multilingual safety; review; safety mechanisms; multilingual; english; languages; biases; mechanisms; systematic
140-MAIN,InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection,"Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, limiting their effectiveness. We introduce InfiGUIAgent, an MLLM-based GUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1 enhances fundamental skills such as GUI understanding and grounding, while Stage 2 integrates hierarchical reasoning and expectation-reflection reasoning skills using synthesized data to enable native reasoning abilities of the agents. InfiGUIAgent achieves competitive performance on several GUI benchmarks, highlighting the impact of native reasoning skills in enhancing GUI interaction for automation tasks.",Yuhang Liu; Pengxiang Li; Zishu Wei; Congkai Xie; Xueyu Hu; Xinchen Xu; Shengyu Zhang; Xiaotian Han; Hongxia Yang; Fei Wu,Yuhang Liu,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",,gui; native; skills; reasoning; stage; reflection; reasoning skills; automation; agents; agent
141-MAIN,"Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish","We introduce Cetvel, a comprehensive benchmark designed to evaluate large language models (LLMs) in Turkish. Existing Turkish benchmarks often lack either task diversity or culturally relevant content, or both. Cetvel addresses these gaps by combining a broad range of both discriminative and generative tasks ensuring content that reflects the linguistic and cultural richness of Turkish language. Cetvel covers 23 tasks grouped into seven categories, including tasks such as grammatical error correction, machine translation, and question answering rooted in Turkish history and idiomatic language. We evaluate 33 open-weight LLMs (up to 70B parameters) covering different model families and instruction paradigms. Our experiments reveal that Turkish-centric instruction-tuned models generally underperform relative to multilingual or general-purpose models (e.g. Llama 3 and Mistral), despite being tailored for the language. Moreover, we show that tasks such as grammatical error correction and extractive question answering are particularly discriminative in differentiating model capabilities. Cetvel offers a comprehensive and culturally grounded evaluation suite for advancing the development and assessment of LLMs in Turkish.",Yakup Abrek Er; Ilker Kesen; GГ¶zde GГјl Ећahin; Aykut Erdem,Ilker Kesen,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Machine Translation,"LLM Evaluation, Benchmarks & Metrics",turkish; grammatical error; discriminative; grammatical; correction; culturally; tasks; cultural; error; instruction
142-MAIN,CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation,"Lexical semantics is concerned with both the multiple senses a word can adopt in different contexts, and the semantic relations that exist between meanings of different words. To investigate them, Contextualized Language Models are a valuable tool that provides context-sensitive representations that can be used to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the task of Word-in-Context to fine-tune them to get more semantically accurate representations, but Word-in-Context only compares occurrences of the same lemma, limiting the range of captured information. In this paper, we propose an extension, Concept Differentiation, to include inter-words scenarios. We provide a dataset for this task, derived from SemCor data. Then we fine-tune several representation models on this dataset. We call these models Concept-Aligned Embeddings (CALE). By challenging our models and other models on various lexical semantic tasks, we demonstrate that the proposed models provide efficient multi-purpose representations of lexical meaning that reach best performances in our experiments. We also show that CALE's fine-tuning brings valuable changes to the spatial organization of embeddings.",Bastien LiГ©tard; Gabriel Loiseau,Gabriel Loiseau,Oral,In-person,SALLE  LA PALMERAIE,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Linguistics, Syntax & Semantics",Interpretability & Model Analysis,lemma; lexical; word; concept; differentiation; embeddings; representations; fine tune; tune; inter
143-MAIN,Do NOT Classify and Count: Hybrid Attribute Control Success Evaluation,"Evaluating attribute control success in controllable text generation and related generation tasks typically relies on pretrained classifiers. We show that this widely used classify-and-count approach yields biased and inconsistent results, with estimates varying significantly across classifiers. We frame control success estimation as a quantification task and apply a hybrid Bayesian method that combines classifier predictions with a small number of human labels for calibration. To test our approach, we collected a two-modality test dataset consisting of 600 human-rated samples and 60,000 automatically rated samples. Our experiments show that our approach produces robust estimates of control success across both text and text-to-image generation tasks, offering a principled alternative to current evaluation practices.",Felix Matthias Saaro; Pius von DГ¤niken; Mark Cieliebak; Jan Milan Deriu,Pius von Däniken,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Summarization & Generation,"LLM Evaluation, Benchmarks & Metrics",control; success; rated; attribute control; count; generation tasks; classify; estimates; classifiers; attribute
145-MAIN,Detecting Training Data of Large Language Models via Expectation Maximization,"Membership inference attacks (MIAs) aim to determine whether a specific example was used to train a given language model. While prior work has explored prompt-based attacks such as ReCALL, these methods rely heavily on the assumption that using known non-members as prompts reliably suppresses the modelвЂ™s responses to non-member queries. We propose EM-MIA, a new membership inference approach that iteratively refines prefix effectiveness and membership scores using an expectation-maximization strategy without requiring labeled non-member examples. To support controlled evaluation, we introduce OLMoMIA, a benchmark that enables analysis of MIA robustness under systematically varied distributional overlap and difficulty. Experiments on WikiMIA and OLMoMIA show that EM-MIA outperforms existing baselines, particularly in settings with clear distributional separability. We highlight scenarios where EM-MIA succeeds in practical settings with partial distributional overlap, while failure cases expose fundamental limitations of current MIA methods under near-identical conditions. We will release our code and evaluation pipeline upon publication to encourage reproducible and robust MIA research.",Gyuwan Kim; Yang Li; Evangelia Spiliopoulou; Jie Ma; Miguel Ballesteros; William Yang Wang,Gyuwan Kim,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",mia; distributional; membership; maximization; non member; member; expectation; membership inference; non; overlap
150-MAIN,How effective are VLMs in assisting humans in inferring the quality of mental models from Multimodal short answers?,"Mental models can play a critical role in assessing studentsвЂ™ conceptual understanding of a topic. They not only offer insights into what students know but also into how effectively they can apply, relate to, and integrate concepts across various contexts. Thus, students' responses are critical markers of the quality of their understanding and not entities that should be merely graded. However, inferring mental models from student answers is challenging as it requires deep reasoning skills. We propose MMGrader, an approach that infers the quality of students' mental models from their multimodal responses using concept graphs as an analytical framework. In our evaluation with 9 openly available models, we found that the best-performing models fall short of human-level performance. This is because they only achieved an accuracy of approximately 40%, a prediction error of 1.1 units, and a scoring distribution fairly aligned with human scoring patterns. With improved accuracy, these can be highly effective assistants to teachers in inferring the mental models of their entire classrooms, enabling them to do so efficiently and help improve their pedagogies more effectively by designing targeted help sessions and lectures that strengthen areas where students collectively demonstrate lower proficiency.",Pritam Sil; DURGAPRASAD KARNAM; Vinay Reddy Venumuddala; Pushpak Bhattacharyya,Pritam Sil,Oral,In-person,SALLE  WALILI,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,mental; students; inferring; models multimodal; help; scoring; short; quality; answers; responses
154-MAIN,Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism,"The ability of Large Language Models (LLMs) to mimic human behavior triggered a plethora of computational social science research, assuming that empirical studies of humans can be conducted with AI agents instead. Since there have been conflicting research findings on whether and when this hypothesis holds, there is a need to better understand the differences in their experimental designs. We focus on replicating the behavior of social network users with the use of LLMs for the analysis of communication on social networks. First, we provide a formal framework for the simulation of social networks, before focusing on the sub-task of imitating user communication. We empirically test different approaches to imitate user behavior on in English and German. Our findings suggest that social simulations should be validated by their empirical realism measured in the setting in which the simulation components were fitted. With this paper, we argue for more rigor when applying generative-agent-based modeling for social simulation.",Simon MГјnker; Nils Schwager; Achim Rettinger,Simon Münker,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents",Interpretability & Model Analysis,social; social networks; networks; simulation; communication; realism; empirical; behavior; mimic; generative
155-MAIN,Persona Prompting as a Lens on LLM Social Reasoning,"For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.",Jing Yang; Moritz Hechtbauer; Elisabeth Khalilov; Evelyn Luise Brinkmann; Vera Schmitt; Nils Feldhus,Jing Yang,Oral,In-person,Pavillon  DE RABAT,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness",Multimodal & Speech/Audio,demographic; rationales; persona; socially sensitive; socially sensitive tasks; rationale quality; persona prompting; sensitive tasks; socially; personas
166-MAIN,PartisanLens: A Multilingual Dataset of Hyperpartisan and Conspiratorial Immigration Narratives in European Media,"Detecting hyperpartisan narratives and Population Replacement Conspiracy Theories is essential to addressing the spread of misinformation. However, existing resources are scarce, predominantly English-centric, and often analyze hyperpartisanship, stance, and rhetorical bias in isolation rather than as interrelated aspects of political discourse. To bridge this gap, we introduce PartisanLens, the first multilingual dataset of 1617 hyperpartisan news headlines in Spanish, Italian, and Portuguese. We first evaluate the classification performance of Large Language Models (LLMs) on this dataset, establishing new baselines and showing that LLMs outperform mBERT. In addition, we assess the viability of using LLMs as automatic annotators for this task, analyzing their ability to approximate human annotation. Results highlight both their potential and current limitations. Finally, moving beyond standard judgements, we explore whether LLMs can emulate human annotation patterns by conditioning them on socio-economic and ideological profiles that simulate annotator perspectives. By providing this resource and evaluation, PartisanLens supports future research on detecting partisan and conspiratorial narratives in European contexts.",Michele Joshua Maggini; Paloma Piot; Anxo PГ©rez; Erik Bran Marino; LГєa SantamarГ­a Montesinos; Ana Lisboa Cotovio; Marta VГЎzquez AbuГ­n; Javier Parapar; Pablo Gamallo,Michele Joshua Maggini; Paloma Piot;,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics",Multilinguality & Low-Resource NLP,narratives; multilingual dataset; european; human annotation; detecting; annotation; dataset; llms; portuguese; media detecting
169-MAIN,Progressive Visual Refinement for Multi-modal Summarization,"Multi-modal summarization (MMS) has emerged as a critical research area driven by the proliferation of multimedia content, focusing on generating condensed summaries by cross-modal complementary information synthesis. Previous studies have demonstrated the effectiveness of heterogeneous fusion paradigms, particularly through visual-centric feature extraction mechanisms, in constructing cross-modal representations that yield substantial performance gains. However, the use of multi-modal information and the inter-correlation among textual content, visual elements, and summary generation are still underestimated. We propose the Patch-Refined Visual Information Network (PRVIN) to address the insufficient exploitation of visual information. The essential patch selector and patch refiner components in PRVIN work collaboratively to progressively identify and refine critical visual features. An additional vision-to-summary alignment mechanism is also introduced to enhance the semantic connections between multi-modal representations and summary outputs. Extensive experiments conducted on two public MMS benchmark datasets demonstrate the superiority of PRVIN while quantitatively validating the crucial role of comprehensive visual information utilization in MMS tasks.\footnote{Our code will be available at",Ye Xiong; Hidetaka Kamigaito; Soichiro Murakami; Peinan Zhang; Hiroya Takamura; Manabu Okumura,YE XIONG,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Summarization & Generation,,modal; visual; multi modal; visual information; patch; summary; information; cross modal; multi; summarization
170-MAIN,Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties,"Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth, and involve a limited range of temporal entities. To address these limitations, we introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate LLMs on temporal reasoning within the extensive scope of Chinese dynastic chronology. CTM emphasizes cross-entity relationships, pairwise temporal alignment, and contextualized and culturally-grounded reasoning, providing a comprehensive evaluation. Extensive experimental results reveal the challenges posed by CTM and highlight potential avenues for improvement.",Zhenglin Wang; Jialong Wu; Pengfei LI; Yong Jiang; Deyu Zhou,Zhenglin Wang,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",temporal; temporal reasoning; chinese; reasoning; extensive; extensive experimental results; designed evaluate llms; alignment chinese; reasoning fundamental; temporal entities
171-MAIN,Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition,"Neuro-symbolic NLP methods aim to leverage the complementary strengths of large language models and formal logical solvers. However, current approaches are mostly static in nature, i.e., the integration of a target solver is predetermined at design time, hindering the ability to employ diverse formal inference strategies. To address this, we introduce an adaptive, multi-paradigm, neuro-symbolic inference framework that: (1) automatically identifies formal reasoning strategies from problems expressed in natural language; and (2) dynamically orchestrates a suite of specialized formal logical solvers via autoformalization interfaces. Extensive experiments on individual and multi-paradigm reasoning tasks support the following conclusions: LLMs are effective at predicting the necessary formal reasoning strategies with an accuracy above 90%, which enables effective integration across different formal paradigms, with the proposed framework outperforming competing baselines (outperform the second-best by 27% and 6% for GPT-4o and DeepSeek-V3.1, respectively). Moreover, adaptive reasoning can positively impact pure LLM methods, yielding significant improvements across zero-shot and CoT settings on GPT-4o, with gains of 10%, 5%, and 6% on zero-sho, CoT, and CoT$_{sym}$, respectively. Experiments reveal that smaller models still struggle with adaptive neuro-symbolic reasoning, yet limitations can be alleviated through post-training. Overall, this work establishes the foundations for adaptive LLM-based material-formal integration, offering a path forward for unifying material and formal inferences on heterogeneous reasoning challenges. The code and data are available online\footnote{\url{https://anonymous.4open.science/r/ACB4}}.",Lei Xu; Pierre Beckmann; Marco Valentino; Andre Freitas,Lei Xu,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",formal; adaptive; neuro symbolic; symbolic; neuro; reasoning; cot; logical; integration; reasoning strategies
177-MAIN,Lexical Popularity: Quantifying the Impact of Pre-training for LLM Performance,"Large Language Models (LLMs) excel in numerous and varied tasks. Yet, the mechanisms that underlie this success remain insufficiently understood. In particular, the size and the limited transparency of their pre-training materials make it difficult to state what the properties of the pre-training material are when compared to the test data. In this paper, we investigate whether LLMs learned generalized linguistic abstraction or rely on surface-level features, like lexical patterns, that match their pre-training data. We explore this by examining the relationship between lexical overlap of test data and task performance. We observe that lexical overlap with the pre-training material is mostly beneficial to model performance on tasks requiring functional linguistic knowledge. To further explore the impact of lexical features, we also demonstrate that LLMs are fragile with respect to lexical perturbations that preserve semantics. While we expected models to rely on lexical overlap between test instances and pre-training data for tasks requiring functional knowledge, lexical perturbations reveal that models also exhibit, to a lesser extent, this dependence for tasks requiring formal linguistic knowledge.",Elena Sofia Ruzzetti; Fabio Massimo Zanzotto; Tommaso Caselli,Fabio Massimo Zanzotto,Oral,In-person,SALLE  LA PALMERAIE,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Linguistics, Syntax & Semantics",Domain NLP (Biomedical/Clinical/Legal/Scientific),lexical; pre training; pre; tasks requiring; overlap; training; pre training data; linguistic knowledge; test data; requiring
184-MAIN,Training in Step-by-Step Formal Reasoning Improves Pronominal Reasoning in Language Models,"Large reasoning models, which are trained to solve problems by decomposing them into steps, show impressive progress on reasoning tasks, but ``reasoning'' here is typically limited to formal reasoning, i.e., math, code, and logic. An open question is whether these abilities transfer to pronominal reasoning, where step-by-step thinking in non-reasoning models worsens performance, but code pre-training may help. We answer this question by evaluating six pairs of original and DeepSeek-distilled models ($1.5$B--$7$B parameters) on six challenging datasets for English pronoun resolution (identifying whom a pronoun refers to) and pronoun fidelity (learning and applying a pronoun mapping correctly). Performance improves statistically significantly on five of six datasets ($31$% relative increase), indicating that distilling step-by-step formal reasoning does in fact help with pronominal reasoning. With a qualitative evaluation of $720$ generations, we show that improvements are partly due to improved instruction-following and mostly due to plausible-looking reasoning. However, the gains put models just above random performance on these datasets, leaving plenty of room for improvement.",Vagrant Gautam,Vagrant Gautam,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents",Multilinguality & Low-Resource NLP,pronoun; reasoning; step; formal reasoning; step step; formal; help; reasoning models; datasets; worsens
188-MAIN,Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification,"Large language models (LLMs) are increasingly applied to tasks involving causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that fail to assess whether a modelвЂ™s output is formally valid under causal semantics. We propose DoVerifier, a symbolic verification framework that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers that would otherwise be marked incorrect due to superficial differences. Evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness than standard metrics, offering a more rigorous and informative way to evaluate LLMs on causal tasks.",Paul He; Yinya Huang; Mrinmaya Sachan; Zhijing Jin,Paul He,Oral,In-person,SALLE  LE RIAD,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Linguistics, Syntax & Semantics","Reasoning, Planning & Agents",causal; causal reasoning; symbolic; correctness; verification; metrics; reasoning current benchmarks; reasoning symbolic; correctness llm; verification large
192-MAIN,When Words Wear Masks: Detecting Malicious Intents and Hostile Impacts of Online Hate Speech,"Hate speech on social media poses significant challenges for content moderation and user safety. While various datasets exist for hate speech detection, existing approaches treat hate speech as a monolithic phenomenon, detecting hateful content by using simple categorical labels such as hate, offensive, or toxic. This approach fails to distinguish between the speaker's underlying motivations and the content's potential societal consequences. This paper introduces I2-Hate, a novel dataset with a dual taxonomy that separately captures Intent (why the speaker produced hate speech) and Impact (what harm it may cause to individuals and communities) of online hateful posts. This dual-taxonomy approach enables moderation systems to differentiate hateful content based on underlying motivation and potential harm, supporting more nuanced intervention strategies. We release the code publicly.",Priyansh Singhal; Piyush Joshi,Priyansh Singhal,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Multimodal & Speech/Audio,hate; hate speech; speech; hateful; content; harm; speaker; moderation; dual; taxonomy
195-MAIN,CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures,"Game-theoretic interactions between agents with large language models (LLMs) have revealed many emergent capabilities, yet the linguistic diversity of these interactions has not been sufficiently quantified. In this paper, we present the Conversational Robustness Evaluation Score: CORE, a metric to quantify the effectiveness of language use within multi-agent systems across different game-theoretic interactions. CORE integrates measures of cluster entropy, lexical repetition, and semantic similarity, providing a direct lens of dialog quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws to characterize word frequency distributions and vocabulary growth. Our findings show that cooperative settings exhibit both steeper Zipf distributions and higher Heap exponents, indicating more repetition alongside greater vocabulary expansion. In contrast, competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies. These results provide new insights into how social incentives influence language adaptation, and highlight CORE as a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems.",Punya Syon Pandey; Yongjin Yang; Jiarui Liu; Zhijing Jin,Punya Syon Pandey,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",core; repetition; game; theoretic; interactions; multi agent llm; agent llm; cooperative; multi agent; agent
197-MAIN,Improving German-language LLM pre-training with model-based data curation and synthetic data generation,"Scaling data quantity is essential for large language models (LLMs), yet recent findings show that data quality can significantly boost performance and training efficiency. We introduce a German-language dataset curation pipeline that combines heuristic and model-based filtering techniques with synthetic data generation. We use our pipeline to create GermanWeb, a large-scale German pre-training dataset which draws from: (1) Common Crawl web data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual, organic web data. We evaluate our dataset by pre-training both a 1B Llama-style model and an 8B tokeniser-free hierarchical autoregressive transformer (HAT). A comparison on German-language benchmarks, including MMMLU, shows significant performance gains of GermanWeb over FineWeb2 alone. This advantage holds at the 8B scale even when FineWeb2 is enriched by human-curated high-quality data sources such as Wikipedia. Our findings support the growing body of evidence that model-based data curation and synthetic data generation can significantly enhance LLM pre-training datasets.",Thomas F Burns; Letitia Parcalabescu; Stephan Waeldchen; Michael Barlow; Gregor Ziegltrum; Volker Stampa; Bastian Harren; Björn Deiseroth,Thomas F Burns,Oral,In-person,SALLE  LA PALMERAIE,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Summarization & Generation,"Efficiency, Scaling & NLP Systems",german; pre training; curation; model based; synthetic data generation; data generation; pre; synthetic data; llm pre; llm pre training
199-MAIN,Ultra-Low-Dimensional Prompt Tuning via Random Projection,"Large language models achieve state-of-the-art performance but are increasingly costly to fine-tune. Parameter-efficient fine-tuning methods like prompt tuning address this by learning prompt embeddings, but these are typically tied to the modelвЂ™s dimensionality, limiting efficiency for larger or customized LLMs. In this paper, we propose Ultra-Low-dimensional Prompt Tuning (ULPT), a simple yet effective method that optimizes prompts in a low-dimensional space (e.g., 2D) and use a frozen random matrix for up-projection. ULPT achieves up to 98% reduction in the tunable parameters compared to vanilla prompt tuning while preserving performance. Extensive experiments across 20+ NLP tasks demonstrate that ULPT consistently outperforms recent parameter-efficient tuning methods while using significantly fewer parameters, making it well-suited for scalable, storage-efficient LLM customization.",Zijun Wu; Yongchang Hao; Lili Mou,Zijun Wu,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Efficiency, Scaling & NLP Systems",,low dimensional; prompt; tuning; dimensional; projection; tuning methods; parameter efficient; random; low; efficient
201-MAIN,NP-Hard Lower Bound Complexity for Semantic Self-Verification,"We model Semantic Self-Verification (SSV) as the problem of determining whether a statement accurately characterizes its own semantic properties within a given interpretive framework that formalizes a challenge in AI safety and fairness: can an AI system verify that it has correctly interpreted rules intended to govern its behavior? We prove that SSV, in this specification, is NP-complete by constructing a polynomial-time reduction from 3-Satisfiability (3-SAT). Our reduction maps a 3-SAT formula to an instance of SSV involving ambiguous terms with binary interpretations and semantic constraints derived from logical clauses. This establishes that even simplified forms of semantic self-verification should face computational barriers. The NP-complete lower bound has implications for AI safety and fairness approaches that rely on semantic interpretation of instructions, including but not limited to constitutional AI, alignment via natural language, and instruction-following systems. Any approach where an AI system verify its understanding of directives faces this computational barrier. We argue that more realistic verification scenarios likely face even greater complexity.",Robin Young,Robin Young,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",,self verification; semantic; verification; ai safety; np complete; safety fairness; lower bound; self; bound; fairness
207-MAIN,STAMP: Selective Task-Aware Mechanism for Text Privacy,"We present STAMP (Selective Task-Aware Mechanism for Text Privacy), a new framework for task-aware text privatization that achieves an improved privacyвЂ“utility trade-off. STAMP selectively allocates privacy budgets across tokens by jointly considering (i) each tokenвЂ™s importance to the downstream task (as measured via a task- or query-specific representation), and (ii) its privacy sensitivity (e.g., names, dates, identifiers). This token-level partitioning enables fine-grained, group-wise control over the level of noise applied to different parts of the input, balancing privacy protection with task relevance. To privatize individual token embeddings, we introduce the polar mechanism, which perturbs only the direction of embeddings on the unit sphere while preserving their magnitude. Decoding is performed via cosine nearest-neighbor search, aligning the perturbation geometry with the decoding geometry. Unlike isotropic noise mechanisms, the polar mechanism maintains semantic neighborhoods in the embedding space and better preserves downstream utility. Experimental evaluations on SQuAD, Yelp, and AG News datasets demonstrate that STAMP, when combined with the normalized polar mechanism, consistently achieves superior privacyвЂ“utility trade-offs across varying per-token privacy budgets.",Fengwei Tian; Payel Bhattacharjee; Heidi Hanson; Geoffrey D Rubin; Joseph Y. Lo; Ravi Tandon,Fengwei Tian,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,privacy; polar; mechanism; task aware; privacyвђ utility trade; privacyвђ utility; privacyвђ; utility trade; utility; budgets
208-MAIN,Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities,"Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.",Alberto Purpura; Li Wang; Sahil Badyal; Gene Beaufrand; Adam Faulkner,Sahil Badyal,Oral,In-person,SALLE  LE RIAD,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",Interpretability & Model Analysis,granular; compliance; instructions; new benchmark; capability; instruction; language model instruction; model instruction; recency; instruction compliance
210-MAIN,Utterance-level Detection Framework for LLM-Involved Content Detection in Conversational Setting,"As Large Language Models(LLMs) increasingly power chatbots, social media, and other interactive platforms, the ability to detect AI in conversational settings is critical for ensuring transparency and preventing potential misuse. However, existing detection methods focus on static, document-level content, overlooking the dynamic nature of dialogues. To address this, we propose an utterance-level detection framework, which integrates features from individual and combined analysis of dialogue participants' responses to detect LLM-generated text under conversational setting. Leveraging a transformer-based recurrent architecture and a curated dataset of human-human, human-LLM, and LLM-LLM dialogues, this framework achieves an accuracy of 98.14% with high inference speed, supported by extensive results of experiments on different models and settings. This work provides an effective solution for detecting LLM-generated text in real-time conversations, promoting transparency, and mitigating risks of misuse.",Muyang Zhou; Huaxia Rui,Muyang Zhou,Oral,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP",,detection; conversational; human human; level detection; misuse; llm generated text; detection framework; llm; utterance; dialogues
211-MAIN,ClinSQL: A Challenging Benchmark for Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL,"Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce ClinSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving ClinSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 20 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on ClinSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.",Yifei Shen; Yilun Zhao; Justice Ou; Tinglin Huang; Arman Cohan,Yifei Shen,Oral,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Reasoning, Planning & Agents",sql; text sql; clinical; clinical text; ehr; clinically; executable; text; patient; execution
215-MAIN,Lost in Activations: A Neuron-level Analysis of Encoders for Cross-Lingual Emotion Detection,"The rapid advancement of multilingual pre-trained transformers has fueled significant progress in natural language understanding across diverse languages. Yet, their inner workings remain opaque, especially with regard to how individual neurons encode and generalize semantic and affective features across languages. This paper presents an interpretability study of a fine-tuned XLM-R model for multilingual emotion classification. Using neuron-level activation analysis, we investigate the variance of neurons across labels, cross-lingual alignment of activations, and the existence of вЂњpolyglotвЂќ versus language-specific neurons. Our results reveal that while certain neurons consistently encode emotion-related concepts across languages, others show strong monolingual specialization.",Pranaydeep Singh; Orphee De Clercq; Els Lefever,Pranaydeep Singh,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multilinguality & Low-Resource NLP,Interpretability & Model Analysis,neurons; emotion; neuron level; neuron; activations; encode; languages; cross lingual; lingual; rapid advancement
216-MAIN,iBERT: Interpretable Style Embeddings via Sense Decomposition,"We present iBERT (interpretable-BERT), an encoder to produce inherently interpretable and controllable embeddings - designed to modularize and expose the discriminative cues present in language, such as stylistic and semantic structure. Each input token is represented as a sparse, nonвЂ‘negative mixture over k contextвЂ‘independent sense vectors, which can be pooled into sentence embeddings or used directly at the token level. This enables modular control over representation, before any decoding or downstream use. To demonstrate our model's interpretability, we evaluate it on a suite of styleвЂ‘focused tasks. On the STEL benchmark, it improves style representation effectiveness by ~8 points over SBERT-style baselines (prior SOTA), while maintaining competitive performance on authorship verification. Because each embedding is a structured composition of interpretable senses, we highlight how specific style attributes - such as emoji use, formality, or misspelling can be assigned to specific sense vectors. While our experiments center on style, iBERT is not limited to stylistic modeling. Its structural modularity is designed to interpretably decompose whichever discriminative signals are present in the data вЂ” enabling generalization even when supervision blends stylistic and semantic factors.",Vishal Anand; Milad Alshomary; Kathleen McKeown,Vishal Anand,Oral,In-person,SALLE  LA PALMERAIE,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Interpretability & Model Analysis,"LLM Evaluation, Benchmarks & Metrics",style; stylistic; interpretable; sense; sense vectors; embeddings; discriminative; vectors; present; representation
219-MAIN,Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World,"This paper investigates the real-world vulnerabilities of audio-based large language models (ALLMs), such as Qwen2-Audio. We first demonstrate that an adversary can craft stealthy audio perturbations to manipulate ALLMs into exhibiting specific targeted behaviors, such as eliciting responses to wake-keywords (e.g., ""Hey Qwen""), or triggering harmful behaviors (e.g., ""Change my calendar event""). Subsequently, we show that playing adversarial background noise during user interaction with the ALLMs can significantly degrade the response quality. Crucially, our research illustrates the scalability of these attacks to real-world scenarios, impacting other innocent users when these adversarial noises are played through the air. Further, we discuss the transferability of the attack and potential defensive measures.",Vinu Sankar Sadasivan; Soheil Feizi; Rajiv Mathews; Lun Wang,Vinu Sankar Sadasivan,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Trustworthy, Safety, Privacy & Fairness",audio; manipulate; noise; real world; world; behaviors; real; adversarial; calendar; hey
224-MAIN,Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework,"Large language models (LLMs) are highly sensitive to subtle changes in prompt phrasing, posing challenges for reliable auditing. Prior methods often apply unconstrained prompt paraphrasing, which risk missing linguistic and demographic factors that shape authentic user interactions. We introduce AUGMENT (Automated User-Grounded Modeling and Evaluation of Natural Language Transformations), a framework for generating controlled paraphrases, grounded in user behaviors. AUGMENT leverages linguistically informed rules and enforces quality through checks on instruction adherence, semantic similarity, and realism, ensuring paraphrases are both reliable and meaningful for auditing. Through case studies on the BBQ and MMLU datasets, we show that controlled paraphrases uncover systematic weaknesses that remain obscured under unconstrained variation. These results highlight the value of the AUGMENT framework for reliable auditing.",Clea Chataigner; Rebecca Ma; Prakhar Ganesh; Yuhao Chen; Afaf Taik; Elliot Creager; Golnoosh Farnadi,Clea Chataigner; Rebecca Ma,Oral,In-person,Pavillon  DE RABAT,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Linguistics, Syntax & Semantics",auditing; augment; paraphrases; user; paraphrasing; grounded; reliable; controlled; automated; prompt
229-MAIN,AutoBool: Reinforcement-Learned LLM for Effective Automatic Systematic Reviews Boolean Query Generation,"We present AutoBool, a reinforcement learning (RL) framework that trains large language models (LLMs) to generate effective Boolean queries for medical systematic reviews. Boolean queries are the primary mechanism for literature retrieval in this domain and must achieve high recall while maintaining reasonable precision---a challenging balance that existing prompt-based LLM approaches often struggle to achieve. A major limitation in this space is the lack of ground-truth best Boolean queries for each topic, which makes supervised fine-tuning impractical. AutoBool addresses this challenge by leveraging RL to directly optimize query generation against retrieval performance metrics, without requiring ideal target queries. To support this effort, we create and release the largest dataset of its kind: 65 588 topics in total for training and evaluating the task of automatic Boolean query formulation. Experiments on our new dataset and two established datasets (CLEF TAR and Seed Collection) show that AutoBool significantly outperforms zero-shot/few-shot prompting and matches or exceeds the effectiveness of much larger GPT-based models (e.g., GPT-4o, O3) using smaller backbones. It also approaches effectiveness of expert-authored queries while retrieving 10вЂ“16 times fewer documents. Ablation studies reveal the critical roles of model backbone, size, decoding temperature, and prompt design. Code and data are available at https://anonymous.4open.science/r/AutoBool-B3E5/.",Shuai Wang; Harrisen Scells; Bevan Koopman; Guido Zuccon,Guido Zuccon,Oral,In-person,Pavillon  DE RABAT,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,queries; query generation; query; reviews; reinforcement; automatic; gpt; shot; achieve; systematic
235-MAIN,McMining: Automated Discovery of Misconceptions in Student Code,"When learning to code, students often develop misconceptions about various programming language concepts. These can not only lead to bugs or inefficient code, but also slow down the learning of related concepts. In this paper, we introduce McMining, the task of mining programming misconceptions from samples of code from a student. To enable the training and evaluation of McMining systems, we develop an extensible benchmark dataset of misconceptions together with a large set of code samples where these misconceptions are manifested. We then introduce two LLM-based McMiner approaches and, through extensive evaluations, show that models from the Gemini, Claude, and GPT families are effective at discovering misconceptions in student code.",Erfan Al-Hossami; Razvan Bunescu,Razvan Bunescu,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",misconceptions; code; student; programming; concepts; develop; samples; related concepts; introduce llm based; evaluations models
236-MAIN,Improving LLM Domain Certification with Pretrained Guide Models,"Large language models (LLMs) often generate off-domain or harmful responses when deployed in specialized, high-stakes domains, motivating the need for rigorous LLM domain certification. While the VALID algorithm (Emde et al., 2025) achieves formal domain certificate guarantee using a guide model $G$ trained from scratch on in-domain data, it suffers from poor generalization due to limited training. In this work, we propose PRISM, a novel approach that overcomes this key limitation by leveraging pretrained language models as guide models, enhanced via contrastive fine-tuning to sharply distinguish acceptable from refused content. We explore and experiment variants of PRISM with different loss functions to ensure that the model exploits the rich world knowledge of pretrained models while aligned to the target domain. We show that two variants of PRISM, PRISM-BC and PRISM-GA, achieve superior OOD rejection and tighter certification bounds across eight diverse data regimes and perturbations, establishing a more reliable approach to domain-adherent LLM deployment.",Jiaqian Zhang; Zhaozhi Qian; Faroq AL-Tam; Ignacio Iacobacci; Muhammad AL-Qurishi; Riad Souissi,Jiaqian Zhang,Oral,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),,prism; domain; pretrained; guide models; guide; variants; contrastive fine tuning; establishing reliable; perturbations establishing; tighter
238-MAIN,TDFlow: Agentic Workflows for Test Driven Development,"We introduce TDFlow, a novel test-driven agentic workflow that frames repository-scale software engineering as a test-resolution task, specifically designed to solve human-written tests. Given a set of tests, TDFlow repeatedly proposes, revises, and debugs repository-scale patches using precisely engineered sub-agents and tightly constrained tools. The workflow decomposes software engineering program repair into four components governed by respective sub-agents. This simple, forced decoupling of patch proposing, debugging, patch revision, and optional test generation (1) reduces long-context burden on any individual sub-agent, (2) focuses each sub-agent on specific, pre-defined sub-tasks, and (3) allows for specialized performance improvement on specific sub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on SWE-Bench Lite (an absolute improvement of 27.8% over the next best baseline) and 94.3% on SWE-Bench Verified. In this work, we further show that the primary obstacle to human-level software engineering performance lies within writing successful reproduction tests. Manual inspection of the 800 TDFlow runs within SWE-Bench Lite and Verified uncover only 7 instances of test hacking, which were subsequently counted as failures. We envision a human-LLM interactive system powered by TDFlow where human developers write tests solved by LLM systems. Together, these results show that modern LLMs, when embedded in a narrowly engineered, test-driven workflow, already achieve human-level test resolution -- with the final frontier for fully autonomous repository repair being accurate reproduction test generation.",Kevin Han; Siddharth Maddikayala; Tim Knappe; Om Patel; Austen Liao; Amir Barati Farimani,Kevin Han,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Dialogue, Conversational & Interactive NLP",sub; test; tests; swe bench; swe; software engineering; software; repository; workflow; bench
239-MAIN,Contrastive Learning with Narrative Twins for Modeling Story Salience,"Understanding narratives requires identifying which events are most salient for a story's progression. We present a contrastive learning framework for modeling narrative salience that learns story embeddings from narrative twins: stories that share the same plot but differ in surface form. Our model is trained to distinguish a story from both its narrative twin and a distractor with similar surface features but different plot. Using the resulting embeddings, we evaluate four narratologically motivated operations for inferring salience (deletion, shifting, disruption, and summarization). Experiments on short narratives (ROCStories) and longer Wikipedia plot summaries show that contrastively learned story embeddings outperform a masked-language-model baseline, and that summarization is the most reliable operation for identifying salient sentences. If retellings are not available, random dropout can be used to generate narrative twins from a single story. Effective distractors can be obtained either by prompting LLMs or, in long-form narratives, by using different parts of the same story.",Igor Sterner; Alex Lascarides; Frank Keller,Igor Sterner,Oral,In-person,SALLE  LE LIXUS,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Summarization & Generation,,story; narrative; narratives; embeddings; salient; contrastive learning; surface; summarization; contrastive; identifying
241-MAIN,ExAnte: A Benchmark for Ex-Ante Inference in Large Language Models,"Large language models (LLMs) struggle with ex-ante reasoningвЂ”making inferences or predictions without access to future information. Even under explicit temporal cutoffs, they often rely on internalized post-cutoff knowledge. To systematically evaluate this issue, we introduce a benchmark that assesses LLMsвЂ™ ex-ante inference ability across four tasks: stock prediction, question answering, Wikipedia event generation, and scientific publication generation. We quantify temporal leakage using a leakage rate metric, which measures modelsвЂ™ reliance on future information beyond cutoff timestamps, and a quality measure that evaluates task performance. Experimental results show that LLMs frequently violate temporal constraints across tasks, revealing persistent challenges in ex-ante reasoning. Our benchmark serves as a rigorous testbed for studying temporal reasoning in time-sensitive contexts and provides complete datasets, results, and evaluation resources to support future research on improving temporal consistency in modern LLMs.",Yachuan Liu; Xiaochun Wei; Lin Shi; Xinnuo Li; Bohan Zhang; Paramveer Dhillon; Qiaozhu Mei,Yachuan Liu,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",,temporal; future; leakage; benchmark; issue introduce; introduce benchmark; benchmark assesses; violate; stock; improving temporal consistency
243-MAIN,CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection,"Detecting mental health crisis situations such as suicide ideation, rape, domestic violence, child abuse, and sexual harassment is a critical yet underexplored challenge for language models. When such situations arise during userвЂ“model interactions, models must reliably flag them, as failure to do so can have serious consequences. In this work, we introduce \textsc{CRADLE BENCH}, a benchmark for multi-faceted crisis detection. Unlike previous efforts that focus on a limited set of crisis types, our benchmark covers seven types defined in line with clinical standards and is the first to incorporate temporal labels. Our benchmark provides 600 clinician-annotated evaluation examples and 420 development examples, together with a training corpus of around 4K examples automatically labeled using a majority-vote ensemble of multiple language models, which significantly outperforms single-model annotation. We further fine-tune six crisis detection models on subsets defined by consensus and unanimous ensemble agreement, providing complementary models trained under different agreement criteria. \noindent \textit{\textbf{Content warning:} This paper discusses sensitive topics such as suicide ideation, self-harm, rape, domestic violence, and child abuse.}",Grace Byun; Rebecca Lipschutz; SEAN T. MINTON; Abigail Powers; Jinho D. Choi,Grace Byun,Oral,In-person,SALLE  WALILI,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",crisis; violence; examples; clinician; ideation; multi faceted; situations; faceted; child; benchmark multi
245-MAIN,Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly,"There has been considerable interest in using surprisal from Transformer-based language models (LMs) as predictors of human sentence processing difficulty. Recent work has observed an inverse scaling relationship between Transformers' per-word estimated probability and the predictive power of their surprisal estimates on reading times, showing that LMs with more parameters and trained on more data are less predictive of human reading times. However, these studies focused on predicting latency-based measures. Tests on brain imaging data have not shown a trend in any direction when using a relatively small set of LMs, leaving open the possibility that the inverse scaling phenomenon is constrained to latency data. This study therefore conducted a more comprehensive evaluation using surprisal estimates from 17 pre-trained LMs across three different LM families on two functional magnetic resonance imaging (fMRI) datasets. Results show that the inverse scaling relationship between models' per-word estimated probability and model fit on both datasets still obtains, resolving the inconclusive results of previous work and indicating that this trend is not specific to latency-based measures.",Yi-Chien Lin; William Schuler,Yi-Chien Lin,Oral,In-person,Pavillon  DE RABAT,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Efficiency, Scaling & NLP Systems","LLM Evaluation, Benchmarks & Metrics",surprisal; lms; inverse scaling; inverse; scaling; latency; transformer based language; surprisal estimates; reading times; based measures
250-MAIN,Coordinates from Context: Using LLMs to Ground Complex Location References,"Geocoding is the task of linking a location reference to an actual geographic location and is essential for many downstream analyses of unstructured text. In this paper, we explore the challenging setting of geocoding compositional location references. Building on recent work demonstrating LLMs' abilities to reason over geospatial data, we evaluate LLMs' geospatial knowledge versus reasoning skills relevant to our task. Based on these insights, we propose an LLM-based strategy for geocoding compositional location references. We show that our approach improves performance for the task and that a relatively small fine-tuned LLM can achieve comparable performance with much larger off-the-shelf models.",Tessa Masis; Brendan O'Connor,Tessa Masis,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents",,location; references; geospatial; compositional; relatively small; llms ground; insights propose; challenging setting; llms abilities; geospatial data
254-MAIN,Discourse Graph Guided Document Translation with Large Language Models,"Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.",Viet Thanh Pham; Minghan Wang; Hao-Han Liao; Thuy-Trang Vu,Thuy-Trang Vu,Oral,Virtual,ZOOM,Virtual TBA,,,Machine Translation,"Retrieval, Grounding & External Knowledge (RAG)",translation; discourse; document; memory; graph; guided; mt benchmarks spanning; guided framework; explicitly models; context document
259-MAIN,StarFlow: Generating Structured Workflow Outputs From Sketch Images,"Workflows are a fundamental component of automation in enterprise platforms, enabling the orchestration of tasks, data processing, and system integrations. Despite being widely used, building workflows can be complex, often requiring manual configuration through low-code platforms or visual programming tools. To simplify this process, we explore the use of generative foundation models, particularly vision-language models (VLMs), to automatically generate structured workflows from visual inputs. Translating hand-drawn sketches or computer-generated diagrams into executable workflows is challenging due to the ambiguity of free-form drawings, variations in diagram styles, and the difficulty of inferring execution logic from visual elements. To address this, we introduce StarFlow, a framework for generating structured workflow outputs from sketches using vision-language models. We curate a diverse dataset of workflow diagrams -- including synthetic, manually annotated, and real-world samples -- to enable robust training and evaluation. We finetune and benchmark multiple vision-language models, conducting a series of ablation studies to analyze the strengths and limitations of our approach. Our results show that finetuning significantly enhances structured workflow generation, outperforming large vision-language models on this task.",Patrice Bechard; Chao Wang; Amirhossein Abaskohi; Juan A. Rodriguez; Christopher Pal; David Vazquez; Spandana Gella; Sai Rajeswar; Perouz Taslakian,Patrice Bechard,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,workflow; workflows; vision language models; vision language; vision; structured; generating structured; diagrams; platforms; visual
263-MAIN,Adaptive HelpfulnessвЂ“Harmlessness Alignment with Preference Vectors,"Ensuring that large language models (LLMs) are both helpful and harmless is a critical challenge, as overly strict constraints can lead to excessive refusals, while permissive models risk generating harmful content. Existing approaches, such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), attempt to balance these trade-offs but suffer from performance conflicts, limited controllability, and poor extendability. To address these issues, we propose Preference Vector, a novel framework inspired by task arithmetic. Instead of optimizing multiple preferences within a single objective, we train separate models on individual preferences, extract behavior shifts as preference vectors, and dynamically merge them at test time. This modular approach enables fine-grained, user-controllable preference adjustments and facilitates seamless integration of new preferences without retraining. Experiments show that our proposed Preference Vector framework improves helpfulness without excessive conservatism, allows smooth control over preference trade-offs, and supports scalable multi-preference alignment.",Ren-Wei Liang; Chin Ting Hsu; Chan-Hung Yu; Saransh Agrawal; Shih-Cheng Huang; Chieh-Yen Lin; Shang-Tse Chen; Kuan-Hao Huang; Shao-Hua Sun,Ren-Wei Liang,Oral,In-person,SALLE  LE RIAD,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Dialogue, Conversational & Interactive NLP","Trustworthy, Safety, Privacy & Fairness",preference; preferences; excessive; trade offs; offs; vector; vectors; trade; strict constraints; models risk
266-MAIN,How Reliable are Confidence Estimators for Large Reasoning Models? A Systematic Benchmark on High-Stakes Domains,"The miscalibration of Large Reasoning Models (LRMs) undermines their reliability in high-stakes domains, necessitating methods to accurately estimate the confidence of their long-form, multi-step outputs. To address this gap, we introduce the Reasoning Model Confidence estimation Benchmark (RMCB), a public resource of 347,496 reasoning traces from six popular LRMs across different architectural families. The benchmark is constructed from a diverse suite of datasets spanning high-stakes domainsвЂ”including clinical, financial, legal, and mathematical reasoningвЂ”alongside complex general reasoning benchmarks, with correctness annotated at both the final answer and intermediate chunk levels. Using RMCB, we conduct a large-scale empirical evaluation of over ten distinct representation-based methods, spanning sequential, graph-based, and text-based architectures. Our central finding is a persistent trade-off between discrimination (AUROC) and calibration (ECE): text-based encoders achieve the best AUROC (0.672), while structurally-aware models yield the best ECE (0.148), with no single method dominating both. Furthermore, we find that increased architectural complexity does not reliably outperform simpler sequential baselines, suggesting a performance ceiling for methods relying solely on chunk-level hidden states. This work provides the most comprehensive benchmark for this task to date, establishing rigorous baselines and demonstrating the limitations of current representation-based paradigms.",Reza Khanmohammadi; Erfan Miahi; Simerjot Kaur; Charese Smiley; Ivan Brugere; Kundan S Thind; Mohammad M. Ghassemi,Reza Khanmohammadi,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Domain NLP (Biomedical/Clinical/Legal/Scientific),stakes; high stakes; auroc; representation based; confidence; chunk; lrms; text based; stakes domains; high stakes domains
269-MAIN,WebRollback: Enhancing Web Agents with Explicit Rollback Mechanisms,"With recent advancements in large language models, web agents have been greatly improved. However, dealing with complex and dynamic web environments requires more advanced planning and search abilities. Previous studies usually adopt a greedy one-way search strategy, which may struggle to recover from erroneous states. In this work, we enhance web agents with an explicit rollback mechanism, enabling the agent to revert back to a previous state in its navigation trajectory. This mechanism gives the model the flexibility to directly control the search process, leading to an effective and efficient web navigation method. We conduct experiments on two live web navigation benchmarks with zero-shot and fine-tuning settings. The results demonstrate the effectiveness of our proposed approach.",Zhisong Zhang; Tianqing Fang; Kaixin Ma; Wenhao Yu; Hongming Zhang; Haitao Mi; Dong Yu,Zhisong Zhang,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",,web; web agents; navigation; web navigation; search; agents; explicit; previous; mechanism; dealing
270-MAIN,SearchLLM: Detecting LLM Paraphrased Text by Measuring the Similarity with Regeneration of the Candidate Source via Search Engine,"With the advent of large language models (LLMs), it has become common practice for users to draft text and utilize LLMs to enhance its quality through paraphrasing. However, this process can sometimes result in the loss or distortion of the original intended meaning. Due to the human-like quality of LLM-generated text, traditional detection methods often fail, particularly when text is paraphrased to closely mimic original content. In response to these challenges, we propose a novel approach named SearchLLM, designed to identify LLM-paraphrased text by leveraging search engine capabilities to locate potential original text sources. By analyzing similarities between the input and regenerated versions of candidate sources, SearchLLM effectively distinguishes LLM-paraphrased content. SearchLLM is designed as a proxy layer, allowing seamless integration with existing detectors to enhance their performance. Experimental results across various LLMs demonstrate that SearchLLM consistently enhances the accuracy of recent detectors in detecting LLM-paraphrased text that closely mimics original content. Furthermore, SearchLLM also helps the detectors prevent paraphrasing attacks.",Hoang-Quoc Nguyen-Son; Minh-Son Dao; Koji Zettsu,Nguyen Son Hoang Quoc,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Reasoning, Planning & Agents",,paraphrased; original; text; detectors; search engine; detecting llm; engine; paraphrasing; closely; llm
271-MAIN,RoZO: Geometry-Aware Zeroth-Order Fine-Tuning on Low-Rank Adapters for Black-Box Large Language Models,"Large language models (LLMs) have achieved remarkable success across a wide range of tasks, yet fine-tuning them efficiently under black-box or memory-constrained settings remains challenging. Parameter-efficient fine-tuning (PEFT) techniques such as LoRA alleviate memory usage by restricting updates to low-rank adapters, while zeroth-order (ZO) optimization further avoids back-propagation by estimating gradients from function evaluations. Recent work, such as LOZO, leverages random low-rank perturbations to reduce the variance of ZO estimates, but it overlooks the intrinsic geometric structure of LoRA adapters and suffers from unstable convergence and limited integration with adaptive optimizers. To address these limitations, we propose RoZO, a Riemannian zeroth-order optimization framework that constrains updates to the tangent space of the LoRA manifold. By exploiting geometry-aware updates with parallel transport, adaptive preconditioning, and trust-region control, RoZO achieves more stable convergence, tighter variance bounds, and superior performance compared to existing ZO methods.",Zichen Song; Weijia Li,Zichen Song,Oral,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",,zeroth order; zeroth; adapters; low rank; lora; updates; rank; order; rank adapters; low rank adapters
273-MAIN,Mitigating Degree Bias in Hypergraphs via Attribute-as-Structure Approach,"Entity representation learning on hypergraphs is hindered by degree bias, where nodes with sparse connections suffer from limited structural information for aggregation. Prevailing ``attribute-as-feature`` approaches, which treat rich textual attributes (e.g., titles, abstracts, keywords) merely as node features, fail to address this structurally rooted problem as they do not create new aggregation pathways. To overcome this limitation, we propose a novel ``attribute-as-structure`` approach specifically designed for heterogeneous hypergraphs. Our approach integrates attributes directly into the hypergraph topology as distinct node types, creating new structural pathways to enrich sparsely connected entities while preserving semantic distinctiveness within complex many-to-many hyperedge interactions. We introduce an entity-attribute aware learning framework featuring two key innovations: (1) a specialized heterogeneous hypergraph encoder with dual attention mechanismsвЂ”self-attention for entity-entity relationships and cross-type attention for entity-attribute relevance, and (2) Attribute-Attentive Contrastive Learning (AACL), a novel objective that dynamically weighs attribute importance while explicitly aligning entity representations with their structural attributes. Experiments on multiple hypergraph datasets demonstrate consistent improvements in node classification performance, with particularly significant gains for structurally sparse nodes, demonstrating the effectiveness of our approach for degree bias mitigation.",Ryusei Nishide; Makoto Miwa,Ryusei Nishide,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Interpretability & Model Analysis,"Trustworthy, Safety, Privacy & Fairness",attribute; entity; degree bias; node; degree; attributes; entity attribute; structurally; structural; pathways
278-MAIN,Generative Personality Simulation via Theory-Informed Structured Interview,"LLMs have recently been explored by psychometrics researchers as proxies for human participants. Although they show considerable potential in automating labor-intensive tasks and bridging AI research with psychological studies, these models often fail to generate heterogeneous data with human-like diversity, which diminishes their value in advancing social science research. To address this gap, we proposed a novel method to incorporate psychological insights into LLM simulation through the Personality Structured Interview (PSI). PSI leveraged psychometric scale-development procedures to capture personality-related linguistic information from a formal psychological perspective. To systematically evaluate the simulation fidelity, we developed a measurement theory grounded evaluation procedure that considers the latent construct nature of personality and evaluates its reliability, structural validity, and external validity. Results from three experiments demonstrate that PSI effectively improves human-like heterogeneity in LLM-simulated personality data and predicts personality-related behavioral outcomes. We further develop a theoretical framework for designing theory-informed structured interviews to enhance the reliability and effectiveness of LLMs in simulating human-like data for broader psychometric research.",Pengda Wang; Huiqi Zou; Han Jiang; Hanjie Chen; Tianjun Sun; Xiaoyuan Yi; Ziang Xiao; Frederick L. Oswald,Pengda Wang,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Interpretability & Model Analysis,personality; psychological; human like; simulation; interview; personality related; theory informed; structured interview; theory; psychometric
283-MAIN,Unraveling LLM Jailbreaks Through Safety Knowledge Neurons,"Large Language Models (LLMs) have achieved substantial progress in alignment, ensuring safer and more reliable outputs. However, jailbreak attacks can still bypass these safeguards and provoke harmful responses from well-aligned models. While some studies have achieved defenses against jailbreak attacks by modifying output distributions or detecting harmful content, the exact rationale still remains elusive. In this work, we present a novel neuron-level interpretability method that focuses on the role of safety-related knowledge neurons. Unlike existing approaches, our method projects the model's internal representation into a more consistent and interpretable vocabulary space. We then show that adjusting the activation of safety-related neurons can effectively control the modelвЂ™s behavior with a mean ASR higher than 97%. Building on this insight, we propose SafeTuning, a fine-tuning strategy that reinforces safety-critical neurons to improve model robustness against jailbreaks. SafeTuning consistently reduces attack success rates across multiple LLMs and outperforms all four baseline defenses. These findings offer a new perspective on understanding and defending against jailbreak attacks.",Chongwen Zhao; Yutong Ke; Kaizhu Huang,Chongwen Zhao,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,neurons; jailbreak attacks; jailbreak; safety; attacks; jailbreaks; defenses; harmful; achieved; related
284-MAIN,Hacking Neural Evaluation Metrics with a Single Text,"Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models, and they must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for the translation task, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for creating a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, in order to identify the vulnerabilities in evaluation metrics. Our found single hub text achieved 83.1 and 79.1 COMET% on the WMT'23 and WMT'24 English-to-Japanese translation tasks, respectively, outperforming translations generated individually for each source sentence by M2M100, a general translation model. Furthermore, we confirmed that our created single hub text generalizes across multiple language pairs such as Ja--En, En--De, and De--En.",Hiroyuki Deguchi; Katsuki Chousa; Yusuke Sakai,Hiroyuki Deguchi,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",evaluation metrics; metrics; hub; single; text; neural; comet; translation; evaluation; development
295-MAIN,ELLA: Efficient Lifelong Learning for Adapters in Large Language Models,"Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to 9.6% and a 35x smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.",Shristi Das Biswas; Yue Zhang; Anwesan Pal; Radhika Bhargava; Kaushik Roy,Shristi Das Biswas,Oral,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",Multilinguality & Low-Resource NLP,replay; freedom; residual; energy; overlap; based methods; memory; transfer; operator; realized
296-MAIN,To Paraphrase or Not: Efficient Comment Detoxification with Unsupervised Detoxifiability Discrimination,"Mitigating toxic content is critical for maintaining a healthy social platform, yet existing detoxification systems face significant limitations: overcorrection from uniformly processing all toxic comments, and parallel data scarcity in paraphrasing model training. To tackle these challenges, we propose Detoxifiability-Aware Detoxification (DID), a novel paradigm that adaptively conducts filtering or paraphrasing for each toxic comment based on its detoxifiability, namely whether it can be paraphrased into a benign comment in essence. Specifically, DID integrates three core modules: (1) an unsupervised detoxifiability discriminator, (2) a semantic purification module that extracts harmful intents and then performs targeted paraphrasing only on detoxifiable comments and (3) a feedback-adaptive refinement loop that processes remaining harmful contents only when they are detoxifiable. Experimental results demonstrate that DID significantly outperforms existing approaches on academic data and an industrial platform, establishing a novel and practical modeling paradigm for comment detoxification.",Jing Ke; Zheyong Xie; Shaosheng Cao; Tong Xu; Enhong Chen,Jing Ke,Oral,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,"Efficiency, Scaling & NLP Systems",detoxification; comment; did; paraphrasing; toxic; platform; comments; unsupervised; harmful; paradigm
305-MAIN,LingGen: Linguistic Fine-grained Controlled Generation,"We present LingGen, a novel controlled text generation system that enables precise control over a variable number of linguistic attributes through a dedicated attribute embedding network and optimized attribute integration mechanisms. Such fine-grained control is critical for applications like generating accessible educational materials. We also introduce a sample-based masking strategy that selectively masks linguistic control attributes according to a power law distribution during training. This approach improves robustness when controlling different combinations of attributes. Our experiments demonstrate that LingGen significantly outperforms current state-of-the-art models in multi-attribute controlled generation. Ablation studies reveal several key findings: Our approach for attribute integration effectively handles multi-attribute generation, the masking strategy enhances model performance across varying attribute combinations, and LingGen maintains consistent performance regardless of the chosen foundational model.",Mohamed Elgaar; Hadi Amiri,Mohamed Elgaar,Oral,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,,attribute; attributes; controlled generation; multi attribute; control; controlled; combinations; masking; generation; linguistic
307-MAIN,"Hey, wait a minute: on at-issue sensitivity in Language Models","Evaluating the naturalness of dialogue in language models (LMs) is not trivial: notions of *naturalness* vary, and scalable quantitative metrics remain limited. This study leverages the linguistic notion of *at-issueness* to assess dialogue naturalness and introduces a new method: Divide, Generate, Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii) generates continuations for subparts using LMs, (iii) recombines the dialogue and continuations, and (iv) compares the likelihoods of the recombined sequences. This approach mitigates bias in linguistic analyses of LMs and enables systematic testing of discourse-sensitive behavior. Applying DGRC, we find that LMs prefer to continue dialogue on at-issue content, with this effect enhanced in instruct-tuned models. They also reduce their at-issue preference when relevant cues (e.g., ""Hey, wait a minute"") are present. Although instruct-tuning does not further amplify this modulation, the pattern reflects a hallmark of successful dialogue dynamics.",Sanghee J. Kim; Kanishka Misra,Kanishka Misra,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Linguistics, Syntax & Semantics","Dialogue, Conversational & Interactive NLP",dialogue; naturalness; lms; hey; wait; issue; continuations; instruct; recombines; likelihoods
314-MAIN,RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion,"Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped relations between entities. While Large Language Models (LLMs) show promise for TKG completion, current approaches typically apply generic pipelines (neighborhood sampling, supervised fine-tuning, uncalibrated inference) without task-specific adaptation to temporal relational reasoning. Through systematic analysis under unified evaluation, we reveal three key failure modes: (1) retrieval strategies miss multi-hop dependencies when target entities are not directly observed in history, (2) standard fine-tuning reinforces memorization over relational generalization, and (3) uncalibrated generation produces contextually implausible entities. We present RECIPE-TKG, a parameter-efficient framework that addresses each limitation through principled, task-specific design: rule-based multi-hop sampling for structural grounding, contrastive fine-tuning to shape relational compatibility, and test-time semantic filtering for contextual alignment. Experiments on four benchmarks show that RECIPE-TKG outperforms prior LLM-based methods across input regimes, achieving up to 22.4% relative improvement in Hits@10, with particularly strong gains when historical evidence is sparse or indirect.",Г–mer Faruk AkgГјl; Feiyu Zhu; Yuxin Yang; Rajgopal Kannan; Viktor Prasanna,Omer Faruk Akgul,Oral,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)","Trustworthy, Safety, Privacy & Fairness",tkg; recipe; relational; temporal; entities; temporal knowledge; history; completion; task specific; sampling
320-MAIN,"Barriers to Discrete Reasoning with Transformers: A Survey Across Depth, Exactness, and Bandwidth","Transformers have become the foundational architecture for a broad spectrum of sequence modeling applications, underpinning state-of-the-art systems in natural language processing, vision, and beyond. However, their theoretical limitations in discrete reasoning tasks, such as arithmetic, logical inference, and algorithmic composition, remain a critical open problem. In this survey, we synthesize recent advances from three theoretical perspectives: circuit complexity, approximation theory, and communication complexity, to clarify the structural and computational barriers that transformers face when performing symbolic computations. By connecting these established theoretical frameworks, we provide an accessible and unified account of why current transformer architectures struggle to implement exact discrete algorithms, even as they excel at pattern matching and interpolation. We review key definitions, seminal results, and illustrative examples, highlighting challenges such as depth constraints, difficulty approximating discontinuities, and bottlenecks in inter-token communication. Finally, we discuss implications for model design and suggest promising directions for overcoming these foundational limitations.",Michelle Yuan; Weiyi Sun; Amir H. Rezaeian; Jyotika Singh; SANDIP GHOSHAL; Yao-Ting Wang; Miguel Ballesteros; Yassine Benajiba,Yassine Benajiba,Oral,In-person,SALLE  LE RIAD,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",discrete; transformers; theoretical; barriers; foundational; depth; communication; survey; complexity; limitations
322-MAIN,PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR,"Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training; they are available on Hugging Face. Finally, our data creation methods are scalable and easily extendable to other scientific domains.",James Burgess; Jan N. Hansen; Duo Peng; Yuhui Zhang; Alejandro Lozano; Min Woo Sun; Emma Lundberg; Serena Yeung-Levy,James Burgess,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents",Domain NLP (Biomedical/Clinical/Legal/Scientific),search; rlvr; search agents; scientific; agents; reason; scientific papers; corpus; technical; papers
323-MAIN,Exploring Speaker Anonymization Methods for Low-Resource Text-to-Speech,"We describe and compare multiple approaches for low-resource speaker anonymization. We build and evaluate speaker-anonymized text-to-speech systems for two Canadian Indigenous languages, nГЄhiyawГЄwin and SENД†OЕ¦EN, and show that cross-lingual speaker transfer via multilingual training with English data produces the best results. Our research also underscores the need for better evaluation metrics tailored to anonymization. Our code can be found at https://github.com/anonymous/anonymous",Shenran Wang; Aidan Pine; Mengzhe Geng,Shenran Wang,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multilinguality & Low-Resource NLP,,speaker; anonymization; text speech; anonymous; speech; low resource; speech systems; code https github; text speech systems; metrics tailored
325-MAIN,Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation,"Large Language Models (LLMs) are increasingly used to simulate public opinion and other social phenomena. Most current studies constrain these simulations to multiple-choice or short-answer formats for ease of scoring and comparison, but such closed designs overlook the inherently generative nature of LLMs. In this position paper, we argue that open-endedness, using free-form text that captures topics, viewpoints, and reasoning processes ""in"" LLMs, is essential for realistic social simulation. Drawing on decades of survey methodology research and recent advances in NLP, we argue why this open-endedness is valuable in LLM social simulations, showing how it can improve measurement and design, support exploration of unanticipated views, and reduce researcher-imposed directive bias. It also captures expressiveness and individuality, aids in pretesting, and ultimately enhances methodological utility. We call for novel practices and evaluation frameworks that leverage rather than constrain the open-ended generative diversity of LLMs, creating synergies between NLP and social science.",Bolei Ma; Yong Cao; Indira Sen; Anna-Carolina Haensch; Frauke Kreuter; Barbara Plank; Daniel Hershcovich,Yong Cao,Oral,In-person,SALLE  WALILI,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",social; open; social simulation; simulations; constrain; opinion; simulation; argue; captures; generative
327-MAIN,Respecting Temporal-Causal Consistency: EntityвЂ“Event Knowledge Graphs for Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) based on large language models often falters on narrative documents with inherent temporal structures. Standard unstructured RAG methods rely solely on embedding-similarity matching and lack any general mechanism to encode or exploit chronological information, while knowledge graph RAG (KG-RAG) frameworks collapse every mention of an entity into a single node, erasing the evolving context that drives many queries. To formalize this challenge and draw the communityвЂ™s attention, we construct ChronoQA, a robust and discriminative QA benchmark that measures temporal, causal, and character consistency understanding in narrative documents (e.g., novels) under the RAG setting. We then introduce Entity-Event RAG (E$^{ 2}$RAG), a dual-graph framework that keeps separate entity and event subgraphs linked by a bipartite mapping, thereby preserving the temporal and causal facets needed for fine-grained reasoning. Across ChronoQA, our approach outperforms state-of-the-art unstructured and KG-based RAG baselines, with notable gains on causal and character consistency queries. E$^{ 2}$RAG therefore offers a practical path to more context-aware retrieval for tasks that require precise answers grounded in chronological information.",Ze Yu Zhang; Zitao Li; Yaliang Li; Bolin Ding; Bryan Kian Hsiang Low,Ze Yu Zhang,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)","LLM Evaluation, Benchmarks & Metrics",rag; temporal causal; causal; temporal; event; chronological; consistency; entity; character; narrative
329-MAIN,Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant for Question Answering in the Era of LLMs?,"The advent of Large Language Models (LLMs) has significantly advanced web-based Question Answering (QA) systems over semi-structured content, raising questions about the continued utility of knowledge extraction for question answering. This paper investigates the value of triple extraction in this new paradigm by extending an existing benchmark with knowledge extraction annotations and evaluating commercial and open-source LLMs of varying sizes. Our results show that web-scale knowledge extraction remains a challenging task for LLMs. Despite achieving high QA accuracy, LLMs can still benefit from knowledge extraction, through augmentation with extracted triples and multi-task learning. These findings provide insights into the evolving role of knowledge triple extraction in web-based QA and highlight strategies for maximizing LLM effectiveness across different model sizes and resource settings.",Kai Sun; Yin Huang; Srishti Mehra; Mohammad Kachuee; Xilun Chen; Renjie Tao; Zhaojiang Lin; Andrea Jessee; Nirav Shah; Alex L Betty; Yue Liu; Anuj Kumar; Wen-tau Yih; Xin Luna Dong,Kai Sun,Oral,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","LLM Evaluation, Benchmarks & Metrics",knowledge extraction; extraction; knowledge; web; structured content; semi structured; semi; question answering; answering; sizes
330-MAIN,Inferring the Unseen: A Computational Approach to Visual Metonymy,"Visual metonymy is a cognitive mechanism through which a concept is evoked indirectly by presenting associated visual cues rather than explicitly depicting it. These associative links prompt viewers to infer broader concepts. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory, that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines ability to interpret indirect visual references.",Saptarshi Ghosh; Linfeng Liu; Tianyu Jiang,Tianyu Jiang,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,,visual; cognitive; computational; reveal significant gap; computational approach; indirectly; viewers; concepts work; 000 multiple; cognitive reasoning
331-MAIN,A Tale of Two Scripts: Transliteration and Post-Correction for Judeo-Arabic,"Judeo-Arabic refers to Arabic variants historically spoken by Jewish communities across the Arab world, primarily during the Middle Ages. Unlike standard Arabic, it is written in Hebrew script by Jewish writers and for Jewish audiences. Transliterating Judeo-Arabic into Arabic script is challenging due to ambiguous letter mappings, inconsistent orthographic conventions, and frequent code-switching into Hebrew and Aramaic. In this paper, we introduce a two-step approach to automatically transliterate Judeo-Arabic into Arabic script: simple character-level mapping followed by post-correction to address grammatical and orthographic errors. We also present the first benchmark evaluation of LLMs on this task. Finally, we show that transliteration enables Arabic NLP tools to perform morphosyntactic tagging and machine translation, which would have not been feasible on the original texts.",Juan Moreno Gonzalez; Bashar Alhafni; Nizar Habash,Juan Moreno Gonzalez,Oral,In-person,SALLE  LA PALMERAIE,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Machine Translation,"LLM Evaluation, Benchmarks & Metrics",arabic; script; arabic script; hebrew; orthographic; transliteration; correction; post; arabic nlp; standard arabic
337-MAIN,Multimodal Evaluation of Russian-language Architectures,"Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capability, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce AnonymBench, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",Artem Chervyakov; Ulyana Isaeva; Anton Emelyanov; Artem Safin; Maria Tikhonova; Alexander Kharitonov; Yulia Lyakh; Petr Surovtsev; Denis Shevelev; Vildan Saburov; Vasily Konovalov; Elisei Rykov; Ivan Sviridov; Amina Miftakhova; Ilseyar Alimova; Alexander Panchenko; Alexander Kapitanov; Alena Fenogenova,"Artem Chervyakov, Ulyana Isaeva, Alena Fenogenova",Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",russian; multimodal; architectures; currently; text; video; methodology; audio; image; attention
340-MAIN,Don’t Judge a Book by its Cover: Testing LLMs’ Robustness Under Logical Obfuscation,"Tasks such as solving arithmetic equations, evaluating truth tables, and completing syllogisms are handled well by large language models (LLMs) in their standard form, but they often fail when the same problems are posed in logically equivalent yet obfuscated formats. To study this vulnerability, we introduce Logifus, a structure-preserving logical obfuscation framework, and, utilizing this, we present LogiQAte, a first-of-its-kind diagnostic benchmark with 1,108 questions across four reasoning tasks: (i) Obfus FOL (first-order logic entailment under equivalence-preserving rewrites), (ii) Obfus Blood Relation (family-graph entailment under indirect relational chains), (iii) Obfus Number Series (pattern induction under symbolic substitutions), and (iv) Obfus Direction Sense (navigation reasoning under altered directions and reference frames). Across all the tasks, evaluating six state-of-the-art models, we find that obfuscation severely degrades zero-shot performance, with performance dropping on average by 47% for GPT-4o, 27% for GPT-5, and 22% for reasoning model, o4-mini. Our findings reveal that current LLMs parse questions without deep understanding, highlighting the urgency of building models that genuinely comprehend and preserve meaning beyond surface form.",Abhilekh Borah; Shubhra Ghosh; Kedar Joshi; Aditya Kumar Guru; Kripabandhu Ghosh,Abhilekh Borah,Oral,In-person,SALLE  LE RIAD,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",entailment; logical; form; preserving; gpt; questions; reasoning; equations; syllogisms; equivalence
341-MAIN,I know you are different! Towards Persona Driven Knowledge-infused Dialogue Assistant,"Despite advances in large language models (LLMs), Task-Oriented Dialogue (TOD) systems often fall short in delivering personalized, context-rich responses, especially in low-resource, code-mixed, and multimodal settings like Hinglish (Hindi-English). To bridge this gap, we introduce $\textit{HiVisTask}$, the first Hinglish multimodal, multidomain, persona-based TOD dataset that captures user-agent interactions across text and visual modalities. We also propose $\textit{G$^{3}$ TOD}$, a generalizable framework that enhances personalization using three structured knowledge graphs: entity context, user persona, and commonsense reasoning, all extracted from conversation history. Extensive experiments with LLMs (e.g., LLaMA3.2, Phi3, GPT4, Mistral7b, Qwen3, Gemma3) show that $\textit{G$^{3}$ TOD}$ consistently outperforms both standard and ablated baselines. We observe substantial gains across evaluation metrics (both quantitative: BLEU $\uparrow$ and qualitative: Human Eval $\uparrow$) over existing models. The observed improvements strongly underscore the value of structured and selective contextualization in generating personalized and engaging multimodal responses. The dataset and code will be publicly available after acceptance.",Shifali Agrahari; Moushumi Mahato; Abhisek Tiwari; Javaid Nabi,Shifali Agrahari,Oral,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP","Reasoning, Planning & Agents",persona; textit; multimodal; personalized; dialogue; responses; user; models llms task; task oriented dialogue; infused
350-MAIN,Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation of LLMs' Legal Reasoning Capabilities,"We introduce the Korean Canonical Legal Benchmark (KCL), a benchmark designed to assess language modelsвЂ™ legal-reasoning capabilities independently of domain-specific knowledge. KCL provides question-level supporting precedents enabling a more faithful disentanglement of reasoning ability from parameterized knowledge. KCL consists of two components: (1) KCL-MCQA, multiple-choice problems of 283 questions with 1,103 aligned precedents, and (2) KCL-Essay, open-ended generation problems of 169 questions with 550 aligned precedents and 2,739 instance-level rubrics for automated evaluation. Our systematic evaluation of 30+ models shows large remaining gaps, particularly in KCL-Essay, and that reasoning-specialized models consistently outperform their general-purpose counterparts.",Hongseok Oh; Wonseok Hwang; Kyoung-Woon On,Hongseok Oh,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics",Domain NLP (Biomedical/Clinical/Legal/Scientific),precedents; legal; canonical; legal reasoning capabilities; legal reasoning; essay; korean; reasoning capabilities; reasoning; problems
352-MAIN,Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning,"Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing language model's reasoning capabilities. However, generating long and correct CoT trajectories is challenging. Recent studies have demonstrated that Looped Transformers, a standard Transformer with cross-block parameter-sharing architecture, possess remarkable length generalization capabilities, but their limited generality and adaptability prevent them from serving as an alternative to auto-regressive solutions. To better leverage the strengths of Looped Transformers, we propose **RELAY** (**RE**asoning through **L**oop **A**lignment iterativel**Y**). Specifically, we align the steps of Chain-of-Thought (CoT) reasoning with loop iterations and apply intermediate supervision during the training of Looped Transformers. This additional iteration-wise supervision not only preserves the Looped Transformer's ability for length generalization but also enables it to predict CoT reasoning steps for unseen data. Therefore, we leverage this Looped Transformer to generate accurate reasoning chains for complex problems that exceed the training length, which will then be used to fine-tune an auto-regressive model. We conduct extensive experiments, and the results demonstrate the effectiveness of our approach, with significant improvements in the performance of the auto-regressive model.",Qifan Yu; Zhenyu He; Sijie Li; zhou Xun; Jun Zhang; Jingjing Xu; Di He,Qifan Yu,Oral,In-person,SALLE  LE RIAD,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents",,auto; cot; transformers; length; length generalization; transformer; chain thought; chain; thought; cot reasoning
353-MAIN,Task-Level Instructions Induction for Audio Question Answering from Few Examples,"Large audio-language models (LALMs) benefit from Chain-of-Thought (CoT) prompting for audio question answering (AQA), but acquiring audio CoT examples is particularly challenging as it requires sequential listening and careful integration of acoustic and linguistic information. Surprisingly, our experiments reveal that standard few-shot prompting yields inconsistent results compared to zero-shot CoT, with several models showing degraded accuracy. Moreover, few-shot prompting incurs substantially higher inference costs by processing multiple audio demonstrations per inference. We propose Audio-Induct, which induces reusable textual task instructions from few audio examples once per task, requiring no additional demonstrations at inference. Evaluated on 9 LALMs across two benchmarks, Audio-Induct outperforms state-of-the-art prompting methods while maintaining low inference costs. Inducted Task Instructions transfer effectively across models, enabling scalable deployment.",Po-Chun Chen; Hen-Hsen Huang; Hsin-Hsi Chen,Po-Chun Chen,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Reasoning, Planning & Agents",audio; cot; prompting; instructions; examples; inference costs; inference; lalms; shot prompting; demonstrations
359-MAIN,Layer-wise Swapping for Generalizable Multilingual Safety,"Despite the rapid advancements of Large Language Models (LLMs), safety risks remain a critical challenge for low-resource languages. Existing safety datasets are predominantly English-centric, limiting progress in multilingual safety alignment. As a result, low-resource expert modelsвЂ”fine-tuned on their respective instruction datasetsвЂ”tend to exhibit higher unsafety rates compared to their high-resource counterparts. In this work, we propose a safety aware layer swapping method that transfers safety alignment from an English safety expert to low-resource language experts without additional training. To further enhance transfer ability, our method adaptively selects or blends modules based on their degree of specialization. Our approach preserves performance on general language understanding tasks while enhancing safety in the target languages. Experimental results show that the proposed method achieves comparable performance to the language expert on general benchmarks such as MMMLU, BELEBELE, and MGSM, while producing more aligned and less harmful responses on the MultiJail safety benchmark",Hyunseo Shin; Wonseok Hwang,Hyunseo Shin,Oral,In-person,SALLE  WALILI,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Multilinguality & Low-Resource NLP,"Trustworthy, Safety, Privacy & Fairness",safety; multilingual safety; swapping; resource; expert; low resource; safety alignment; low; layer; general
360-MAIN,Measuring Idiomaticity in Text Embedding Models with $\epsilon$-compositionality,"The principle of compositionality, which concerns the construction of meaning from constituent parts, is a longstanding topic in various disciplines, most commonly associated with formal semantics. In NLP, recent studies have focused on the compositional properties of text embedding models, particularly regarding their sensitivity to idiomatic expression, as idioms have traditionally been seen as non-compositional. In this paper, we argue that it is unclear how previous work relates to formal definitions of the principle. To address this limitation, we take a theoretically motivated approach based on definitions in formal semantics. We present $\epsilon$-compositionality, a continuous relaxation of compositionality derived from these definitions. We measure $\epsilon$-compositionality on a dataset containing both idiomatic and non-idiomatic sentences, providing a theoretically motivated assessment of sensitivity to idiomaticity. Our findings indicate that most text embedding models differentiate between idiomatic and non-idiomatic phrases, although to varying degrees.",Sondre Wold; Г‰tienne Simon; Erik Velldal; Lilja Г�vrelid,Étienne Simon,Oral,In-person,SALLE  LA PALMERAIE,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Linguistics, Syntax & Semantics",,idiomatic; compositionality; text embedding models; definitions; text embedding; embedding models; formal; embedding; principle; theoretically
361-MAIN,Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers,"Large language model (LLM)-based systems now power chatbots, computer-use agents, and even robots, relying on prompts that often require costly manual tuning. LLM-based prompt optimizers cut that effort by iteratively refining prompts with scored feedback, yet their security has gone largely unexamined. Our work provides the first systematic analysis of poisoning risks in this optimization loop, revealing that optimization itself can become a new vulnerability point. Using HarmBench, we show optimized systems are far more vulnerable to manipulated feedback than to malicious queries, with Attack Success Ratio jumping by up by 48% absolute points. We propose a feedback-poisoning attack that embeds fake reward tokens in prompts, significantly boosting attack success under realistic assumptions, and evaluate tailored defenses that drastically decrease that threat. Our findings establish prompt optimization pipelines as an urgent and underexplored attack surface for LLM safety research, calling for stronger safeguards in future optimization frameworks.",Andrew Zhao; Reshmi Ghosh; Vitor R. Carvalho; Emily Lawton; Keegan Hines; Gao Huang; Jack W. Stokes,Andrew Zhao,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Dialogue, Conversational & Interactive NLP",attack; prompts; optimization; poisoning; optimizers; feedback; attack success; llm based; optimized; llm
365-MAIN,MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling,"Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, an end-to-end multi-agent collaborative framework for long-sequence video storytelling. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E PrincipleвЂ”Explore, Examine, and EnhanceвЂ”to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief prompt, MAViS enables users to rapidly explore diverse visual storytelling and creative directions for sequential video generation by efficiently producing high-quality, complete long-sequence videos. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.",Qian Wang; Ziqi Huang; Ruoxi Jia; Paul Debevec; Ning Yu,Qian Wang,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,Summarization & Generation,video; sequence; storytelling; long; capability; expressiveness; assistive; generative; generative models; videos
367-MAIN,Computational Benchmarks for Egyptian Arabic Child Directed Speech,"We present AraBabyTalk-EGY, an enriched release of the Egyptian Arabic CHILDES corpus, that opens the child-adult interactions genre to modern Arabic NLP research. Starting from the original CHILDES recordings and IPA transcriptions of caregiverвЂ“child sessions, we (i) map each IPA token to fully diacritized Arabic script, and (ii) add core part-of-speech tags and lemmas aligned with existing dialectal Arabic morphological resources. These layers yield 26K annotated tokens suitable for both text- and speech-based NLP tasks. We provide a benchmark on morphological disambiguation and Arabic ASR. We outline lexical and morphosyntactic differences between AraBabyTalk-EGY and general Egyptian Arabic resources, underscoring the value of genre-specific training data for language acquisition studies and Arabic speech technology.",Salam Khalifa; Abed Qaddoumi; Nizar Habash; Owen Rambow,Salam Khalifa,Oral,In-person,SALLE  LA PALMERAIE,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",arabic; egyptian; egyptian arabic; child; speech; ipa; genre; morphological; resources; nlp
368-MAIN,K-LegalDeID: A Benchmark Dataset and KLUEBERT-CRF for De-identification in Korean Court Judgments,"The Korean legal system mandates public access to court judgments to ensure judicial transparency. However, this requirement conflicts with privacy protection obligations due to the prevalence of Personally Identifiable Information (PII) in legal documents. To address this challenge, we introduce **K-LegalDeID**, a large-scale benchmark dataset and an efficient KLUEBERT-CRF model for de-identification for Korean court judgments. Our primary contribution is a new large-scale benchmark dataset spanning 39 legal domains, with its quality is validated by a high inter-annotator agreement (IAA) with Fleiss' Kappa of 0.7352. Our results demonstrate that a lightweight KLUEBERT-CRF model, when trained on our dataset, achieves state-of-the-art performance with an entity-level micro F1 score of 0.9923. Our end-to-end framework offers a practical and computationally efficient solution for real-world legal systems.",Wooseok Choi; Hyungbin Kim; Yon Dohn Chung,Wooseok Choi,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Trustworthy, Safety, Privacy & Fairness",court judgments; legal; court; korean; benchmark dataset; korean court; large scale benchmark; scale benchmark; judgments; dataset
369-MAIN,Optical Character Recognition for the International Phonetic Alphabet,"As grammar books are increasingly used as additional reference resources specifically for very low-resource languages, a significant portion comes from scans and relies on the quality of the Optical Character Recognition (OCR) tool. We focus here on a particular script used in linguistics to transcribe sounds: the International Phonetic Alphabet (IPA). We consider two data sources: actual grammar book PDFs for two languages under documentation, Japhug and Kagayanen, and a synthetically generated dataset based on Wiktionary. We compare two neural OCR frameworks, Tesseract and Calamari, and a recent large vision-language model, Qwen2.5-VL-7B, all three in an off-the-shelf setting and with fine-tuning. While their zero-shot performance is relatively poor for IPA characters in general due to character set mismatch, fine-tuning with the synthetic dataset leads to notable improvements.",Shu Okabe; Dejvi Zelo; Alexander Fraser,Shu Okabe,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multilinguality & Low-Resource NLP,Multimodal & Speech/Audio,character; ipa; international; grammar; ocr; recognition; documentation; neural ocr; fine tuning synthetic; qwen2 vl 7b
377-MAIN,Specialization through Collaboration: Understanding Expert Interaction in Mixture-of-Expert Large Language Models,"Mixture-of-Experts (MoE) based large language models (LLMs) have gained popularity due to their multi-task capability, where each input token activates only a subset of ""expert"" subnetworks. However, whether each expert can truly specialize to a certain task remains poorly understood, while activation analysis shows frequent cross-layer co-activation of experts for the same input, resembling a collaborative behavior. In this paper, we use a dictionary learning approach to show that experts in MoE LLMs form hierarchical and semantically coherent collaborative groups that correspond to specific linguistic and cognitive functions (e.g., mathematical reasoning, syntactic processing), mirroring specialized functional region observed in neuroscience. Furthermore, leveraging these discovered expert groups enables significant model compression with minimal performance degradation, outperforming existing methods by 2.5% while enabling up to 50% expert reduction. These findings provide the first systematic analysis of expert collaboration mechanisms in MoE LLMs, revealing that specialization emerges from joint activation of experts across all layers. We further developed an interactive visualization platform that enables researchers to explore expert collaboration patterns and their semantic associations.",yuanbo tang; Naifan Zhang; Yan Tang; Meixuan Chen; Shuhan Huang; Tingyu Cao; Yang Li,yuanbo tang,Oral,In-person,SALLE  WALILI,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Dialogue, Conversational & Interactive NLP",Interpretability & Model Analysis,expert; experts; moe; collaboration; activation; experts moe; specialization; collaborative; groups; mixture
378-MAIN,Compact Language Models with Iterative Text Refinement for Health Dialogue Summarization,"Health wellness agents typically rely on large language models (LLMs) for response generation, where contextual information from a user's conversation history can be used for response grounding and personalization. High-quality conversation summaries are one such method which can reduce the number of input tokens during response generation, decreasing overhead and inference latency. However, directly purposing LLMs for this task is infeasible due to the scale of the task, the compute overhead, and health data compliance regulations. Furthermore, ground truth for real-world datasets is scarce due to privacy concerns and the high cost of health expert annotators. These factors necessitate the development of small, potentially on-device, language models capable of health dialogue summarization, particularly in the absence of ground truth labels. In this paper, we first present a comprehensive empirical study that benchmarks a variety of state-of-the-art smaller language models to better understand their baseline capabilities. Second, we present an unsupervised method that uses the summaries from multiple models, refined with iterative feedback, to generate high-quality summaries of health dialogues. Experiments illustrate that our method is able to outperform baseline on both open-source and proprietary benchmarks. Notably, our method can be run viably on local compute without a GPU, using just a single Macbook with 16 GB of memory.",Kellen Tan Cheng; Ganesh Ramesh; Nafiul Rashid; Geoffrey Jay Tso; Jilong Kuang,Kellen Tan Cheng,Oral,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP","Efficiency, Scaling & NLP Systems",health; summaries; dialogue summarization; health dialogue; response generation; response; ground truth; truth; conversation; overhead
384-MAIN,"Mind the Gap: Benchmarking LLM Uncertainty, Discrimination, and Calibration in Specialty-Aware Clinical QA","Reliable uncertainty quantification (UQ) is essential when employing large language models (LLMs) in high-risk domains such as clinical question answering (QA). In this work, we evaluate uncertainty estimation methods for clinical QA focusing, for the first time, on eleven clinical specialties and six question types, and across ten open-source LLMs (general-purpose, biomedical, and reasoning models). We analyze score-based UQ methods, present a case study introducing a novel lightweight method based on behavioral features derived from reasoning-oriented models, and examine conformal prediction as a complementary set-based approach. Our findings reveal that uncertainty reliability is not a monolithic property, but one that depends on clinical specialty and question type due to shifts in calibration and discrimination. Our results highlight the need to select or ensemble models based on their distinct, complementary strengths and clinical use.",Alberto Testoni; Iacer Calixto,Alberto Testoni,Oral,In-person,SALLE  LE LIXUS,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",Domain NLP (Biomedical/Clinical/Legal/Scientific),clinical; uncertainty; specialty; discrimination; calibration; question; complementary; based; models examine; specialties
386-MAIN,Controlling Reading Ease with Gaze-Guided Text Generation,"The way our eyes move while reading can tell us about the cognitive effort required to process the text. In the present study, we use this fact to generate texts with controllable reading ease. Our method employs a model that predicts human gaze patterns to steer language model outputs towards eliciting certain reading behaviors. We evaluate the approach in an eye-tracking experiment with native and non-native speakers of English. The results demonstrate that the method is effective at making the generated texts easier or harder to read, measured both in terms of reading times and perceived difficulty of the texts. A statistical analysis reveals that the changes in reading behavior are mostly due to features that affect lexical processing. Possible applications of our approach include generation of personalized educational material for language learning and text simplification for information accessibility.",Andreas SГ¤uberli; Darja Jepifanova; Diego Frassinelli; Barbara Plank,Andreas Säuberli,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Summarization & Generation,Interpretability & Model Analysis,reading; reading ease; gaze; texts; ease; native; text; reading behaviors; method employs; evaluate approach
389-MAIN,PictureStories: Predicting the Task Adherence of Language Learner Answers to a Picture Story-Based Writing Task,"We investigate the automated evaluation of English language learner answers to writing prompts featuring picture stories. Automatic evaluation of such tasks is usually limited to language proficiency only, neglecting the context of the picture. Instead, our analysis focuses on task adherence, which for example allows detection of off-topic answers. Since there is a lack of suitable training and evaluation data, our first step is to build the PictureStories dataset. To this end, we develop a marking rubric that covers task adherence with respect to form and content. Six annotators mark 713 learner answers written in response to one of five picture stories. Having assembled the dataset, we then explore to what extent task adherence can be predicted automatically. Our experiments assume a scenario where no or just a few labelled answers are available for the picture story which is being marked. For content-focused criteria, few-shot prompting Qwen emerges as the best-performing method. We examine the trade-off between including the story image vs. example answers in the prompt and find that examples suffice in many cases. For form-focused criteria, we find that it is beneficial to finetune models across prompts. For some LLMs, few-shot prompting results may look promising on the surface, but we demonstrate that a much simpler method can go just as far when shown the same examples.",Marie Bexte; Andrew Caines; Diane Nicholls; Paula Buttery; Torsten Zesch,Marie Bexte,Oral,In-person,SALLE  WALILI,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",Interpretability & Model Analysis,answers; adherence; story; stories; shot prompting; just; criteria; writing; example; examples
392-MAIN,Assessing the Impact of Typological Features on Multilingual Machine Translation in the Age of Large Language Models,"Despite major advances in multilingual modeling, large quality disparities persist across languages. Besides the obvious role of training data representation, typological properties have also been proposed to determine the intrinsic difficulty of modeling a language. The existing evidence, however, is mostly based on small monolingual language models or bilingual translation models trained from scratch. We expand on this line of work by analyzing two large pre-trained multilingual translation models, NLLB-200 and Tower+, which are state-of-the-art representatives of encoder-decoder and decoder-only machine translation, respectively. Based on a broad set of languages, we find that target language typology drives translation quality of the NLLB model, even after controlling for more trivial factors, such as data resourcedness and writing script. Additionally, languages with certain typological properties benefit more from a wider search of the output space, suggesting that such languages could benefit from alternative decoding strategies beyond the standard left-to-right beam search. By contrast, variations in translation quality of the LLM-based Tower+ model remain largely unexplained by our predictors. To facilitate further research in this area, we release a set of fine-grained typological properties for 212 languages of the FLORES+ MT evaluation benchmark.",Vitalii Hirak; Jaap Jumelet; Arianna Bisazza,Vitalii Hirak,Oral,In-person,SALLE  LA PALMERAIE,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Machine Translation,Multilinguality & Low-Resource NLP,typological; translation; languages; properties; translation models; benefit; translation quality; multilingual; decoder; machine translation
393-MAIN,Large Language Models as Oracles for Ontology Alignment,"There are many methods and systems to tackle the ontology alignment problem, yet a major challenge persists in producing high-quality mappings among a set of input ontologies. Adopting a human-in-the-loop approach during the alignment process has become essential in applications requiring very accurate mappings. However, user involvement is expensive when dealing with large ontologies. In this paper, we evaluate the feasibility of using Large Language Models (LLM) to aid the ontology alignment problem. The use of LLMs is focused only on the validation of a subset of correspondences where an ontology alignment system (e.g., LogMap) is very uncertain. We have conducted an extensive analysis over several tasks of the Ontology Alignment Evaluation Initiative (OAEI), reporting in this paper the performance of several state-of-the-art LLMs using different ontology-driven prompt templates. In the OAEI 2025 Bio-ML track, LogMap with an LLM-based Oracle has achieved the top-2 overall results. LLM efficacy is also assessed against simulated Oracles with varying error rates.",Sviatoslav Lushnei; Dmytro Shumskyi; Severyn Shykula; Ernesto JimГ©nez-Ruiz; Artur d'Avila Garcez,Ernesto Jimenez-Ruiz,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",ontology; alignment; ontologies; mappings; problem; art llms using; producing high quality; alignment evaluation; use llms; initiative
400-MAIN,Disentangling Knowledge and Reasoning in Biomedical QA Benchmarks,"Medical reasoning in large language models seeks to replicate clinicians' cognitive processes in interpreting patient data and making diagnostic decisions. However, widely used benchmarksвЂ”such as MedQA, MedMCQA, and PubMedQAвЂ”mix questions that require multi-step reasoning with those answerable through factual recall, complicating evaluation. We demonstrate this by training a PubMedBERT-based classifier on expert-curated labels and applying it to 11 widely used biomedical QA benchmarks, where we find that only 32.8% of the questions require multi-step reasoning, indicating that current evaluations largely measure recall. This stratified evaluation of biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3) reveals consistently lower performance on reasoning versus knowledge (e.g., HuatuoGPT-o1: 56.9% vs. 44.8%). Beyond accuracy, we assess robustness through adversarial evaluations in which models are prefixed with uncertainty-inducing statements; biomedical reasoning models degrade sharply in this setting (e.g., MedReason: 50.4% в†’ 24.4%), with declines especially pronounced on reasoning-heavy questions. Finally, we show that fine-tuning on high-quality reasoning examples augmented with adversarial traces, followed by reinforcement learning with GRPO, improves both robustness and accuracy across knowledge and reasoning subsets.",Rahul Thapa; Qingyang Wu; Kevin Wu; Harrison G Zhang; Angela Zhang; Eric Wu; Haotian Ye; James Zou,Rahul Thapa,Oral,In-person,SALLE  LE LIXUS,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Reasoning, Planning & Agents",biomedical; reasoning; biomedical qa; require multi; huatuogpt; questions require multi; questions require; knowledge reasoning; multi step reasoning; step reasoning
404-MAIN,Effective QA-Driven Annotation of PredicateвЂ“Argument Relations Across Languages,"Explicit representations of predicate-argument relations form the basis of interpretable semantic analysis, supporting reasoning, generation, and evaluation. However, attaining such semantic structures requires costly annotation efforts and has remained largely confined to English. We leverage the Question-Answer driven Semantic Role Labeling (QA-SRL) framework --- a natural-language formulation of predicate-argument relations --- as the foundation for extending semantic annotation to new languages. To this end, we introduce a cross-linguistic projection approach that reuses an English QA-SRL parser within a constrained translation and word-alignment pipeline to automatically generate question-answer annotations aligned with target-language predicates. Applied to Hebrew, Russian, and French --- spanning diverse language families --- the method yields high-quality training data and fine-tuned, language-specific parsers that outperform strong multilingual LLM baselines (GPT-4o, LLaMA-Maverick). By leveraging QA-SRL as a transferable natural-language interface for semantics, our approach enables efficient and broadly accessible predicate-argument parsing across languages.",Jonathan Davidov; Aviv Slobodkin; Shmuel Tomi Klein; Reut Tsarfaty; Ido Dagan; Ayal Klein,Jonathan Davidov,Oral,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics",,argument; relations; semantic; annotation; question answer; languages; driven; natural language; answer; requires costly
409-MAIN,Form and Meaning in Intrinsic Multilingual Evaluations,"Intrinsic evaluation metrics for conditional language models, such as perplexity or bits-per-character, are widely used in both mono- and multilingual settings. These metrics are rather straightforward to use and compare in monolingual setups, but rest on a number of assumptions in multilingual setups. One such assumption is that comparing the perplexity of CLMs on parallel sentences is indicative of their quality since the information content (here understood as the semantic meaning) is the same. However, the metrics are inherently measuring information content in the information-theoretic sense. We make this and other such assumptions explicit and discuss their implications. We perform experiments with six metrics on two multi-parallel corpora both with mono- and multilingual models. Ultimately, we find that current metrics are not universally comparable. We look at the form-meaning debate to provide some explanation for this.",Wessel Poelman; Miryam de Lhoneux,Wessel Poelman,Oral,In-person,SALLE  LA PALMERAIE,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics",Multilinguality & Low-Resource NLP,metrics; meaning; multilingual; setups; assumptions; intrinsic; perplexity; parallel; information; form
410-MAIN,What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning under Incomplete Knowledge,"Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is an increasingly explored approach for combining the reasoning capabilities of large language models with the structured evidence of knowledge graphs. However, current evaluation practices fall short: existing benchmarks often include questions that can be directly answered using existing triples in KG, making it unclear whether models perform reasoning or simply retrieve answers directly. Moreover, inconsistent evaluation metrics and lenient answer matching criteria further obscure meaningful comparisons. In this work, we introduce a general method for constructing benchmarks, together with an evaluation protocol, to systematically assess KG-RAG methods under knowledge incompleteness. Our empirical results show that current KG-RAG methods have limited reasoning ability under missing knowledge, often rely on internal memorization, and exhibit varying degrees of generalization depending on their design.",Dongzhuoran Zhou; Yuqicheng Zhu; Xiaxia Wang; Hongkuan Zhou; Yuan He; Jiaoyan Chen; Steffen Staab; Evgeny Kharlamov,Dongzhuoran Zhou,Oral,In-person,SALLE  LE RIAD,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)",,kg rag; knowledge; rag; knowledge graph based; rag methods; graph based; knowledge graph; reasoning; empirical; directly
411-MAIN,Assessing Web Search Credibility and Response Groundedness in Chat Assistants,"Chat assistants increasingly integrate web search functionality, enabling them to retrieve and cite external sources. While this promises more reliable answers, it also raises the risk of amplifying misinformation from low-credibility sources. In this paper, we introduce a novel methodology for evaluating assistants' web search behavior, focusing on source credibility and the groundedness of responses with respect to cited sources. Using 100 claims across five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity, and Qwen Chat. Our findings reveal differences between the assistants, with Perplexity achieving the highest source credibility, whereas GPT-4o exhibits elevated citation of non-credibility sources on sensitive topics. This work provides the first systematic comparison of commonly used chat assistants for fact-checking behavior, offering a foundation for evaluating AI systems in high-stakes information environments.",Ivan Vykopal; MatГєЕЎ Pikuliak; Simon Ostermann; Marian Simko,Ivan Vykopal,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Dialogue, Conversational & Interactive NLP","Retrieval, Grounding & External Knowledge (RAG)",credibility; assistants; chat; web search; sources; web; search; gpt; misinformation; perplexity
413-MAIN,How DDAIR you? Disambiguated Data Augmentation for Intent Recognition,"Large Language Models (LLMs) are effective for data augmentation in classification tasks like intent detection. In some cases, they inadvertently produce examples that are ambiguous with regard to untargeted classes. We present DDAIR (Disambiguated Data Augmentation for Intent Recognition) to mitigate this problem. We use Sentence Transformers to detect ambiguous class-guided augmented examples generated by LLMs for intent recognition in low-resource scenarios. We identify synthetic examples that are semantically more similar to another intent than to their target one. We also provide an iterative re-generation method to mitigate such ambiguities. Our findings show that sentence embeddings effectively help to (re)generate less ambiguous examples, and suggest promising potential to improve classification performance in scenarios where intents are loosely or broadly defined.",Galo Castillo-LГіpez; GaГ«l de Chalendar; Alexis Lombard; Nasredine Semmar,Galo Castillo-López,Oral,In-person,SALLE  WALILI,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multilinguality & Low-Resource NLP,Summarization & Generation,intent; examples; ambiguous; data augmentation; augmentation; recognition; mitigate; sentence; scenarios; classification
414-MAIN,"When the Model Said вЂ�No CommentвЂ™, We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified","Ensuring Large Language Models (LLMs) are aligned with human valuesвЂ”being helpful, harmless, and honest (HHH)вЂ”is crucial for safe deployment. Existing methods use supervised fine-tuning (SFT) and Mixture-of-Experts (MoE) to align LLMs. However, these methods face challenges in multi-objective settings: SFT leads to interference between conflicting goals, while MoEs suffer from miscalibrated routing. We term this failure mode Axis Collapse, marked by (1) disjoint feature spaces causing catastrophic forgetting, and (2) unreliable inference from misrouted experts. To address this, we propose AlignX, a two-stage alignment framework. Stage 1 uses prompt-injected fine-tuning to extract axis-specific task features, mitigating catastrophic forgetting. Stage 2 deploys a MoCaE module that calibrates expert routing using fractal and natural geometry, improving inference reliability. AlignX achieves significant gains on Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty), with +171.5% win rate, +110.1% in truthfulness-informativeness, and 4.3% fewer safety violations. It also reduces latency and memory usage by over 35% compared to prior MoEs. Results across four LLMs validate its generalizability. Code and data are available at: https://anonymous.4open.science/r/AlignX-40BE/README.md",Gautam Siddharth Kashyap; Mark Dras; Usman Naseem,Gautam Siddharth Kashyap,Oral,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","Trustworthy, Safety, Privacy & Fairness",stage; axis; helpfulness; catastrophic; catastrophic forgetting; forgetting; sft; routing; experts; safety
417-MAIN,NeuronMoE: Efficient Cross-Lingual Extension via Neuron-Guided Mixture-of-Experts,"How should we allocate expert parameters when extending language models to new languages? Current Mixture-of-Experts (MoE) approaches use layer-level similarity heuristics, but language processing happens at individual neurons. We propose $\textbf{NeuronMoE}$, a method that analyzes language-specific neurons across all transformer components to guide expert allocation per layer based on empirically measured cross-lingual neuron diversity. Applied to LLaMA-3.2-3B for low-resource languages (Greek, Turkish, and Hungarian), this approach achieves 40% average parameter reduction (47-50 vs. 84 experts) while matching layer-wise baseline performance. Remarkably, we find that low-resource language experts independently develop neuron specialization patterns mirroring the high-resource languageвЂ”concentrated in early and late layersвЂ”revealing universal architectural principles in how multilingual models organize linguistic knowledge. Our approach generalizes across architectures (validated on Qwen) and shows that allocation strategy matters more than total expert count.",Rongzhi Li; Hitomi Yanaka,Rongzhi Li,Oral,In-person,SALLE  LA PALMERAIE,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multilinguality & Low-Resource NLP,,experts; neuron; allocation; layer; expert; neurons; mixture experts; mixture; resource; cross lingual
421-MAIN,From Emotion to Expression: Theoretical Foundations and Resources for Fear Speech,"Few forces rival fear in their ability to mobilize societies, distort communication, and reshape collective behavior. In computational linguistics, fear is primarily studied as an emotion, but not as a distinct form of speech. Fear speech content is widespread and growing, and often outperforms hate-speech content in reach and engagement because it appears ""civiler"" and evades moderation. Yet the computational study of fear speech remains fragmented and under-resourced. This can be understood by recognizing that fear speech is a phenomenon shaped by contributions from multiple disciplines. In this paper, we bridge cross-disciplinary perspectives by comparing theories of fear from psychology, political science, communication science, and linguistics. Building on this, we review existing definitions. We follow up with a survey of datasets from related research areas and propose a taxonomy that consolidates different dimensions of fear for studying fear speech. By reviewing current datasets and defining core concepts, our work offers both theoretical and practical guidance for creating datasets and advancing fear speech research.",Vigneshwaran Shankaran; Gabriella Lapesa; Claudia Wagner,Vigneshwaran Shankaran,Oral,In-person,SALLE  WALILI,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Multimodal & Speech/Audio,,fear; speech; linguistics; emotion; theoretical; communication; science; datasets; computational; concepts work
423-MAIN,AdaptBPE: From General Purpose to Specialized Tokenizers,"Subword tokenization methods, such as Byte-Pair Encoding (BPE), significantly impact the performance and efficiency of large language models (LLMs). The standard approach involves training a general-purpose tokenizer that uniformly processes all textual data during both training and inference. However, the use of a generic set of tokens can incur inefficiencies when applying the model to specific domains or languages. To address this limitation, we propose a post-training adaptation strategy that selectively replaces low-utility tokens with more relevant ones based on their frequency in an adaptation corpus. Our algorithm identifies the token inventory that most effectively encodes the adaptation corpus for a given target vocabulary size. Extensive experiments on generation and classification tasks across multiple languages demonstrate that our adapted tokenizers compress test corpora more effectively than baselines using the same vocabulary size. This method serves as a lightweight adaptation mechanism, akin to a vocabulary fine-tuning process, enabling optimized tokenization for specific domains or tasks.",Vijini Pilana Liyanage; FranГ§ois Yvon,Pilana Liyanage Vijini Supun Keerthisrini,Oral,In-person,Pavillon  DE RABAT,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Efficiency, Scaling & NLP Systems",Summarization & Generation,adaptation; vocabulary; specific domains; tokenizers; tokenization; general purpose; purpose; size; corpus; tokens
424-MAIN,Measuring Linguistic Competence of LLMs on Indigenous Languages of the Americas,"This paper presents an evaluation framework for probing large language modelsвЂ™ linguistic knowledge of Indigenous languages of the Americas using zero- and few-shot prompting. The framework consists of three tasks: (1) language identification, (2) cloze completion of Spanish sentences supported by Indigenous-language translations, and (3) grammatical feature classification. We evaluate models from five major families (GPT, Gemini, DeepSeek, Qwen, and LLaMA) on 13 Indigenous languages, including Bribri, Guarani, and Nahuatl. The results show substantial variation across both languages and model families. While a small number of model-language combinations demonstrate consistently stronger performance across tasks, many others perform near chance, highlighting persistent gaps in current modelsвЂ™ abilities on Indigenous languages.",Justin Vasselli; Arturo MP; Frederikus Hudi; Haruki Sakajo; Taro Watanabe,Justin Vasselli,Oral,In-person,SALLE  WALILI,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Interpretability & Model Analysis,"Linguistics, Syntax & Semantics",indigenous; indigenous languages; languages; modelsвђ; families; measuring linguistic; competence llms; using zero; prompting framework; framework consists
425-MAIN,Reassessing Active Learning Adoption in Contemporary NLP: A Community Survey,"Supervised learning relies on data annotation which usually is time-consuming and therefore expensive. A longstanding strategy to reduce annotation costs is active learning, an iterative process, in which a human annotates only data instances deemed informative by a model. Research in active learning has made considerable progress, especially with the rise of large language models (LLMs). However, we still know little about how these remarkable advances have translated into real-world applications, or contributed to removing key barriers to active learning adoption. To fill in this gap, we conduct an online survey in the NLP community to collect previously intangible insights on current implementation practices, common obstacles in application, and future prospects in active learning. We also reassess the perceived relevance of data annotation and active learning as fundamental assumptions. Our findings show that data annotation is expected to remain important and active learning to stay highly relevant while benefiting from LLMs. Consistent with a community survey from over 15 years ago, three key challenges yet persist---setup complexity, uncertain cost reduction, and tooling---for which we propose alleviation strategies. We publish an anonymized version of the dataset.",Julia Romberg; Christopher SchrГ¶der; Julius Gonsior; Katrin Tomanek; Fredrik Olsson,Julia Romberg,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Dialogue, Conversational & Interactive NLP",,active learning; active; learning; data annotation; annotation; community; survey; adoption; nlp; key
430-MAIN,Beyond вЂњNot Novel EnoughвЂќ: Enriching Scholarly Critique with LLM-Assisted Feedback,"Novelty assessment is a central yet understudied aspect of peer review, particularly in high-volume fields like NLP where reviewer capacity is increasingly strained. We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence-based assessment. Our method is informed by analysis of human-written novelty reviews and captures key patterns such as independent claim verification and contextual reasoning. Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusionsвЂ”substantially outperforming existing LLM-based baselines. The method produces detailed, literature-aware analyses and improves consistency over ad hoc reviewer judgments. These results highlight the potential for structured LLM-assisted approaches to support more rigorous and transparent peer review without displacing human expertise. Data and code are made available.",OSAMA MOHAMMED AFZAL; Preslav Nakov; Tom Hope; Iryna Gurevych,Preslav Nakov,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Retrieval, Grounding & External Knowledge (RAG)",novelty; reviewer; peer review; assisted; peer; structured; review; human; assessment; particularly high
432-MAIN,AfriVox: Probing Multilingual and Accent Robustness of Speech LLMs,"Recent advances in multimodal and speech-native large language models (LLMs) have delivered impressive speech recognition, translation, understanding, and question-answering capabilities for high-resource languages. However, African languages and non-native French or English accents remain dramatically underrepresented in benchmarks limiting the understanding and applicability of leading LLMs for millions of francophone and anglophone users in low-resource settings. We presents AfriVox, an open-source benchmark (including novel domain-specific and unscripted datasets) across 20 African languages, African-accented French, Arabic, and 100+ African English accents, contrasting leading multimodal speech LLMs with traditional unimodal automatic speech transcription (ASR) and translation (AST) models. Our analysis reveals significant language coverage variation, surprising LLM translation performance gains (e.g. Gemini), robustness concerns with unscripted speech, and substantial performance disparities for ""supported"" African languages. We profile the strengths, limitations, and language support of each model, and conduct the first targeted fine-tuning of a modern speech LLM (Qwen2.5-Omni) for three Nigerian languages, exceeding SOTA, and achieving up to 54% relative WER reduction and significant BLEU gains, offering practical guidance for implementers seeking to serve local language users.",Busayo Awobade; Mardhiyah Sanni; Tassallah Abdullahi; Chibuzor Okocha; Kelechi Ezema; Devendra Deepak Kayande; Lukman Enegi Ismaila; Tobi Olatunji; Gloria Ashiya Katuka,Busayo Awobade,Oral,In-person,SALLE  WALILI,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Multimodal & Speech/Audio,Multilinguality & Low-Resource NLP,african; speech; african languages; languages; speech llms; multimodal speech; translation; french; native; users
433-MAIN,PortOldBERT: Portuguese Historical Language Models,"Historical language models play a crucial role in the study of languages, and can benefit tasks such as named-entity recognition (NER), part-of-speech (PoS) tagging, and post-OCR correction, among others. Despite their relevance, most efforts have been concentrated on English. To the best of our knowledge, no such model exists for historical Portuguese. In this work, we introduce PortOldBERT, the first historical Portuguese encoder language model. We demonstrate its usefulness by comparing PortOldBERT's performance with Albertina, the encoder on which it is based, across multiple tasks---pseudo-perplexity, NER, PoS tagging, word error rate (WER) prediction, and OCR error detection---and for different historical periods. PortOldBERT consistently outperforms Albertina in historical data, demonstrating its ability to effectively integrate historical linguistic contexts while retaining the ability to process contemporary text.",Tomas Freitas Osorio; Henrique Lopes Cardoso,Tomás Freitas Osório,Oral,Virtual,ZOOM,Virtual TBA,,,Information Extraction & Structured Prediction,Multimodal & Speech/Audio,historical; portuguese; pos; tagging; ocr; ner; encoder; error; ability effectively; periods
435-MAIN,ReMedQA: Are We Done With Medical Multiple-Choice Benchmarks?,"Medical multiple-choice question answering (MCQA) benchmarks report near-human accuracy, with some approaching saturation and fueling claims of clinical readiness. Yet a single accuracy score is a poor proxy for competence: models that change answers under minor perturbations cannot be considered reliable. We argue that reliability underpins accuracyвЂ“only consistent predictions make correctness meaningful. To address this, we release ReMedQA, a benchmark suite that augments three standard medical MCQA datasets with open-answer variants and systematically perturbed items. Building on this design, we introduce ReAcc and ReCon, two reliability metrics: ReAcc measures the proportion of questions answered correctly across all variations, while ReCon measures the proportion answered consistently regardless of correctness. Our evaluation shows that high MCQA accuracy masks low reliability: models remain sensitive to format and perturbation changes, and domain specialization offers no robustness gain. MCQA further underestimates smaller models while inflating large ones that exploit structural cuesвЂ“with some producing correct answers without seeing the question. These findings show that, despite near-saturated accuracy, we are not yet done with medical multiple-choice benchmarks.",Alessio Cocchieri; Luca Ragazzi; Giuseppe Tagliavini; Gianluca Moro,Luca Ragazzi,Oral,In-person,SALLE  WALILI,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",Domain NLP (Biomedical/Clinical/Legal/Scientific),mcqa; medical multiple choice; medical multiple; medical; multiple choice; answered; choice; reliability; proportion; near
441-MAIN,Can activation steering support language-agnostic reasoning in language models? A study on syllogistic inferences,"Large Language Models (LLMs) struggle with syllogistic reasoning, frequently conflating content plausibility with logical validity. This well-known content effect undermines their capacity to act as reliable deductive reasoners, particularly in multilingual contexts where both linguistic variability and world knowledge may exacerbate biases. Prior work shows that prompting and tuning interventions can alleviate these issues only partially, leaving models vulnerable to semantic interference. To make reasoning more consistent, robust, and transferable across languages, we investigate the use of activation steeringвЂ”an inference-time intervention that modulates internal representations to suppress content effects. Our experiments demonstrate that steering techniques constructed for English-based syllogisms generalise effectively to multilingual datasets, yielding higher formal reasoning accuracy while minimally affecting language modelling performance. Moreover, steering supports partial transfer to out-of-distribution tasks, highlighting its potential as a scalable mechanism for content-invariant reasoning. These findings advance the prospect of developing LLMs that can serve as reliable soft reasoners across language landscapes.",Gabriele Maraia; Leonardo Ranaldi; Marco Valentino; Fabio Massimo Zanzotto,Fabio Massimo Zanzotto,Oral,In-person,SALLE  LE RIAD,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multilinguality & Low-Resource NLP,Interpretability & Model Analysis,steering; syllogistic; reasoners; content; reasoning; activation; reliable; content effect; modulates; suppress
446-MAIN,Morpheme Matters: Morpheme-Based Subword Tokenization for Korean Language Models,"Tokenization plays a crucial role in the performance of language models. However, most existing tokenizers rely on frequency-based segmentation, which fails to capture the morphological structure of languages and often leads to inefficient token representations. In this study, we propose a novel tokenization method that emphasizes the importance of Korean morphological structures in eojeol (Korean spacing unit). This method is designed to accommodate both inter-eojeol segmentation and intra-eojeol segmentation, enabling the selection of subwords based on morphemes. We pretrained a language model using the proposed method and evaluated its performance on Korean benchmark tasks. Experimental results demonstrate that the proposed method generally outperforms existing approaches. Notably, it produces significantly fewer tokens per input sequence, indicating its effectiveness and efficiency for Korean language modeling.",DongHyeok Lee; Jeongyeon Park; Kyungbeen Cho; Jae Sung Lee,Donghyeok Lee,Oral,In-person,SALLE  LE LIXUS,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Efficiency, Scaling & NLP Systems","LLM Evaluation, Benchmarks & Metrics",korean; segmentation; tokenization; morpheme; morphological; proposed method; proposed; demonstrate proposed method; accommodate; fewer tokens
447-MAIN,SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space,"Multimodal large language models (MLLMs) have shown impressive capabilities in vision-language tasks such as reasoning segmentation, where models generate segmentation masks based on textual queries. While prior work has primarily focused on perturbing image inputs, semantically equivalent textual paraphrasesвЂ”crucial in real-world applications where users express the same intent in varied waysвЂ”remain underexplored. To address this gap, we introduce a novel adversarial paraphrasing task: generating grammatically correct paraphrases that preserve the original query meaning while degrading segmentation performance. To evaluate the quality of adversarial paraphrases, we develop a comprehensive automatic evaluation protocol validated with human studies. Furthermore, we introduce SPARTAвЂ”a black-box, sentence-level optimization method that operates in the low-dimensional semantic latent space of a text autoencoder, guided by reinforcement learning. SPARTA achieves significantly higher success rates, outperforming prior methods by up to 2x on both the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive baselines to assess the robustness of advanced reasoning segmentation models. We reveal that they remain vulnerable to adversarial paraphrasingвЂ”even under strict semantic and grammatical constraints. All code and data will be released publicly upon acceptance.",Viktoriia Zinkovich; Anton Antonov; Andrei Spiridonov; Denis Shepelev; Andrey Moskalenko; Daria Pugacheva; Elena Tutubalina; Andrey Kuznetsov; Vlad Shakhuro,Viktoriia Zinkovich,Oral,In-person,SALLE  LE RIAD,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multimodal & Speech/Audio,,segmentation; adversarial; latent space; autoencoder; paraphrases; paraphrasing; latent; black box; black; box
451-MAIN,Knowledge Augmentation Enhances Token Classification for Recipe Understanding,"In this work, we propose an entity type-specific and knowledge-augmented token classification framework designed to improve encoder models' performance on recipe texts. Our empirical analysis shows that this approach achieves state-of-the-art (SOTA) results on 5 out of 7 benchmark recipe datasets, significantly outperforming traditional token classification methods. We introduce a novel methodology leveraging curated domain-specific knowledge contexts to guide encoder models such as BERT and RoBERTa, which we refer to as RecipeBERT-KA and RecipeRoBERTa-KA. Additionally, we release a newly reprocessed entity type-specific and knowledge-enriched dataset that merges seven widely used food datasets, making it the largest annotated food-related dataset to date. Comparative analysis with SOTA large language models (GPT-4o, Mistral-7B, LLaMA 3-13B and LLaMA 3-70B) highlights the practical advantages of our smaller and specialised models. Finally, we analyse the impact of the different knowledge contexts, our models' potential for transfer learning, the effect of combining the datasets and scenarios where traditional token classification may still perform competitively, offering nuanced insight into method selection.",Nuhu Ibrahim; Robert Stevens; Riza Batista-Navarro,Nuhu Ibrahim,Oral,In-person,SALLE  WALILI,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Interpretability & Model Analysis,"LLM Evaluation, Benchmarks & Metrics",recipe; specific knowledge; classification; knowledge; token; food; encoder models; sota; type; encoder
452-MAIN,Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches,"Eliciting cooperation in multi-agent LLM systems is critical for AI alignment. We investigate two approaches: direct communication and curriculum learning. In a 4-player Stag Hunt, a one-word ""cheap talk"" channel increases cooperation from 0% to 48.3%, demonstrating communication as a robust coordination mechanism. In contrast, we find that curriculum learning is highly sensitive to design choices: our pedagogical curriculum through progressively complex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game with Punishment. Qualitative analysis reveals that curricula emphasizing defection-equilibrium games can induce ""learned pessimism"" in agents. These findings suggest that for coordination problems, simple communication protocols may be more reliable than experience-based training, and that curriculum design for social dilemmas requires careful attention to the strategic lessons embedded in game sequences.",Hachem Madmoun; Salem Lahlou,Salem Lahlou,Oral,In-person,Pavillon  DE RABAT,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Interpretability & Model Analysis,"Reasoning, Planning & Agents",curriculum; cooperation; communication; games; curriculum learning; coordination; game; design; agent; agents
454-MAIN,Argumentation and Judgement Factors: LLM-based Discovery and Application in Insurance Disputes,"In this work, we focus on discovery of legal factors for a specific case type under consideration (e.g., vehicle insurance disputes). We refer to these legal factors more explicitly as ""Argumentation and Judgement Factors"" (AJFs). AJFs encode specific legal knowledge that is important for legal argumentation and judicial decision making. We propose a multi-step approach for discovering a list of AJFs for a given case type using a set of relevant legal documents (e.g., past judgements, relevant acts) and Symbolic Knowledge Distillation (SKD) from a Large Language Model (LLM). We propose a novel geneRatE-CRitic-reviEW (RECREW) prompting strategy for effective SKD. We construct and evaluate the discovered list of AJFs on two different types of cases (auto-insurance and life insurance) and show their utility in a dispute resolution application.",Basit Ali; Anubhav sinha; Nitin Ramrakhiyani; Sachin Pawar; Girish Keshav Palshikar; Manoj Apte,Anubhav Sinha,Oral,In-person,SALLE  WALILI,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Reasoning, Planning & Agents",Domain NLP (Biomedical/Clinical/Legal/Scientific),insurance; legal; argumentation; factors; list; discovery; type; case; application; relevant
455-MAIN,ViGoEmotions: A Benchmark Dataset using LLM Annotation For Fine-grained Emotion Detection on Vietnamese Texts,"Emotion classification, a vital aspect of natural language processing (NLP), plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotion - a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT.These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.",Tran Quang Hung; Pham Tien Nam; Son T. Luu; Kiet Van Nguyen,Hung Quang Tran,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",,emotion; normalization; converting; emotion classification; vietnamese; preprocessing; f1 score; corpus; based; annotation
456-MAIN,PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs,"Current evaluations of sentence embedding models typically rely on static test beds such as the Massive Text Embedding Benchmark (MTEB). While invaluable, repeated tuning on a fixed suite can inflate reported performance and obscure real-world robustness. We introduce the Paraphrasing Text Embedding Benchmark (PTEB), a dynamic protocol that stochastically generates meaning-preserving paraphrases at evaluation time and aggregates results across multiple runs. Using a cost-efficient LLM-based method grounded in semantic textual similarity gold ratings, we show that LLMs generate token-diverse but semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our hypothesis that the performance of sentence encoders is sensitive to changes in token space even when semantics remain fixed. We also observe that smaller models are not disproportionately affected relative to larger ones. Our results are statistically robust over multiple runs and we extended our experiments to 3 multilingual datasets covering 10 languages. More generally, we aim to propose a new evaluation paradigm in NLP that relies less on static, pre-defined benchmarks but shifts towards dynamic, stochastic evaluation leveraging eval-time compute. We make the code to run PTEB publicly available.",Manuel Frank; Haithem Afli,Manuel Frank,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Linguistics, Syntax & Semantics",text embedding; embedding; multiple runs; embedding benchmark; text embedding benchmark; runs; mteb; stochastic; paraphrases; paraphrasing
458-MAIN,DETECT: Determining Ease and Textual Clarity of German Text Simplifications,"Current evaluation of German automatic text simplification (ATS) relies on general-purpose metrics such as SARI, BLEU, and BERTScore, which insufficiently capture simplification quality in terms of simplicity, meaning preservation, and fluency. While specialized metrics like LENS have been developed for English, corresponding efforts for German have lagged behind due to the absence of human-annotated corpora. To close this gap, we introduce DETECT, the first German-specific metric that holistically evaluates ATS quality across all three dimensions of simplicity, meaning preservation, and fluency, and is trained entirely on synthetic large language model (LLM) responses. Our approach adapts the LENS framework to German and extends it with (i) a pipeline for generating synthetic quality scores via LLMs, enabling dataset creation without human annotation, and (ii) an LLM-based refinement step for aligning grading criteria with simplification requirements. To the best of our knowledge, we also construct the largest German human evaluation dataset for text simplification to validate our metric directly. Experimental results show that DETECT achieves substantially higher correlations with human judgments than widely used ATS metrics, with particularly strong gains in meaning preservation and fluency. Beyond ATS, our findings highlight both the potential and the limitations of LLMs for automatic evaluation and provide transferable guidelines for general language accessibility tasks.",Maria Korobeynikova; Alessia Battisti; Lukas Fischer; Yingqiang Gao,Maria Korobeynikova,Oral,In-person,Pavillon  DE RABAT,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics",,german; simplification; meaning preservation; preservation; fluency; meaning; text simplification; detect; simplicity; metrics
460-MAIN,MathEDU: Feedback Generation on Problem-Solving Processes for Mathematical Learning Support,"The increasing reliance on Large Language Models (LLMs) across various domains extends to education, where students progressively use generative AI as a tool for learning. While prior work has examined LLMs' mathematical ability, their reliability in grading authentic student problem-solving processes and delivering effective feedback remains underexplored. This study introduces MathEDU, a dataset consisting of student problem-solving processes in mathematics and corresponding teacher-written feedback. We systematically evaluate the reliability of various models across three hierarchical tasks: answer correctness classification, error identification, and feedback generation. Experimental results show that fine-tuning strategies effectively improve performance in classifying correctness and locating erroneous steps. However, the generated feedback across models shows a considerable gap from teacher-written feedback. Critically, the generated feedback is often verbose and fails to provide targeted explanations for the student's underlying misconceptions. This emphasizes the urgent need for trustworthy and pedagogy-aware AI feedback in education.",Wei-Ling Hsu; Yu-Chien Tang; An-Zi Yen,An-Zi Yen,Oral,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP",Summarization & Generation,feedback; problem solving; student; processes; solving; feedback generation; generated feedback; teacher; problem; education
462-MAIN,Test-Time Scaling of Reasoning Models for Machine Translation,"Test-time scaling (TTS) has enhanced the performance of Reasoning Models (RMs) on various tasks such as math and coding, yet its efficacy in machine translation (MT) remains underexplored. This paper investigates whether increased inference-time computation improves translation quality. We evaluate 12 RMs across a diverse suite of MT benchmarks spanning multiple domains, examining three scenarios: direct translation, forced-reasoning extrapolation, and post-editing. Our findings show that for general-purpose RMs, TTS provides limited and inconsistent benefits for direct translation, with performance quickly plateauing. However, the effectiveness of TTS is unlocked by domain-specific fine-tuning, which aligns a modelвЂ™s reasoning process with task requirements, leading to consistent improvements up to an optimal, self-determined reasoning depth. We also find that forcing a model to reason beyond its natural stopping point consistently degrades translation quality. In contrast, TTS proves highly effective in a post-editing context, reliably turning self-correction into a beneficial process. These results indicate that the value of inference-time computation in MT lies not in enhancing single-pass translation with general models, but in targeted applications like multi-step, self-correction workflows and in conjunction with task-specialized models.",Zihao Li; Shaoxiong Ji; JГ¶rg Tiedemann,Zihao Li,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Machine Translation,"Reasoning, Planning & Agents",translation; tts; rms; test time scaling; time scaling; time; self correction; self; reasoning; editing
465-MAIN,How Good Are LLMs at Processing Tool Outputs?,"Most realistic task automation problems require large language models (LLMs) to call tools, which often return complex JSON responses. These responses must be further processed to derive the information necessary for task completion. The ability of LLMs to do so is under-studied. In this paper, we study the tool response processing task and LLMs' abilities to process structured (JSON) responses. We created a dataset for this task, and evaluated 15 open and closed weight models using multiple prompting approaches. Our results show that JSON processing remains a difficult task even for frontier models across multiple prompting strategies. The optimal response processing strategy depends on both the nature and size of the tool outputs, as well as the complexity of the required reasoning. Variations in processing approaches can lead to performance differences ranging from 3% to 50%.",Kiran Kate; Yara Rizk; Poulami Ghosh; Ashu Gulati; Tathagata Chakraborti; Zidane Wright; Mayank Agarwal,Zidane Wright,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",,processing; json; multiple prompting; tool; responses; response; prompting; outputs; llms; open closed weight
468-MAIN,Tug-of-war between idioms' figurative and literal interpretations in LLMs,"Idioms present a unique challenge for language models due to their non-compositional figurative interpretations, which often strongly diverge from the idiom's literal interpretation. In this paper, we employ causal tracing to systematically analyze how pretrained causal transformers deal with this ambiguity. We localize three mechanisms: (i) Early sublayers and specific attention heads retrieve figurative interpretation, while suppressing literal interpretation. (ii) When disambiguating context precedes the idiom, the model leverages it from the earliest layer and later layers refine the interpretation if the context conflicts with the retrieved interpretation. (iii) Then, selective, competing pathways carry both interpretations: an intermediate pathway that prioritizes the figurative interpretation and a parallel direct route that favors literal interpretation, ensuring that both readings remain available. Our findings provide mechanistic evidence for idiom comprehension in autoregressive transformers.",Soyoung Oh; Xinting Huang; Mathis Pink; Michael Hahn; Vera Demberg,Soyoung Oh,Oral,In-person,SALLE  LA PALMERAIE,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Interpretability & Model Analysis,"Retrieval, Grounding & External Knowledge (RAG)",interpretation; literal; figurative; interpretations; idioms; transformers; causal; tug; deal; localize
469-MAIN,Do LLM hallucination detectors suffer from low-resource effect?,"LLMs, while outperforming humans in a wide range of tasks, can still fail in unanticipated ways. We focus on two pervasive failure modes: (i) hallucinations, where models produce incorrect information about the world, and (ii) the low-resource effect, where the models show impressive performance in high-resource languages like English but the performance degrades significantly in low-resource languages like Bengali. We study the intersection of these issues and ask: do hallucination detectors suffer from the low-resource effect? We conduct experiments on five tasks across three domains (factual recall, STEM, and Humanities). Experiments with four LLMs and three hallucination detectors reveal a curious finding: As expected, the task accuracies in low-resource languages experience large drops (compared to English). However, the drop in hallucination detection accuracy is often several times smaller than the drop in task accuracy. Our finding suggests that even in low-resource languages, the internal mechanisms of LLMs might encode valuable signals about their uncertainty.",Debtanu Datta; Mohan Kishore Chilukuri; Yash Kumar; Saptarshi Ghosh; Muhammad Bilal Zafar,Debtanu Datta,Oral,In-person,SALLE  WALILI,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Multilinguality & Low-Resource NLP,"LLM Evaluation, Benchmarks & Metrics",low resource; resource; low; hallucination; resource languages; detectors; effect; low resource languages; resource languages like; languages
475-MAIN,Mind Your Special Tokens! On the Importance of Dedicated Sequence-End Tokens in Vision-Language Embedding Models,"Large Vision-Language Models (LVLMs), trained by aligning visual encoders to LLMs on extensive vision-language data, demonstrate impressive performance across a broad variety of tasks that require understanding of both visual and textual inputs. Acknowledging this, recent work proposed to post-hoc convert generative LVLMs into vision-language encoders (VLEs) via supervised contrastive learning objectives. This type of training enables LVLMs to produce better representations, i.e., embeddings for image and text input, used in retrieval and (semantic) similarity tasks. Having observed that this type of VLEs (i.e., LVLMs turned into encoders) commonly employ last-token pooling in downstream tasks, without using special sequence-end tokens, in this focused contribution, we study the effect of pooling strategies on VLEs' downstream performance. We empirically show that, in contrast to mean pooling, last-token pooling (without special sequence-end tokens) makes VLEs highly sensitive to end-of-input artifacts in fine-tuning and inference data, e.g., whether input sequences end with punctuation or newline characters. Finally, we show that introducing the special end-of-sequence token removes this sensitivity and makes VLEs robust to formatting artifacts of input text.",Elio Musacchio; Giovanni Semeraro; Goran GlavaЕЎ,Elio Musacchio,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Multimodal & Speech/Audio,"Efficiency, Scaling & NLP Systems",special; pooling; end; lvlms; sequence end; sequence; vision language; encoders; tokens; input
484-MAIN,Coupling Local Context and Global Semantic Prototypes via a Hierarchical Architecture for Rhetorical Roles Labeling,"Rhetorical Role Labeling (RRL) identifies the functional role of each sentence in a document, a key task for discourse understanding in domains such as law and medicine. While hierarchical models capture local dependencies effectively, they are limited in modeling global, corpus-level features. To address this limitation, we propose two prototype-based methods that integrate local context with global representations. Prototype-Based Regularization (PBR) learns soft prototypes through a distance-based auxiliary loss to structure the latent space, while Prototype-Conditioned Modulation (PCM) constructs corpus-level prototypes and injects them during training and inference. Given the scarcity of RRL resources, we introduce SCOTUS-Law, the first dataset of U.S. Supreme Court opinions annotated with rhetorical roles at three levels of granularity: category, rhetorical function, and step. Experiments on legal, medical, and scientific benchmarks show consistent improvements over strong baselines, with $\sim4$ Macro-F1 gains on low-frequency roles. We further analyze the implications in the era of Large Language Models and complement our findings with expert evaluation.",Anas Belfathi; Nicolas Hernandez; Monceaux Laura; Warren Bonnard; Mary Catherine LavissiГЁre; Christine Jacquin; Richard Dufour,Anas Belfathi,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Domain NLP (Biomedical/Clinical/Legal/Scientific),,rhetorical; prototypes; roles; local; corpus level; local context; global; labeling; law; hierarchical
485-MAIN,Guided by the Plan: Enhancing Faithful Autoregressive Text-to-Audio Generation with Guided Decoding,"Autoregressive (AR) models excel at generating temporally coherent audio by producing tokens sequentially, yet they often falter in faithfully following complex textual promptsвЂ”especially those describing complex sound events. We uncover a surprising capability in AR audio generators: their early prefix tokens implicitly encode global semantic attributes of the final output, such as event count and sound-object category, revealing a form of implicit planning. Building on this insight, we propose Plan-Critic, a lightweight auxiliary model trained with a Generalized Advantage Estimation (GAE)-inspired objective to predict final instruction-following quality from partial generations. At inference time, Plan-Critic enables guided exploration: it evaluates candidate prefixes early, prunes low-fidelity trajectories, and reallocates computation to high-potential planning seeds. Our Plan-Critic-guided sampling achieves up to a 10 points improvement in CLAP score over the AR baselineвЂ”establishing a new state of the art in AR text-to-audio generationвЂ”while maintaining computational parity with standard best-of-N decoding. This work bridges the gap between causal generation and global semantic alignment, demonstrating that even strictly autoregressive models can plan ahead.",Juncheng Wang; Zhe Hu; Chao Xu; Siyue Ren; Yuxiang Feng; Yang Liu; Baigui Sun; Shujun Wang,Juncheng Wang,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,Summarization & Generation,plan; critic; audio; guided; autoregressive; global semantic; sound; text audio; early; planning
486-MAIN,Safe-Unsafe Concept Separation Emerges from a Single Direction in Language Models Activation Space,"Deploying Large Language Models (LLMs) in high-stakes applications requires reliable safeguarding mechanisms. Existing approaches typically rely on costly fine-tuning or reinforcement learning from human feedback, which are resource-intensive and difficult to adapt. We propose a lightweight, concept activation-based methodology that identifies the layer where safe and unsafe concepts are maximally separable in a pretrained modelвЂ™s representation space. By selecting this optimal layer and training a simple linear classifier on its activations, we enable effective safety detection without modifying model weights or requiring extensive labeled data. We validate our framework across multiple safety-critical domains (regulation, law, finance, cybersecurity, education, code, human resources, and social media), diverse tasks (safety classification, prompt injection, and toxicity detection), and 16 non-English languages, on both encoder and decoder architectures. Results show that: (i) the separation between safe and unsafe concepts emerges from a single layer direction in the activation space, and (ii) our approach consistently outperforms existing guardrail solutions while drastically reducing computational and annotation costs.",Antonio Serino; Andrea Ermellino; Lorenzo Malandri; Fabio Mercorio,Lorenzo Malandri,Oral,In-person,SALLE  LE LIXUS,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness",,safe unsafe; unsafe; safe; activation; layer; emerges; activation space; space; separation; safety
490-MAIN,PEFT-Bench: A Parameter-Efficient Fine-Tuning Methods Benchmark,"Despite the state-of-the-art performance of Large Language Models (LLMs) achieved on many tasks, their massive scale often leads to high computational and environmental costs, limiting their accessibility. Parameter-efficient fine-tuning (PEFT) methods address this challenge by reducing the number of trainable parameters while maintaining strong downstream performance. Despite the increased development in PEFT methods, current evaluations remain limited (in terms of evaluated models and datasets) and difficult to reproduce. To bridge this gap, we introduce PEFT-Bench, a unified end-to-end benchmark for evaluating diverse PEFT methods on autoregressive LLMs. We demonstrate its usage across 27 NLP datasets and 6 PEFT methods. To account for different PEFT training and inference factors, we also introduce the PEFT Soft Score Penalties (PSCP) metric, which takes trainable parameters, inference speed, and training memory usage into account.",Robert Belanec; Branislav Pecher; Ivan Srba; Maria Bielikova,Robert Belanec,Oral,In-person,Pavillon  DE RABAT,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Efficiency, Scaling & NLP Systems","LLM Evaluation, Benchmarks & Metrics",peft; trainable parameters; trainable; methods; parameter efficient fine; efficient fine; efficient fine tuning; parameter efficient; account; bench
496-MAIN,Decoding the Market's Pulse: Context-Enriched Agentic Retrieval Augmented Generation for Predicting Post-Earnings Price Shocks,"Accurately forecasting large stock price movements after corporate earnings announcements is a longstanding challenge. Existing methods--sentiment lexicons, fine-tuned encoders, and standalone LLMs--often **lack temporal-causal reasoning** and are prone to **narrative bias**, echoing overly optimistic managerial tone. We introduce **Context-Enriched Agentic RAG (CARAG)**, a retrieval-augmented framework that deploys a team of cooperative LLM agents, each specializing in a distinct analytical task: evaluating historical performance, assessing the credibility of guidance, or benchmarking against peers. Agents retrieve structured evidence from a Causal-Temporal Knowledge Graph (CTKG) built from financial statements and earnings calls, enabling grounded, context-rich reasoning. This design mitigates LLM hallucinations and produces more objective predictions. Without task-specific training, our system achieves state-of-the-art zero-shot performance across NASDAQ, NYSE, and MAEC datasets, outperforming both larger LLMs and fine-tuned models in macro-F1, MCC, and Sharpe, beating market benchmarks (S&P 500 and Nasdaq) for the same forecasting horizon. Code, datasets, prompts, and implementation details are included in the supplementary material to ensure full reproducibility.",Chenhui Li; Weihai Lu,Chenhui Li,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Retrieval, Grounding & External Knowledge (RAG)",,price; market; enriched; agentic; causal; context; temporal; fine tuned; retrieval augmented; tuned
498-MAIN,LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring,"Automated Essay Scoring (AES) has gained increasing attention in recent years, yet research on Arabic AES remains limited due to the lack of publicly available datasets. To address this, we introduce LAILA, the largest publicly available Arabic AES dataset to date, comprising 7,859 essays annotated with holistic and trait-specific scores on seven dimensions: relevance, organization, vocabulary, style, development, mechanics, and grammar. We detail the dataset design, collection, and annotations, and provide benchmark results using state-of-the-art Arabic and English models in prompt-specific and cross-prompt settings. LAILA fills a critical need in Arabic AES research, supporting the development of robust scoring systems.",May Bashendy; Walid Massoud; Sohaila Eltanbouly; Salam Albatarni; Marwan Sayed; Abrar Abir; Houda Bouamor; Tamer Elsayed,Tamer Elsayed,Oral,In-person,SALLE  LA PALMERAIE,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",arabic; scoring; essay; trait; publicly available; publicly; development; automated; dataset; available
503-MAIN,Live API-Bench: 2500+ Live APIs for Testing Multi-Step Tool Calling,"Large language models (LLMs) increasingly rely on external tools and APIs to execute complex tasks specified in natural language. Evaluating such {\em tool-calling} capabilities in realistic enterprise settings is challenging: APIs are often proprietary, heterogeneous, and difficult to share, limiting reproducible benchmarks. To address this, we introduce {\bf Live API Bench}, a comprehensive benchmark constructed by transforming NL2SQL datasets into interactive API environments. Our pipeline converts SQL queries from BIRD-SQL into executable API sequences across three formulationsвЂ”{\em SLOT}, {\em SEL}, and {\em REST}вЂ”covering minimal general-purpose operations, domain-specific multi-step tasks, and function-oriented RESTful interactions, respectively. The benchmark spans 11 databases with over 2,500 invocable tools, paired with human-authored queries, ground-truth API sequences, and verified final answers. Live API Bench enables systematic evaluation of core challenges in tool use, including error handling, sequential reasoning, parameter generation, response parsing, and robustness across diverse domains. We evaluate 10 LLMs and 4 ReACT agents, observing low task completion rates (7вЂ“47%), which improve modestly to 50% under interactive agent settings, highlighting substantial scope for improving LLM tool-calling performance. We release all code and data associated with this paper.",Benjamin Elder; Anupama Murthi; Jungkoo Kang; Ankita Naik; Kinjal Basu; Kiran Kate; Danish Contractor,Benjamin Elder,Oral,In-person,Pavillon  DE RABAT,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",api; live; tool calling; apis; calling; tool; bench; sql; sequences; interactive
513-MAIN,MALicious INTent Dataset and Inoculating LLMs for Enhanced Disinformation Detection,"The intentional creation and spread of disinformation poses a significant threat to public discourse. However, existing English datasets and research rarely address the intentionality behind the disinformation. This work presents MALINT, the first human-annotated English corpus developed in collaboration with expert fact-checkers to capture disinformation and its malicious intent. We utilize our novel corpus to benchmark 12 language models, including small language models (SLMs) such as BERT and large language models (LLMs) like Llama 3.3, on binary and multilabel intent classification tasks. Moreover, inspired by inoculation theory from psychology and communication studies, we investigate whether incorporating knowledge of malicious intent can improve disinformation detection. To this end, we propose intent-based inoculation, an intentвЂ‘augmented reasoning for LLMs that integrates intent analysis to mitigate the persuasive impact of disinformation. Analysis on six disinformation datasets, five LLMs, and seven languages shows that intentвЂ‘augmented reasoning improves zeroвЂ‘shot disinformation detection. To support research in intentвЂ‘aware disinformation detection, we release the MALINT dataset with annotations from each annotation step.",Arkadiusz Modzelewski; Witold Sosnowski; Eleni Papadopulos; Elisa Sartori; Tiziano Labruna; Adam Wierzbicki; Giovanni Da San Martino,Arkadiusz Modzelewski,Oral,In-person,SALLE  LE RIAD,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Interpretability & Model Analysis,"LLM Evaluation, Benchmarks & Metrics",disinformation; intent; malicious; detection; augmented reasoning; corpus; augmented; llms; english datasets; bert large
518-MAIN,When Meanings Meet: Investigating the Emergence and Quality of Shared Concept Spaces during Multilingual Language Model Training,"Training Large Language Models (LLMs) with high multilingual coverage is becoming increasingly important вЂ” especially when monolingual resources are scarce. Recent studies have found that LLMs process multilingual inputs in shared concept spaces, thought to support generalization and cross-lingual transfer. However, these prior studies often do not use causal methods, lack deeper error analysis or focus on the final model only, leaving open how these spaces emerge *during training*. We investigate the development of language-agnostic concept spaces during pretraining of EuroLLM through the causal interpretability method of activation patching. We isolate cross-lingual concept representations, then inject them into a translation prompt to investigate how consistently translations can be altered, independently of the language. We find that *shared concept spaces emerge early and continue to refine*, but that *alignment with them is language-dependent*. Furthermore, in contrast to prior work, our fine-grained manual analysis reveals that some apparent gains in translation quality reflect shifts in behavior вЂ” like selecting senses for polysemous words or translating instead of copying cross-lingual homographs вЂ” rather than improved translation ability. Our findings offer new insight into the training dynamics of cross-lingual alignment and the conditions under which causal interpretability methods offer meaningful insights in multilingual contexts.",Felicia KГ¶rner; Max MГјller-Eberstein; Anna Korhonen; Barbara Plank,Felicia Körner,Oral,In-person,SALLE  LA PALMERAIE,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multilinguality & Low-Resource NLP,Interpretability & Model Analysis,spaces; concept; cross lingual; lingual; shared; causal; multilingual; cross; translation; emerge
519-MAIN,Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models,"Does the prior knowledge of the vision encoder constrain the capability boundary of Multi-modal Large Language Models (MLLMs)? While most existing research treats MLLMs as unified systems optimized through end-to-end training, the impact of vision encoder's prior knowledge is seldom investigated. In this work, we introduce a novel metric $Rank_e$ to quantify the effect of prior knowledge of the vision encoder on MLLM performance. Our analysis reveals a positive correlation between prior knowledge and MLLM performance. Moreover, we find that domain-specific fine-tuning using solely end-to-end visual question answering (VQA) data is insufficient, particularly for entities with low inherent visual prior knowledge. To address this issue, we propose VisPRE (Vision Prior Remediation), a two-stage training framework that explicitly incorporates prior knowledge at the vision encoder level. Experimental results demonstrate that augmenting vision encoder's prior knowledge substantially boosts the visual understanding capabilities of MLLMs, offering a novel and effective strategy for improving performance, especially in scenarios involving uncommon visual entities.",Qiao Liang; Yanjiang Liu; Weixiang Zhou; Ben He; Yaojie Lu; Hongyu Lin; Jia Zheng; Xianpei Han; Le Sun; Yingfei Sun,Qiao Liang,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",prior knowledge; vision encoder; prior; vision; knowledge; encoder; end; visual; mllms; modal large language
520-MAIN,The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI,"Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRMвЂ™s reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting вЂњLost in Translation,"" where translation steps lead to errors that would have been avoided by reasoning in the language of the question.",Alan Saji; Raj Dabre; Anoop Kunchukuttan; Ratish Puduppully,Alan Saji,Oral,In-person,SALLE  LE RIAD,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Machine Translation,"Reasoning, Planning & Agents",reasoning; reasoning english; english; language question; lrms; answer accuracy; reasoning traces; traces; question; cognitive
522-MAIN,Classifying and Addressing the Diversity of Errors in Retrieval-Augmented Generation Systems,"Retrieval-augmented generation (RAG) is a prevalent approach for building LLM-based question-answering systems that can take advantage of external knowledge databases. Due to the complexity of real-world RAG systems, there are many potential causes for erroneous outputs. Understanding the range of errors that can occur in practice is crucial for robust deployment. We present a new taxonomy of the error types that can occur in realistic RAG systems, examples of each, and practical advice for addressing them. Additionally, we curate a dataset of erroneous RAG responses annotated by error types. We then propose an auto-evaluation method aligned with our taxonomy that can be used in practice to track and address errors during development. Code and data have been released publicly.",Kin Kwan Leung; Mouloud Belbahri; Yi Sui; Alex Labach; Xueying Zhang; Stephen Anthony Rose; Jesse C. Cresswell,Kin Kwan Leung; Mouloud Belbahri;,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",rag; errors; error types; occur; erroneous; systems; practice; rag systems; addressing; taxonomy
526-MAIN,Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application,"In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation, yet their interdisciplinary, fragmented, and fast-evolving expertise prevents general-purpose LLMs, lacking domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, the first large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) the first smart energy knowledge base, EnerBase, to enrich the modelвЂ™s foundational expertise; (2) the first instruction fine-tuning dataset, EnerInsruct, to strengthen performance on domain-specific downstream tasks; and (3) the first RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, the first benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.",Haoyu Jiang; Boan Qu; Fanjie Zeng; Xiaojie Lin; Wei Zhong,Fanjie Zeng,Oral,In-person,SALLE  LA PALMERAIE,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",smart; energy; domain knowledge; domain; rlhf; knowledge; human preferences; foundational; expertise; preferences
527-MAIN,AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders,"Sparse Autoencoders (SAEs) have emerged as a powerful interpretability tool for understanding neural network representations in text and vision domains, yet their application to audio models remains largely unexplored. We present the first comprehensive study applying SAEs to widely-used audio representation models вЂ“ Whisper and HuBERT вЂ“ training SAEs across all encoder layers and conducting extensive analysis of the learned features. We introduce a novel distributional semantics-based metric for comparing SAE features across different initializations, layers, and model architectures, demonstrating that over 50% of HuBERT features are robust across random seeds. Through systematic validation using clustering, classification, and interpretation methods, we discover that SAE features encode diverse information spanning multiple granularities: high-level characteristics (speech, music, environmental sounds), semantic content (phonemes), paralinguistic events (laughter, whispering, sneezing), and fine-grained acoustic patterns (speech boundaries, frequency structures). Our vowel unlearning experiments reveal significantly improved disentanglement in SAE representations, requiring removal of only 19-27% of features compared to over 90% for raw activations. We demonstrate practical utility by applying feature steering to reduce Whisper's hallucination error rate by 5.5% on non-speech audio while preserving speech recognition performance. Additionally, we find that approximately 20% of SAE features exhibit statistically significant correlation with human EEG responses during speech perception, suggesting alignment with neural processing mechanisms. The model weights, code for experiment reproduction and demo will be publicly available.",Aparin Georgii; Tasnima Sadekova; Alexey Rukhovich; Assel Yermekova; Laida Kushnareva; Vadim Popov; Kristian Kuznetsov; Irina Piontkovskaya,Georgii Aparin,Oral,In-person,SALLE  WALILI,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Interpretability & Model Analysis,Multimodal & Speech/Audio,features; sae; speech; sae features; audio; saes; whisper; autoencoders; sparse autoencoders; neural
529-MAIN,Vision-Language Models Align with Human Neural Representations in Concept Processing,"Recent studies suggest that transformer-based vision-language models (VLMs) capture the multimodality of concept processing in the human brain. However, a systematic evaluation exploring different types of VLM architectures and the role played by visual and textual context is still lacking. Here, we analyse multiple VLMs employing different strategies to integrate visual and textual modalities, along with language-only counterparts. We measure the alignment between concept representations by models and existing (fMRI) brain responses to concept words presented in two experimental conditions, where either visual (pictures) or textual (sentences) context is provided. Our results reveal that VLMs outperform the language-only counterparts in both experimental conditions. However, controlled ablation studies show that only for some VLMs, such as LXMERT and IDEFICS2, brain alignment stems from genuinely learning more human-like concepts during _pretraining_, while others are highly sensitive to the context provided at _inference_. Additionally, we find that vision-language encoders are more brain-aligned than more recent, generative VLMs. Altogether, our study shows that VLMs align with human neural representations in concept processing, while highlighting differences among architectures.",Anna Bavaresco; Marianne de Heer Kloots; Sandro Pezzelle; Raquel FernГЎndez,Anna Bavaresco,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",vlms; brain; concept; align human; textual; vision language; counterparts; processing; visual textual; vision
530-MAIN,FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning,"The growing collaboration between humans and AI models in generative tasks has introduced new challenges in distinguishing between *human-written*, *LLM-generated*, and *human-LLM collaborative* texts. In this work, we collect a multilingual, multi-domain, multi-generator dataset **FAIDSet**. We further introduce a fine-grained detection framework **FAID** to classify text into these three categories, and also to identify the underlying LLM family of the generator. Unlike existing binary classifiers, FAID is built to capture both authorship and model-specific characteristics. Our method combines multi-level contrastive learning with multi-task auxiliary classification to learn subtle stylistic cues. By modeling LLM families as distinct stylistic entities, we incorporate an adaptation to address distributional shifts without retraining for unseen data. Our experimental results demonstrate that FAID outperforms several baselines, particularly enhancing the generalization accuracy on unseen domains and new LLMs, thus offering a potential solution for improving transparency and accountability in AI-assisted writing.",Minh Ngoc Ta; Dong Cao Van; Duc-Anh Hoang; Minh Le-Anh; Truong Nguyen; My Anh Tran Nguyen; Yuxia Wang; Preslav Nakov; Dinh Viet Sang,Minh Ngoc Ta,Oral,In-person,SALLE  WALILI,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multilinguality & Low-Resource NLP,,multi; level contrastive; multi level; multi task; stylistic; contrastive learning; auxiliary; generator; unseen; contrastive
533-MAIN,BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data,"We present BabyBabelLM, a multilingual collection of datasets modeling the language a person observes from birth until they acquire a native language. We curate developmentally plausible pretraining data aiming to cover the equivalent of 100M English words of content in each of 45 languages. We compile evaluation suites and train baseline models in each language. BabyBabelLM aims to facilitate multilingual pretraining and cognitive modeling.",Jaap Jumelet; Abdellah Fourtassi; Akari Haga; Bastian Bunzeck; Bhargav Shandilya; Diana Galvan-Sosa; Faiz Ghifari Haznitrama; Francesca Padovani; Francois Meyer; Hai Hu; Julen Etxaniz; Laurent Prevot; Linyang He; MarГ­a Grandury; Mila Marcheva; Negar Foroutan; Nikitas Theodoropoulos; Pouya Sadeghi; Siyuan Song; Suchir Salhan; Susana Zhou; Yurii Paniv; Ziyin Zhang; Arianna Bisazza; Alex Warstadt; Leshem Choshen,Jaap Jumelet,Oral,In-person,Pavillon  DE RABAT,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",Multilinguality & Low-Resource NLP,developmentally plausible; developmentally; multilingual; plausible; pretraining; modeling; cognitive modeling; native language; birth; content 45
534-MAIN,Personality Editing for Language Models through Adjusting Self-Referential Queries,"Large Language Models (LLMs) are integral to applications such as conversational agents and content creation, where precise control over a modelвЂ™s personality is essential for maintaining tone, consistency, and user engagement. However, prevailing prompt-based or fine-tuning approaches either lack robustness or demand large-scale training data, making them costly and impractical. In this paper, we present PALETTE (Personality Adjustment by LLM SElf-TargeTed quEries), a novel method for personality editing in LLMs. Our approach introduces adjustment queries, where self-referential statements grounded in psychological constructs are treated analogously to factual knowledge, enabling direct editing of personality-related responses. Unlike fine-tuning, PALETTE requires only 12 editing samples to achieve substantial improvements in personality alignment across personality dimensions. Experimental results from both automatic and human evaluations demonstrate that our method enables more stable and well-balanced personality control in LLMs.",Seojin Hwang; Yumin Kim; Byeongjeong Kim; Donghoon Shin; Hwanhee Lee,Seojin Hwang,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Dialogue, Conversational & Interactive NLP","LLM Evaluation, Benchmarks & Metrics",personality; editing; personality editing; self; queries; control; content creation; approaches lack; scale training; editing llms
537-MAIN,How Much Pretraining Does Structured Data Need?,"Large language models (LLMs) are increasingly adopted for handling structured data, including tabular and relational inputs, despite mostly being pretrained on unstructured text. This raises a key question: how effectively do pretrained representations from language-focused LLMs transfer to tasks involving structured inputs? We address this through controlled experiments using two small open-source LLMs, systematically re-initializing subsets of layers with random weights before fine-tuning on structured datasets and comparing results to unstructured datasets. Our analyses show that, for structured data, most pretrained depth contributes little, with performance often saturating after the first few layers, whereas unstructured tasks benefit more consistently from deeper pretrained representations. Pretraining remains useful mainly in low-resource settings, with its impact diminishing as more training data becomes available.",Daniel Fadlon; Kfir Bar,Daniel Fadlon,Oral,In-person,SALLE  LA PALMERAIE,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multilinguality & Low-Resource NLP,,pretrained; structured data; structured; unstructured; pretraining; layers; inputs; representations; inputs despite; controlled experiments using
538-MAIN,Finding Culture-Sensitive Neurons in Vision-Language Models,"Despite their impressive performance, vision-language models (VLMs) still struggle on culturally situated inputs. To understand how VLMs process culturally grounded information, we study the presence of culture-sensitive neurons, i.e. neurons whose activations show preferential sensitivity to inputs associated with particular cultural contexts. We examine whether such neurons are important for culturally diverse visual question answering and where they are located. Using the CVQA benchmark, we identify neurons of culture selectivity and perform causal tests by deactivating the neurons flagged by different identification methods. Experiments on three VLMs across 25 cultural groups demonstrate the existence of neurons whose ablation disproportionately harms performance on questions about the corresponding cultures, while having minimal effects on others. Moreover, we propose a new margin-based selector: Contrastive Activation Selection (CAS), and show that it outperforms existing probability- and entropy-based methods in identifying culture-sensitive neurons. Finally, our layer-wise analyses reveals that such neurons tend to cluster in certain decoder layers. Overall, our findings shed new light on the internal organization of multimodal representations.",Xiutian Zhao; Rochelle Choenni; Rohit Saxena; Ivan Titov,Xiutian Zhao,Oral,In-person,SALLE  WALILI,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multimodal & Speech/Audio,"Trustworthy, Safety, Privacy & Fairness",neurons; culture; culturally; vlms; sensitive; cultural; inputs; vision language models; vision language; vision
540-MAIN,Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions,"Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect multilingualism has been studied on LLM factual recall, few studies have taken interest in language-induced variation in value-laden MCQ responses. This paper seeks to bridge that gap: do multilingual LLMs behave like polyglots, i.e. answer the same question the same way across languages, or do they answer value-laden MCQs depending on the language, like a multitude of monolingual models? Unlike prior work relying on machine translation or ad hoc prompts, we release a new corpus solely comprising human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to about forty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while instruction-tuned, larger models display higher consistency overall, that consistency varies greatly across questions, with certain MCQs eliciting total agreement within and across models while other leave LLM answers evenly split. Language-specific behavior seems to arise in all of the most consistent models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.",LГ©o Labat; Etienne Ollion; FranГ§ois Yvon,Léo Labat,Oral,In-person,Pavillon  DE RABAT,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Machine Translation,"Trustworthy, Safety, Privacy & Fairness",questions; laden; mcqs; llm answers; value; choice questions; multiple choice questions; multilingual llms; answer; multilingual
541-MAIN,ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links,"Understanding fine-grained links between documents is crucial for many applications, yet progress is limited by the lack of efficient methods for for data curation. To address this limitation, we introduce a domain-agnostic framework for bootstrapping sentence-level cross-document links from scratch. Our approach (1) generates and validates semi-synthetic datasets of linked documents, (2) uses these datasets to benchmark and shortlist the best-performing linking approaches, and (3) applies the shortlisted methods in large-scale human-in-the-loop annotation of natural text pairs. We apply the framework in two distinct domains вЂ“ peer review and news вЂ“ and show that combining retrieval models with LLMs achieves a 73% human approval rate for suggested links, more than doubling the acceptance of strong retrievers alone. Our framework allows users to produce novel datasets that enable systematic study of cross-document understanding, supporting downstream tasks such as media framing analysis and peer review assessment. All code, data, and annotation protocols are released to facilitate future research.",Serwar Basch; Ilia Kuznetsov; Tom Hope; Iryna Gurevych,Serwar Basch,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)","LLM Evaluation, Benchmarks & Metrics",links; cross document; bootstrapping; document; peer review; annotation; peer; review; cross; documents
547-MAIN,When Flores Bloomz Wrong: An Analysis of Cross-Lingual Contamination in Machine Translation Evaluation,"Large language models (LLMs) can be benchmark-contaminated, which produces inflated scores that mask memorization as generalization. In a multilingual setting, memorization has been shown to be able to transfer to ``uncontaminated'' languages. Using the FLORES-200 machine translation benchmark as a diagnostic, we study three 7--8B instruction-tuned multilingual LLMs. Using Llama and Qwen's BLEU and COMET scores as a control, we confirm Bloomz's FLORES contamination. We then demonstrate that machine translation contamination happens cross-lingually and is driven by target-side memorization, artificially boosting performance in translating unseen input. Further analysis shows that despite our source paraphrasing or perturbation efforts, recall of memorized references often persists. We discover that this is strongly anchored to source-side named entities; randomizing these sharply reduces recall of memorized texts. This provides insights into potential ways to cleanly benchmark contaminated LLMs.",David Tan; Pinzhen Chen; Josef van Genabith; Koel Dutta Chowdhury,David Tan,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Machine Translation,Multilinguality & Low-Resource NLP,flores; contamination; memorization; machine translation; memorized; translation; machine; recall; scores; benchmark
549-MAIN,Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue,"Meta-reviewing is a pivotal stage in the peer-review process, serving as the final step in determining whether a paper is recommended for acceptance. Prior research on meta-reviewing has treated this as a summarization problem over review reports. However, complementary to this perspective, meta-reviewing is a decision-making process that requires weighing reviewer arguments and placing them within a broader context. Prior research has demonstrated that decision-makers can be effectively assisted in such scenarios via dialogue agents. In line with this framing, we explore the practical challenges for realizing dialog agents that can effectively assist meta-reviewers. Concretely, we first address the issue of data scarcity for training dialogue agents by generating synthetic data using Large Language Models (LLMs) based on a self-refinement strategy to improve the relevance of these dialogues to expert domains. Our experiments demonstrate that this method produces higher-quality synthetic data and can serve as a valuable resource towards training meta-reviewing assistants. Subsequently, we utilize this data to train dialogue agents tailored for meta-reviewing and find that these agents outperform *off-the-shelf* LLM-based assistants for this task. Finally, we apply our agents in real-world meta-reviewing scenarios and confirm their effectiveness in enhancing the efficiency of meta-reviewing.",Sukannya Purkayastha; Nils Dycke; Anne Lauscher; Iryna Gurevych,Nils Dycke,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents","Dialogue, Conversational & Interactive NLP",reviewing; meta; agents; dialogue; decision; prior research; assistants; review; decision making; synthetic data
551-MAIN,HalluZig: Hallucination Detection using Zigzag Persistence,"The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and can enable early detection of hallucinations.",Shreyas N. Samaga; Gilberto Gonzalez Arroyo; Tamal K. Dey,Shreyas N. Samaga,Oral,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Reasoning, Planning & Agents",topological; detection; persistence; hallucination detection; hallucination; factual; attention; propensity; rely surface level; matrices
552-MAIN,Mapping the Course for Prompt-based Structured Prediction,"LLMs have been shown to be useful for a variety of language tasks, without requiring task-specific fine-tuning. However, these models often struggle with hallucinations and complex reasoning problems due to their autoregressive nature. We propose to address some of these issues, specifically in the area of structured prediction, by combining LLMs with combinatorial inference in an attempt to marry the predictive power of LLMs with the structural consistency provided by inference methods. We perform exhaustive experiments in an effort to understand which prompting strategies can effectively estimate LLM confidence values for use with symbolic inference, and show that, regardless of the prompting strategy, the addition of symbolic inference on top of prompting alone leads to more consistent and accurate predictions. Additionally, we show that calibration and fine-tuning using structured prediction objectives leads to increased performance for challenging tasks, showing that structured learning is still valuable in the era of LLMs.",Matt Pauk; Maria Leonor Pacheco,Matt Pauk,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Information Extraction & Structured Prediction,"LLM Evaluation, Benchmarks & Metrics",structured prediction; symbolic inference; structured; inference; prediction; prompting; symbolic; leads; llms; using structured
554-MAIN,Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models,"Large Language Models and Vision-Language Models have achieved impressive performance across a wide range of tasks, yet they remain vulnerable to carefully crafted perturbations. In this study, we seek to pinpoint the sources of this fragility by identifying parameters and input dimensions (pixels or token embeddings) that are susceptible to such perturbations. To this end, we propose a stability measure called FI, First order local Influence, which is rooted in information geometry and quantifies the sensitivity of individual parameter and input dimensions. Our extensive analysis across LLMs and VLMs (from 1.5B to 13B parameters) reveals that: (I) A small subset of parameters or input dimensions with high FI values disproportionately contribute to model brittleness. (II) Mitigating the influence of these vulnerable parameters during model merging leads to improved performance.",Runpeng Dai; Run Yang; Fan Zhou; Hongtu Zhu,Runpeng Dai,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,Interpretability & Model Analysis,parameters; dimensions; input; perturbations; vulnerable; influence; token embeddings; pixels; brittleness; small subset parameters
558-MAIN,Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding,"Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates deliberation once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency.",Huayu Li; ZhengXiao He; siyuan tian; Jinghao Wen; Ao Li,Zhengxiao He,Oral,In-person,SALLE  LA PALMERAIE,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",path; sampling; principled; stochastic process; theory; decoding; stochastic; stopping; paths; pruning
560-MAIN,Is This LLM Library Learning? Evaluation Must Account For Compute and Behaviour,"The in-context learning (ICL) coding, reasoning, and tool-using ability of LLMs has spurred interest in library learning (i.e., the creation and exploitation of reusable and composable functions, tools, or lemmas). Such systems often promise improved task performance and computational efficiency by caching reasoning (i.e., storing generated tools) - all without finetuning. However, we find strong reasons to be skeptical. Specifically, we identify a serious evaluation flaw present in a large number of ICL library learning works: these works do not correct for the difference in computational cost between baseline and library learning systems. Studying three separately published ICL library learning systems, we find that all of them fail to consistently outperform the simple baseline of prompting the model - improvements in task accuracy often vanish or reverse once computational cost is accounted for. Furthermore, we perform an in-depth examination of one such system, LEGO-Prover, which purports to learn reusable lemmas for mathematical reasoning. We find no evidence of the direct reuse of learned lemmas, and find evidence against the soft reuse of learned lemmas (i.e., reuse by modifying relevant examples). Our findings suggest that a serious re-examination of the effectiveness of ICL LLM-based library learning is required, as is much stronger standards for evaluation. An equal computational budget must be used for baselines, alongside behavioural analysis.",Ian Berlot-Attwell; Tobias Sesterhenn; Frank Rudzicz; Xujie Si,Ian Berlot-Attwell,Oral,In-person,SALLE  LE RIAD,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Efficiency, Scaling & NLP Systems","LLM Evaluation, Benchmarks & Metrics",library; lemmas; icl; learning; reuse; computational; reusable; computational cost; learned; works
561-MAIN,"Too Many Frames, Not All Useful: Efficient Strategies for Long-Form Video QA","Long-form videos that span across wide temporal intervals are highly information redundant and contain multiple distinct events or entities that are often loosely related. Therefore, when performing long-form video question answering (LVQA), all information necessary to generate a correct response can often be contained within a small subset of frames. Recent literature leverage large language models (LLMs) in LVQA benchmarks, achieving exceptional performance, while relying on vision language models (VLMs) to convert all visual content within videos into natural language. Such VLMs often independently caption a large number of frames uniformly sampled from long videos, which is not efficient and can mostly be redundant. Motivated by this inefficiency, we propose LVNet, a modular and training-free framework featuring a novel Hierarchical Keyframe Selector (HKS) that efficiently selects a minimal set of informative frames tailored to each question. LVNet's modularity allows easy integration with existing approaches for more efficient LVQA. We achieve state-of-the-art performance among similarly configured models across four benchmark LVQA datasets: EgoSchema, NExT-QA, IntentQA, VideoMME. Our code will be released publicly.",Jongwoo Park; Kanchana Ranasinghe; Kumara Kahatapitiya; Wonjeong Ryu; Donghyun Kim; Michael S Ryoo,Jongwoo Park,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",frames; videos; long form; long; form; redundant; video; efficient; vlms; information redundant
563-MAIN,A Unified View on Emotion Representation in Large Language Models,"Interest in leveraging Large Language Models (LLMs) for emotional support systems motivates the need to understand how these models comprehend and represent emotions internally. While recent works show the presence of emotion concepts in the hidden state representations, it's unclear if the model has a robust representation that is consistent across different datasets. In this paper, we present a unified view to understand emotion representation in LLMs, experimenting with diverse datasets and prompts. We then evaluate the reasoning ability of the models on a complex emotion identification task. We find that LLMs have a common emotion representation in the later layers of the model, and the vectors capturing the direction of emotions extracted from these representations can be interchanged among datasets with minimal impact on performance. Our analysis of reasoning with Chain of Thought (CoT) prompting shows the limits of emotion comprehension. Therefore, despite LLMs implicitly having emotion representations, they are not equally skilled at reasoning with them in complex scenarios. This motivates the need for further research to find new approaches.",Aishwarya Maheswaran; Maunendra Sankar Desarkar,Aishwarya Maheswaran,Oral,In-person,SALLE  LA PALMERAIE,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents",Interpretability & Model Analysis,emotion; representation; motivates; emotions; view; representations; unified; understand; datasets; llms
565-MAIN,TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models,"Reliable mathematical and scientific reasoning remains an open challenge for large visionвЂ“language models (VLMs). Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE (Transparent Reasoning And Consistency Evaluation), a framework for analyzing, diagnosing, and improving reasoning in VLMs. At its core, TRACE leverages Auxiliary Reasoning Sets (ARS), compact sub-questionвЂ“answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS is linked to final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement.",Shima Imani; Seungwhan Moon; Lambert Mathias; Lu Zhang; Babak Damavandi,Shima Imani,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",trace; reasoning; failures; ars; framework analyzing; consistency; final answer; answer; analyzing; steps
566-MAIN,ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs,"We introduce Argument Representation Coverage (ARC), a bottom-up evaluation framework that assesses how well summaries preserve structured salient arguments, a crucial issue in summarizing high-stakes domains such as law. ARC provides an interpretable lens by distinguishing between different information types to be covered and by separating omissions from factual errors. Using ARC, we evaluate summaries from eight open-weight LLMs in two domains where argument roles are central: long legal opinions and scientific articles. Our results show that while LLMs capture some salient roles, they frequently omit critical information, particularly when arguments are sparsely distributed across the input. Moreover, ARC uncovers systematic patternsвЂ”showing how context window positional bias and role-specific preferences shape argument coverageвЂ”providing actionable guidance for developing more complete and reliable summarization strategies.",Mohamed Elaraby; Diane Litman,Mohamed Elaraby,Oral,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),Interpretability & Model Analysis,argument; roles; salient; arguments; summaries; summarization; coverage; representation; long; sparsely
568-MAIN,AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation,"Current speech evaluation suffers from two critical limitations: the need and difficulty of designing specialized systems targeting individual audio characteristics, and poor correlation between automatic evaluation methods and human preferences. This work presents a systematic study of Large Audio Model (LAM) as a Judge, AudioJudge, investigating whether it can provide a unified evaluation framework that addresses both challenges. We systematically explore AudioJudge across audio characteristic detection tasks, including pronunciation, speaking rate, speaker identification and speech quality, and system-level human preference simulation for automated benchmarking. We investigate different prompt engineering strategies, finding that audio concatenation combined with in-context learning significantly improves performance across both audio characteristic detection and human preference simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to enable general-purpose multi-aspect audio evaluation. This method decomposes speech assessment into specialized judges for lexical content, speech quality, and paralinguistic features, achieving up to 0.91 Spearman correlation with human preferences on our system ranking benchmark. Robustness analysis reveals that while LAMs maintain strong performance under acoustic noise, they exhibit significant verbosity and positional biases that require careful mitigation.",Potsawee Manakul; Haosheng Gan; Michael J Ryan; Ali Sartaz Khan; Warit Sirichotedumrong; Kunat Pipatanakul; William Barr Held; Diyi Yang,Warit Sirichotedumrong,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",audio; speech; speech evaluation; multi aspect; characteristic; human preference; large audio; evaluation; human preferences; aspect
573-MAIN,x-SAL: Leading Symbolic Reasoning across Languages via Cross-lingual Symbolic-Aided Language Model,"Reasoning beyond English remains a significant challenge for large language models (LLMs); indeed, the ability to perform step-wise reasoning remains restricted to a single language, making it difficult to generalise across languages and hindering broader global adoption. Although a number of works have proposed techniques to align LLMs' multilingual capabilities, they operate mainly on mathematical tasks, leaving behind multilingual symbolic and commonsense reasoning tasks. We present Cross-lingual Symbolic-Aided Language Model (X-SAL), a method inspired by Program-Aided Language Models (PA L) for aligning symbolic reasoning across languages. X -SAL empower reasoning operating via Multilingual Symbolic Formaliser (converting multilingual questions and problem solutions into symbolic statements) and Language-specific Solver (delivering the final answers using symbolic statements) directly within the language model. Our experimental evaluations show that X -SAL outperforms existing methods both in mathematical and symbolic reasoning tasks. Yet, since there remains a performance gap between larger and smaller LLMs, we then propose self-refining techniques to enable even smaller LLMs to achieve robust performance",Leonardo Ranaldi; Giulia Pucci,Leonardo Ranaldi,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Multilinguality & Low-Resource NLP,"Reasoning, Planning & Agents",symbolic; aided; symbolic reasoning; reasoning; reasoning languages; multilingual; smaller llms; statements; language model; mathematical
574-MAIN,ToxiPrompt: A Two-Stage Red-Teaming Approach for Balancing Adversarial Prompt Diversity and Response Toxicity,"While large language models (LLMs) offer great promise, they also pose concrete safety risks. To audit and mitigate these risks, researchers have developed automated red-teaming methods, which generate adversarial prompts to elicit unsafe behavior of target LLMs during evaluation. Recent automated red-teaming methods for LLMs face a persistent trade-off: techniques that increase prompt diversity often reduce the level of the toxicity elicited from the target LLMs, while toxicity-maximizing methods tend to collapse diversity. To address the limitations, we propose ToxiPrompt, a two-stage framework that explicitly separates exploration (diversity) from exploitation (toxicity) and reunifies them with a single selection criterion to balance between diversity and toxicity. Experimental results show that ToxiPrompt outperforms four state-of-the-art baselines in both adversarial prompt diversity and the level of elicited toxicity from target LLMs, improving 14.6% harmonic mean of toxicity and diversity against the best baseline. The approach also performs well for multiple instruction-tuned target LLMs (Llama-2/3, Qwen, Mistral) without re-tuning, achieving up to 55% harmonic mean improvement against the best baseline. Our code is available at https://anonymous.4open.science/r/ToxiPrompt-DE92",Seungho Lee; Kyumin Lee,Seungho Lee,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",,toxicity; diversity; prompt diversity; teaming; red teaming; red; target; best baseline; elicited; adversarial
575-MAIN,AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages,"Text embeddings are an essential building component of several NLP tasks such as retrieval-augmented generation which is crucial for preventing hallucinations in LLMs. Despite the recent release of massively multilingual MTEB (MMTEB), African languages remain underrepresented, with existing tasks often repurposed from translation benchmarks such as FLORES clustering or SIB-200. In this paper, we introduce AfriMTEB---a regional expansion of MMTEB covering 59 languages, 14 tasks, and 38 datasets, including six newly added datasets. Unlike many MMTEB datasets that include fewer than five languages, the new additions span 14 to 56 African languages and introduce entirely new tasks, such as hate speech detection, intent detection, and emotion classification, which were not previously covered. Complementing this, we present AfriE5, an adaptation of the instruction-tuned mE5 model to African languages through cross-lingual contrastive distillation. Our evaluation shows that AfriE5 achieves state-of-the-art performance, outperforming strong baselines such as Gemini-Embeddings and mE5.",Kosei Uemura; Miaoran Zhang; David Ifeoluwa Adelani,Kosei Uemura,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multilinguality & Low-Resource NLP,Summarization & Generation,african languages; african; languages; tasks; embeddings; datasets; detection; additions; intent detection; covered
580-MAIN,SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context,"Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.",Aishwarya Verma; Laud Ammah; Olivia Nercy Ndlovu Lucas; Andrew Zaldivar; Vinodkumar Prabhakaran; Sunipa Dev,Aishwarya Verma,Oral,In-person,SALLE  WALILI,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multilinguality & Low-Resource NLP,"Trustworthy, Safety, Privacy & Fairness",stereotype; native languages; engaged; african; stereotypes; sub; native; community; coverage; resources
582-MAIN,Better Generalizing to Unseen Concepts: An Evaluation Framework and An LLM-Based Auto-Labeled Pipeline for Biomedical Concept Recognition,"Generalization to unseen concepts is a central challenge due to the scarcity of human annotations in Mention-agnostic Biomedical Concept Recognition (MA-BCR). This work makes two key contributions to systematically address this issue. First, we propose an evaluation framework built on hierarchical concept indices and novel metrics to measure generalization. Second, we explore LLM-based Auto-Labeled Data (ALD) as a scalable resource, creating a task-specific pipeline for its generation. Our research unequivocally shows that while LLM-generated ALD cannot fully substitute for manual annotations, it is a valuable resource for improving generalization, successfully providing models with the broader coverage and structural knowledge needed to approach recognizing unseen concepts. Code and datasets are available at https://github.com/bio-ie-tool/hi-ald.",Shanshan liu; Noriki Nishida; Fei Cheng; Narumi Tokunaga; Rumana Ferdous Munne; Yuki Yamagata; Kouji Kozaki; Takehito Utsuro; Yuji Matsumoto,Shanshan Liu,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Domain NLP (Biomedical/Clinical/Legal/Scientific),"LLM Evaluation, Benchmarks & Metrics",unseen; concepts; concept; generalization; auto; biomedical; labeled; evaluation framework; recognition; annotations
584-MAIN,A Representation Sharpening Framework for Zero Shot Dense Retrieval,"Zero-shot dense retrieval is a challenging setting where a document corpus is provided without relevant queries, necessitating a reliance on pretrained dense retrievers (DRs). However, since these DRs are not trained on the target corpus, they struggle to represent semantic differences between similar documents. To address this failing, we introduce a training-free representation sharpening framework that augments a document's representation with information that helps differentiate it from similar documents in the corpus. On over twenty datasets spanning multiple languages, the representation sharpening framework proves consistently superior to traditional retrieval, setting a new state-of-the-art on the BRIGHT benchmark. We show that representation sharpening is compatible with prior approaches to zero-shot dense retrieval and consistently improves their performance. Finally, we address the performance-cost tradeoff presented by our framework and devise an indexing-time approximation that preserves the majority of our performance gains over traditional retrieval, yet suffers no additional inference-time cost.",Dhananjay Ashok; Suraj Nair; Mutasem Al-Darabsah; Choon Hui Teo; Tarun Agarwal; Jonathan May,Dhananjay Ashok,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)",Interpretability & Model Analysis,sharpening; representation; dense; dense retrieval; retrieval; corpus; zero shot; zero; shot; similar
587-MAIN,Spotlight Your Instructions: Instruction-following with Dynamic Attention Steering,"In many real-world applications, users rely on natural language instructions to guide large language models (LLMs) across a wide range of tasks. These instructions are often complex, diverse, and subject to frequent change. However, LLMs do not always attend to these instructions reliably, and users lack simple mechanisms to emphasize their importance beyond modifying prompt wording or structure. To address this, we present an inference-time method that enables users to emphasize specific parts of their prompt by steering the model's attention toward them, aligning the model's perceived importance of different prompt tokens with user intent. Unlike prior approaches that are limited to static instructions, require significant offline profiling, or rely on fixed biases, we dynamically update the proportion of model attention given to the user-specified parts--ensuring improved instruction following without performance degradation. We demonstrate that our approach improves instruction following across a variety of tasks involving multiple instructions and generalizes across models of varying scales.",Praveen Venkateswaran; Danish Contractor,Danish Contractor,Oral,In-person,Pavillon  DE RABAT,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Interpretability & Model Analysis,"Dialogue, Conversational & Interactive NLP",instructions; instruction following; following; users; emphasize; parts; attention; instruction; prompt; steering
588-MAIN,STREAM-ZH: Simplified Topic Retrieval Exploration and Analysis Module for Chinese Language,"We introduce Simplified Topic Retrieval Exploration and Analysis Module for Chinese language (STREAM-ZH), the first topic modeling package to fully support the Chinese language across a broad range of topic models, evaluation metrics, and preprocessing workflows. Tailored to both simplified and traditional Chinese language, our package extends the STREAM topic modeling framework with a curated collection of preprocessed textual datasets in Chinese from which we assess the performance of classical, neural, and clustering topic models using commonly-used intruder, diversity, and coherence metrics. The results of a benchmark analysis bring evidence that within our framework, topic models may generate coherent and diverse topics from datasets in Chinese language, outperforming those generated by topic models using English-translated textual input. Our framework facilitates multilingual accessibility and research in topic modeling applied to Chinese textual data. The code is available at the following link: https://anonymous.4open.science/r/STREAM-ZH-DC5B",Hongyi Li; Jianjun Lian; Anton Frederik Thielmann; Andre Python,Jianjun Lian,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)","LLM Evaluation, Benchmarks & Metrics",topic; chinese; stream; topic models; simplified; package; textual; modeling; models using; exploration
589-MAIN,FormGym: Doing Paperwork with Agents,"End-to-end form filling refers to automatically populating fields in a document-style form with the appropriate information derived from external data. Although prevalent and useful, no formal benchmark exists for evaluating systems' form completion accuracy. Existing datasets focus on parsing, extraction and web form interaction, rather than end-to-end completion of document-style forms. We propose FormGym, a benchmark formulation of the end-to-end form filling task that evaluates form completion and accuracy. We construct FormGym by repurposing three existing datasets and add one new dataset to achieve more challenging, diverse, and realistic test cases. Our studies show baseline vision language agents (VLAs) perform poorly on FormGym in every scenario, primarily due to poor field localization. GUI agents perform better but suffer from high latency and costs. Therefore we also introduce FieldFinder, a field localization tool that enables zero-shot VLAs to find and accurately place text in input fields. We find that VLAs augmented with FieldFinder achieve better performance compared to baselines in all models.",Matthew Toles; Isaac Song; RATTANDEEP SINGH; Zhou Yu,Matthew Toles,Oral,In-person,Pavillon  DE RABAT,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Efficiency, Scaling & NLP Systems","Linguistics, Syntax & Semantics",form; end; completion; filling; end end; localization; existing datasets; fields; agents; field
590-MAIN,NarraBench: A Comprehensive Framework for Narrative Benchmarking,"We present NarraBench, a theory-informed taxonomy of narrative-understanding tasks, as well as an associated survey of 78 existing benchmarks in the area. We find significant need for new evaluations covering aspects of narrative understanding that are either overlooked in current work or are poorly aligned with existing metrics. Specifically, we estimate that only 27% of narrative tasks are well captured by existing benchmarks, and we note that some areas -- including narrative events, style, perspective, and revelation -- are nearly absent from current evaluations. We also note the need for increased development of benchmarks capable of assessing constitutively subjective and perspectival aspects of narrative, that is, aspects for which there is generally no single correct answer. Our taxonomy, survey, and methodology are of value to NLP researchers seeking to test LLM narrative understanding.",Sil Hamilton; Matthew Wilkens; Andrew Piper,Sil Hamilton,Oral,In-person,SALLE  LE LIXUS,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics",,narrative; aspects; note; taxonomy; existing benchmarks; survey; understanding; benchmarks; evaluations; existing
593-MAIN,From Plausible to Faithful: Optimizing the Faithfulness of LLM Explanations,"Large language models (LLMs) increasingly produce natural language explanations, yet these explanations often lack faithfulness, and they do not reliably reflect the evidence the model uses to decide. We introduce FaithLM, a model-agnostic framework that evaluates and improves the faithfulness of LLM explanations without token masking or task-specific heuristics. FaithLM formalizes explanation faithfulness as an intervention property: a faithful explanation should yield a prediction shift when its content is contradicted. Theoretical analysis shows that the resulting contrary-hint score is a sound and discriminative estimator of faithfulness. Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score. Experiments on three multi-domain datasets and multiple LLM backbones demonstrate that FaithLM consistently increases faithfulness and produces explanations more aligned with human rationales than strong self-explanation baselines. These findings highlight that intervention-based evaluation, coupled with iterative optimization, provides a principled route toward faithful and reliable LLM explanations.",Yu-Neng Chuang; Guanchu Wang; Chia-Yuan Chang; Ruixiang Tang; Shaochen Zhong; Fan Yang; Andrew Wen; Mengnan Du; Xuanting Cai; Vladimir Braverman; Xia Hu,Yu-Neng Chuang,Oral,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"LLM Evaluation, Benchmarks & Metrics",faithfulness; explanations; explanation; faithful; intervention; llm; score; estimator; formalizes; hint
594-MAIN,MiSCHiEF: A Benchmark in Minimal-Pairs of Safety and Culture for Holistic Evaluation of Fine-Grained Image-Caption Alignment,"Fine-grained image-caption alignment is crucial for vision-language models (VLMs), especially in socially critical contexts such as identifying real-world risk scenarios or distinguishing cultural proxies, where correct interpretation hinges on subtle visual or linguistic clues and where minor misinterpretations can lead to significant real-world consequences. We present MiSCHiEF, a set of two benchmarking datasets (MiC and MiS) based on a contrastive pair design in the domains of safety and culture, and evaluate four VLMs on tasks requiring fine-grained differentiation of paired images and captions. In both datasets, each sample contains two minimally differing captions and corresponding minimally differing images. In MiS, the image-caption pairs depict a safe and an unsafe scenario, while in MiC, they depict cultural proxies in two distinct cultural contexts. We find that models generally perform better at confirming the correct image-caption pair than rejecting incorrect ones. Additionally, models achieve higher accuracy when selecting the correct caption from two highly similar captions for a given image, compared to the converse task. The results, overall, highlight persistent modality misalignment challenges in current VLMs, underscoring the difficulty of precise cross-modal grounding required for applications with subtle semantic and visual distinctions.",Sagarika Banerjee; Tangatar Madi; Advait Swaminathan; Nguyen Dao Minh Anh; Shivank Garg; Kevin Zhu; Vasu Sharma,Kevin Zhu,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Multimodal & Speech/Audio,caption; image; captions; differing; correct; vlms; minimally; cultural; mis; proxies
595-MAIN,Is Information Density Uniform when Utterances are Grounded on Perception and Discourse?,"The Uniform Information Density (UID) hypothesis posits that speakers tend to distribute information evenly across words within utterances, minimising surprisal variance. While this hypothesis was verified empirically, prior studies are limited exclusively to textвЂ“only inputs, abstracting away from the perceptual context. In this work, we present the first computational study of UID in grounded settings, estimating surprisal using multilingual visionвЂ“andвЂ“language models over imageвЂ“caption data in 30 languages and visual storytelling data in 14 languages, together spanning 11 families. We find that grounding on perception consistently smooths the distribution of information, lowering both global and local UID metrics across typologically diverse languages compared to text-only settings. In visual narratives, the combination of image and discourse contexts yields additive effects, with the strongest surprisal reductions at unit onsets. Grounded language exhibits greater information uniformity, supporting a context-sensitive formulation of UID. Hence, we take a first step towards modelling the temporal dynamics of information flow in ecologically plausible, multimodal language use.",Matteo Gay; Coleman Haley; Mario Giulianelli; Edoardo Ponti,Matteo Gay,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,"Linguistics, Syntax & Semantics",surprisal; information; density; information density; uniform; grounded; utterances; hypothesis; perception; discourse
599-MAIN,KAD: A Framework for Proxy-based Test-time Alignment with Knapsack Approximation Deferral,"With the accelerated scaling in large language models' (LLMs) sizes, comes the prohibitive surge in computation budget allotted for model's alignment and fine-tuning for downstream tasks. And while factual knowledge has been shown to be learned earlier during pre-training, a model still requires further fine-tuning to adhere to task requirements and stylistic preferences. We propose a novel approach to circumvent costly large model fine-tuning via test-time alignment, i.e. using guidance from a small aligned model. Our approach is based on a new formulation of token-specific cascading as a 0-1 knapsack problem. In this setting, we derive primal and dual approximations of the optimal deferral decision. We further demonstrate the advantages of our method experimentally both in task performance and speculative decoding speed.",Ayoub Hammal; Pierre Zweigenbaum; Caio Corro,Ayoub Hammal,Oral,In-person,Pavillon  DE RABAT,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness",,knapsack; time alignment; test time; alignment; fine tuning; tuning; fine; test; speculative decoding speed; pre training model
600-MAIN,When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable LLM Evaluation,"Evaluating Large Language Models (LLMs) for mental health support poses unique challenges to reliable evaluation due to the emotionally and cognitively complex nature of therapeutic dialogue. Existing benchmarks are limited in scale, authenticity, and reliability, often relying on synthetic or social media data, and lack frameworks to assess when automated judges can be trusted. To address the need for large-scale authentic dialogue datasets and judge-reliability assessment, we introduce two benchmarks that provide a framework for generation and evaluation in this domain. MentalBench-100k consolidates 10,000 authentic single-session therapeutic conversations from three real-world scenarios datasets, each paired with nine LLM-generated responses, yielding 100,000 response pairs. MentalAlign-70k reframes evaluation by comparing four high-performing LLM judges with human experts across 70,000 ratings on seven attributes, grouped into Cognitive Support Score (CSS) and Affective Resonance Score (ARS). We then employ the AffectiveвЂ“Cognitive Agreement Framework, a statistical methodology using intraclass correlation coefficients (ICC) with confidence intervals to quantify agreement, consistency, and bias between LLM judges and human experts. Our analysis reveals systematic inflation by LLM judges, strong reliability for cognitive attributes such as guidance and informativeness, reduced precision for empathy, and some unreliability in safety and relevance. Our contributions establish new methodological and empirical foundations for the reliable and large-scale evaluation of LLMs in mental health contexts.",Abeer Badawi; Elahe Rahimi; Md Tahmid Rahman Laskar; Sheri Grach; Lindsay Bertrand; Lames Danok; Prathiba Dhanesh; Jimmy Huang; Frank Rudzicz; Elham Dolatabadi,Abeer Badawi,Oral,In-person,SALLE  WALILI,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",judges; llm judges; mental health; mental; health; cognitive; therapeutic; scale; reliability; human experts
602-MAIN,DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures,"We propose a novel self-attention mechanism for document understanding that takes into account text block positions in relative polar coordinate system rather than the Cartesian one. Based on this mechanism, we build DocPolarBERT, a layout-aware BERT model for document understanding that eliminates the need for absolute 2D positional embeddings. Despite being pre-trained on a dataset more than six times smaller than the widely used IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results demonstrate that a carefully designed attention mechanism can compensate for reduced pre-training data, offering an efficient and effective alternative for document understanding.",Benno Uthayasooriyar; Antoine LY; Franck Vermet; Caio Corro,Benno Uthayasooriyar,Oral,In-person,SALLE  LE LIXUS,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Interpretability & Model Analysis,,document understanding; document; polar; model document; coordinate; understanding; mechanism; layout; pre; pre trained
604-MAIN,IDEAlign: Comparing Ideas of Large Language Models to Domain Experts,"Large language models (LLMs) are increasingly used to produce open-ended, interpretive annotations, yet there is no validated, scalable measure of ***idea-level similarity*** to expert annotations. We (i) introduce the content evaluation of LLM annotations as a core, understudied task, (ii) propose IDEAlign for capturing expert similarity judgments via the odd-one-out tasks, and (iii) benchmark various similarity methods, such as text embeddings, topic models, and LLM-as-a-judge, against these human ratings. Applying this approach to two real-world educational datasets (interpreting math reasoning and feedback generation), we find that most metrics fail to capture the nuanced dimensions of similarity meaningful to experts. LLM-as-a-judge performs best (11вЂ“18% improvement over other methods) but still falls short of expert alignment, making it useful as a triage filter rather than a substitute for human review. Our work demonstrates the difficulty of evaluating open-ended LLM annotations at scale, and positions IDEAlign as a reusable protocol for benchmarking on this task, thereby informing responsible deployment of LLMs.",HyunJi Nam; LucГ­a Langlois; Jim Malamut; Mei Tan; Dorottya Demszky,Hyunji Nam,Oral,In-person,SALLE  LA PALMERAIE,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",Summarization & Generation,similarity; annotations; expert; open ended; ended; llm judge; experts; judge; llm; fail capture nuanced
607-MAIN,Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning,"Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.",Yue Zhou; Xiaobo Guo; Belhassen Bayar; Srinivasan H. Sengamedu,Xiaobo Guo,Oral,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)","Dialogue, Conversational & Interactive NLP",memory; coherence; time; driven; reasoning; long term; narrative; term; computationally; agentic
609-MAIN,It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models,"Despite the recent advancements in NLP with the advent of Large Language Models (LLMs), Entity Linking (EL) for historical texts remains challenging due to linguistic variation, noisy inputs, and evolving semantic conventions. Existing solutions either require substantial training data or rely on domain-specific rules that limit scalability. In this paper, we present MHEL-LLaMo (Multilingual Historical Entity Linking with Large Language MOdels), an unsupervised ensemble approach combining a Small Language Model (SLM) and an LLM. MHEL-LLaMo leverages a multilingual bi-encoder (BELA) for candidate retrieval and an instruction-tuned LLM for NIL prediction and candidate selection via prompt chaining. Our system uses SLM's confidence scores to discriminate between easy and hard samples, applying an LLM only for hard cases. This strategy reduces computational costs while preventing hallucinations on straightforward cases. We evaluate MHEL-LLaMo on four established benchmarks in six European languages (English, Finnish, French, German, Italian and Swedish) from the 19th and 20th centuries. Results demonstrate that MHEL-LLaMo outperforms state-of-the-art models without requiring fine-tuning, offering a scalable solution for low-resource historical EL. Our error analysis reveals that 41% of false predictions exhibit semantic proximity to ground truth entities, highlighting the LLM's accurate disambiguation of historical references.",Cristian Santini; Marieke van Erp; Mehwish Alam,Cristian Santini,Oral,In-person,SALLE  LA PALMERAIE,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Multilinguality & Low-Resource NLP,Information Extraction & Structured Prediction,historical; entity linking; linking; slm; entity; unsupervised; hard; multilingual; candidate; confidence
612-MAIN,SoS: Analysis of Surface over Semantics in Multilingual Text-To-Image Generation,"Text-to-image (T2I) models are increasingly employed by users worldwide. However, prior research has pointed to the high sensitivity of T2I towards particular input languages - when faced with languages other than English (i.e., different surface forms of the same prompt), T2I models often produce culturally stereotypical depictions, prioritizing the surface over the prompt's semantics. Yet a comprehensive analysis of this behavior, which we dub Surface-over-Semantics (SoS), is missing. We present the first analysis of T2I models' SoS tendencies. To this end, we create a set of prompts covering 171 cultural identities, translated into 14 languages, and use it to prompt seven T2I models. To quantify SoS tendencies across models, languages, and cultures, we introduce a novel evaluation measure and analyze how the tendencies we identify manifest visually. We show that all tested models exhibit strong surface tendencies in at least three languages, and that this effect intensifies throughout the layers of T2Is' text encoders. Furthermore, strong surface tendencies often directly relate to stereotypical depictions and are reflected in distinct color profiles.",Carolin Holtermann; Florian Schneider; Anne Lauscher,Carolin Holtermann,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Linguistics, Syntax & Semantics",Summarization & Generation,tendencies; t2i; surface; t2i models; languages; semantics; stereotypical; text image; prompt; analysis
621-MAIN,Gender and Politeness Perception: A Novel Approach for Exploring Annotations Disagreement,"Politeness is an important social phenomenon which influences the flow of conversations. Several studies proposed models to discover and analyze linguistic cues associated with (im)polite language. However, no prior work computationally studied how politeness perception interacts with other social dimensions such as gender. We propose a model for automatic discovery of linguistic patterns which correlate with disagreement in politeness annotations, specifically focusing on gender differences. The model discovers fine-grained context patterns of words which correlate with disagreement in politeness annotations between men and women annotators. We apply the proposed model on emails annotated for politeness. Results show women rate emails which contain formal cues (e.g. To whom it may concern) more polite than men annotators rate them, while men rate emails exhibiting informal language cues (e.g. haven't seen my new swing) more polite than women annotators rate them. Our findings highlight the importance of studying politeness through multiple demographic perspectives.",Ahmad Aljanaideh,Ahmad Aljanaideh,Oral,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics",,politeness; emails; women; men; rate; disagreement; annotators; gender; cues; correlate
622-MAIN,TempViz: On the Evaluation of Temporal Knowledge in Text-to-Image Models,"Time alters the visual appearance of entities in our world, like objects, places, and animals. Thus, for accurately generating contextually-relevant images, knowledge and reasoning about time can be crucial (e.g., for generating a landscape in spring vs. in winter). Yet, although substantial work exists on understanding and improving temporal knowledge in natural language processing, research on how temporal phenomena appear and are handled in text-to-image (T2I) models remains scarce. We address this gap with TempViz, the first data set to holistically evaluate temporal knowledge in image generation, consisting of 7.9k prompts and more than 600 reference images. Using TempViz, we study the capabilities of five T2I models across five temporal knowledge categories. Human evaluation shows that temporal competence is generally weak, with no model exceeding 75% accuracy across categories. Towards larger-scale studies, we also examine automated evaluation methods, comparing several established approaches against human judgments. However, none of these approaches provides a reliable assessment of temporal cues - further indicating the pressing need for future research on temporal knowledge in T2I.",Carolin Holtermann; Nina Krebs; Anne Lauscher,Carolin Holtermann,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,temporal; temporal knowledge; t2i; knowledge; t2i models; image; text image; categories; images; generating
624-MAIN,ToxiGAN: Toxic Data Augmentation via LLM-Guided Directional Adversarial Generation,"Augmenting toxic language data in a controllable and class-specific manner is crucial for improving robustness in toxicity classification, yet remains challenging due to limited supervision and distributional skew. We propose ToxiGAN, a class-aware text augmentation framework that combines adversarial generation with semantic guidance from large language models (LLMs). To address common issues in GAN-based augmentation such as mode collapse and semantic drift, ToxiGAN introduces a two-step directional training strategy and leverages LLM-generated neutral texts as semantic ballast. Unlike prior work that treats LLMs as static generators, our approach dynamically selects neutral exemplars to provide balanced guidance. Toxic samples are explicitly optimized to diverge from these exemplars, reinforcing class-specific contrastive signals. Experiments on four hate speech benchmarks show that ToxiGAN achieves the strongest average performance in both macro-F1 and hate-F1, consistently outperforming traditional and LLM-based augmentation methods. Ablation and sensitivity analyses further confirm the benefits of semantic ballast and directional training in enhancing classifier robustness.",Peiran Li; Jan Fillies; Adrian Paschke,Peiran Li,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness",Summarization & Generation,directional; augmentation; toxic; class; based augmentation; exemplars; semantic; neutral; hate; guidance
625-MAIN,Text Classification Under Class Distribution Shift: A Survey,"The basic underlying assumption of machine learning (ML) models is that the training and test data are sampled from the same distribution. However, in daily practice, this assumption is often broken, i.e. the distribution of the test data changes over time, which hinders the application of conventional ML models. One domain where the distribution shift naturally occurs is text classification, since people always find new topics to discuss. To this end, we survey research articles studying open-set text classification and related tasks. We divide the methods in this area based on the constraints that define the kind of distribution shift and the corresponding problem formulation, i.e.~learning with the Universum, zero-shot learning, and open-set learning. We next discuss the predominant mitigation approaches for each problem setup. We further identify several future work directions, aiming to push the boundaries beyond the state of the art. Finally, we explain how continual learning can solve many of the issues caused by the shifting class distribution.",Adriana Valentina Costache; Silviu-Florin Gheorghe; Eduard Poesina; Paul Irofti; Radu Tudor Ionescu,Radu Tudor Ionescu,Oral,In-person,SALLE  LE LIXUS,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Summarization & Generation,,distribution; distribution shift; text classification; shift; learning; open set; test data; discuss; classification; assumption
631-MAIN,Reasoning's Razor: Reasoning Improves Accuracy but Hurts Recall at Critical Operating Points in Safety and Hallucination Detection,"Reasoning has become a central paradigm for large language models (LLMs), consistently boosting accuracy across diverse benchmarks. Yet its suitability for precision-sensitive use remains unclear. We present the first systematic study of reasoning for classification tasks under strict low false positive rate (FPR) regimes. Our analysis covers two tasksвЂ”safety detection and hallucination detectionвЂ”evaluated in both fine-tuned and zero-shot settings, using standard LLMs and Large Reasoning Models (LRMs). Our results reveal a clear trade-off: Think On (reasoning-augmented) generation improves overall accuracy, but performs poorly at the low-FPR thresholds essential for practical use. In contrast, Think Off (no reasoning during inference) dominates in these precision-sensitive regimes, with Think On surpassing only when higher FPRs are acceptable. In addition, we find token-based scoring substantially outperforms self-verbalized confidence for precision-sensitive deployments. Finally, a simple ensemble of the two modes recovers the strengths of each. Taken together, our findings position reasoning as a double-edged tool: beneficial for average accuracy, but often ill-suited for applications requiring strict precision.",Atoosa Chegini; Hamid Kazemi; Garrett Souza; Maria Safi; Yang Song; Samy Bengio; Sinead Williamson; Mehrdad Farajtabar,Seyedhamidreza Kazemitabaiezavare,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Trustworthy, Safety, Privacy & Fairness",precision; reasoning; think; regimes; sensitive; accuracy; strict; hallucination; safety; detection
632-MAIN,Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering,"Process reward models (PRMs) improve complex reasoning in large language models (LLMs) by grading candidate solutions step-by-step and selecting answers via aggregated step scores. While effective in domains such as mathematics, their applicability to tasks involving semi-structured data, like table question answering (TQA) remains unexplored. TQA poses unique challenges for PRMs, including abundant irrelevant information, loosely connected reasoning steps, and domain-specific reasoning. This work presents the first systematic study of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from both answer and step perspectives. Results show that PRMs that combine textual and code verification can aid solution selection but struggle to generalize to out-of-domain data. Analysis reveals a weak correlation between performance in step-level verification and answer accuracy, possibly stemming from weak step dependencies and loose causal links. Our findings highlight limitations of current PRMs on TQA and offer valuable insights for building more robust, process-aware verifiers.",Lei Tang; Wei Zhou; Mohsen Mesgar,Wei Zhou,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents",Interpretability & Model Analysis,prms; step; table question answering; process reward; table question; semi structured; semi; structured data; weak; table
634-MAIN,"Out of Distribution, Out of Luck: Process Rewards Misguide Reasoning Models","Process Reward Models (PRMs) have emerged as a promising approach for guiding large language models (LLMs) through multi-step reasoning by providing step-level feedback during inference. However, our evaluation across seven LLMs reveals a surprising failure mode: while PRMs significantly improve performance for instruct mathematical models, they consistently fail to enhance, and sometimes degrade, reasoning model performance despite requiring additional computational resources. Through systematic analysis using linear probes, we identify distinct reward prediction patterns that differentiate reasoning from non-reasoning model outputs. To understand this mechanism, we train Sparse Autoencoders on all layers of the Qwen2.5-Math-PRM and analyze reasoning features. Our analysis reveals that 80% of such features respond to superficial formatting artifacts, rather than mathematical content quality. This demonstrates that reasoning model outputs represent out-of-distribution inputs for current PRMs. Our findings expose a fundamental limitation in applying existing reward models to reasoning systems and provide mechanistic insights into this failure mode. We release our trained SAEs to facilitate future research into reward model interpretability and evaluation methods suited for reasoning paradigms.",Alexey Dontsov; Anton Korznikov; Andrey V. Galichin; Elena Tutubalina,Alexei Dontsov,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Interpretability & Model Analysis,"Reasoning, Planning & Agents",reasoning; prms; reasoning model; reward; failure mode; mode; reward models; model outputs; failure; mathematical
638-MAIN,Learning to Ideate for Machine Learning Engineering Agents,"Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a prompting-based setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its prompted counterpart and surpasses Claude Sonnet 3.5. These results highlight that enabling LLM agents to learn to ideate offers a promising path toward strategic AI systems for scientific discovery.",Yunxiang Zhang; Kang Zhou; Zhichao Xu; Kiran Ramnath; Yun Zhou; Sangmin Woo; Haibo Ding; Lin Lee Cheong,Yunxiang Zhang,Oral,In-person,SALLE  LE LIXUS,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Reasoning, Planning & Agents",Domain NLP (Biomedical/Clinical/Legal/Scientific),implementation; learning; strategic; agent; agents; machine learning; engineering; machine; iteratively optimize; based setup
640-MAIN,Instructional Agents: Reducing Teaching Faculty Workload through Multi-Agent Instructional Design,"Preparing high-quality instructional materials remains a labor-intensive process that often requires extensive coordination among teaching faculty, instructional designers, and teaching assistants. In this work, we present Instructional Agents, a multi-agent large language model (LLM) framework designed to automate end-to-end course material generation, including syllabus creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing AI-assisted educational tools that focus on isolated tasks, Instructional Agents simulates role-based collaboration among educational agents to produce cohesive and pedagogically aligned content. The system operates in four modes: Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling flexible control over the degree of human involvement. We evaluate Instructional Agents across five university-level computer science courses and show that it produces high-quality instructional materials while significantly reducing development time and human workload. By supporting institutions with limited instructional design capacity, Instructional Agents provides a scalable and cost-effective framework to democratize access to high-quality education, particularly in underserved or resource-constrained settings.",Huaiyuan Yao; Wanpeng Xu; Justin Turnau; Nadia Kellam; Hua Wei,Wanpeng Xu,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Domain NLP (Biomedical/Clinical/Legal/Scientific),instructional; agents; teaching; workload; materials; high quality; educational; multi agent; quality; reducing
641-MAIN,Rethinking Reading Order: Toward Generalizable Document Understanding with LLM-based Relation Modeling,"Document understanding requires modeling both structural and semantic relationships between the layout elements within the document, with human-perceived reading order (RO) playing a crucial yet often neglected role compared to heuristic OCR sequences used by most existing models. Previous approaches depend on costly, inconsistent human annotations, limiting scalability and generalization. To bridge the gap, we propose a cost-effective paradigm that leverages large language models (LLMs) to infer global RO and inter-element layout relations without human supervision. By explicitly incorporating RO as structural guidance, our method captures hierarchical, document-level dependencies beyond local adjacency. Experiments on Semantic Entity Recognition, Entity Linking, and Document Question Answering show consistent improvements over baseline methods. Notably, LLM-inferred RO, even when differing from ground-truth adjacency, provides richer global structural priors and yields superior downstream performance. These results and findings demonstrate the scalability and significance of RO-aware modeling, advancing both LLMs and lightweight layout-aware models for robust document understanding.",Weishi Wang; Hengchang Hu; Daniel Dahlmeier,Weishi Wang,Oral,In-person,SALLE  WALILI,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)",Information Extraction & Structured Prediction,document; layout; document understanding; structural; modeling; reading; scalability; order; global; understanding
644-MAIN,Validating Automatic Evaluation of Controllable Counterspeech Generation: Rankings Matter More Than Scores,"Counterspeech generation has emerged as a promising approach to combat online hate speech, with recent work focusing on controlling attributes used in counterspeech, such as strategies or intents. While these attributes are often evaluated automatically using classifiers, a key goal of this evaluation is to compare the performance of different generation models. However, the validity of such evaluation results is questionable when the classifiers themselves have only modest performance. This paper examines the automatic evaluation of counterspeech attributes using a multi-attribute counterspeech dataset containing 2,728 samples. We investigate when automatic evaluation can be trusted for model comparison and address the limitations of current evaluation methodologies. We make concrete recommendations for how to perform classifier validation before model evaluation. Our classifier validation results demonstrate that even limited classifiers can produce trustworthy model rankings. Therefore, we argue that when comparing counterspeech generation models, a classifier's ability to rank generation models is a more direct measure of its practical utility than traditional classification metrics, e.g., accuracy and F1.",Yi Zheng; BjГ¶rn Ross; Walid Magdy,Yi Zheng,Oral,In-person,SALLE  LE LIXUS,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics",Summarization & Generation,generation models; automatic evaluation; evaluation; classifiers; classifier; attributes; generation; rankings; automatic; validation
648-MAIN,Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking,"Reasoning-augmented visionвЂ“language models (VLMs) aim to improve performance by generating explicit reasoning chains. Yet recent work shows that these chains can exacerbate hallucinations rather than reduce them. Beyond producing incorrect answers, such models risk generating reasoning traces that are not visually faithful, misleading users about what the model вЂњsaw.вЂќ We demonstrate that reasoning-chain faithfulness is a distinct problem: accuracy on standard perception or hallucination metrics does not predict whether intermediate steps are grounded in the image. To address this gap, we introduce a framework for evaluating reasoning-chain faithfulness using off-the-shelf VLM judges, validated through a human correlation study. We further propose a simple self-reflection strategy that improves visual grounding without degrading task accuracy. Our findings establish visual faithfulness of reasoning chains as a novel evaluation dimension for VLMs and provide initial tools for its measurement and mitigation.",Rheeya Uppaal; Phu Mon Htut; Min Bai; Nikolaos Pappas; Zheng Qi; Sandesh Swamy,Rheeya Uppaal,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Reasoning, Planning & Agents",faithfulness; chains; reasoning; reasoning chain; reasoning chains; visual; vlms; chain; generating; models risk
649-MAIN,Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools,"Android is the largest mobile platform, yet automatically building applications remains a practical challenge. While Large Language Models (LLMs) show promise for code repair, their use for fixing Android build errors remains underexplored. To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects. Each problem is paired with a verified solution from a subsequent commit, ensuring that fixes are feasible. Second, we propose GradleFixer, an LLM agent with domain-specific tools for inspecting and manipulating the Gradle build environment. GradleFixer achieves a resolve rate of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent that relies on a general-purpose shell. GradleFixer's success suggests that while LLMs possess the high-level knowledge to solve these failures, they struggle to translate this knowledge into effective low-level actions using a general-purpose shell. We demonstrate the effectiveness of a strategy we term *Tool Bridging*, which replaces general-purpose shell commands with domain-aware abstractions. We hypothesize this approach works through two mechanisms: 1) it provides tools in an API-like format that LLMs use more reliably, and 2) it constrains the action space to relevant operations. This approach bridges the gap between the model's high-level reasoning and effective low-level execution.",Ha Min Son; Huan Ren; Xin Liu; Zhe Zhao,Ha Min Son,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",build; general purpose; low level; commit; specific tools; purpose; repair; tools; level; general
650-MAIN,MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments,"Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research. MetaLead dataset and code repository: https://anonymous.4open.science/r/metalead-7CA3",Roelien C. Timmer; Necva BГ¶lГјcГј; Stephen Wan,Roelien C. Timmer,Oral,In-person,SALLE  LE LIXUS,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",Summarization & Generation,leaderboard; proposed method; metadata; transparent; machine learning; dataset; result; type; machine; proposed
654-MAIN,Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations,"Generative medical vision-language models (Med-VLMs) are primarily designed to generate complex textual information (e.g., diagnostic reports) from multimodal inputs including vision modality (e.g., medical images) and language modality (e.g., clinical queries). However, their security vulnerabilities remain underexplored. Med-VLMs should be capable of rejecting harmful queries, such as Provide detailed instructions for using this CT scan for insurance fraud. At the same time, addressing security concerns introduces the risk of over-defense, where safety-enhancing mechanisms may degrade general performance, causing Med-VLMs to reject benign clinical queries. In this paper, we propose a novel inference-time defense strategy to mitigate harmful queries, enabling defense against visual and textual jailbreak attacks. Using diverse medical imaging datasets collected from nine modalities, we demonstrate that our defense strategy based on synthetic clinical demonstrations enhances model safety without significantly compromising performance. Additionally, we find that increasing the demonstration budget alleviates the over-defense issue. We then introduce a mixed demonstration strategy as a trade-off solution for balancing security and performance under few-shot demonstration budget constraints. Warning: This paper contains content that may be deemed harmful.",Zhiyu Xue; Reza Abbasi-Asl; Ramtin Pedarsani,Zhiyu Xue,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Domain NLP (Biomedical/Clinical/Legal/Scientific),defense; med vlms; demonstration; med; medical; security; queries; medical vision; medical vision language; harmful
655-MAIN,HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations,"Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements. Evaluated on six diverse hate speech datasets, \textsf{HateXScore} reveals interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Moreover, human evaluation shows strong agreement with \textsf{HateXScore}, validating it as a practical tool for trustworthy and transparent moderation. \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}",Yujia Hu; Roy Ka-Wei Lee,Roy Ka-Wei Lee,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,hateful; reasoning quality; speech; hate speech; hate; moderation; explanations; suite; component; metric
656-MAIN,Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?,"We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using cross-task evaluation across name recognition, profession associations, and education predictions, we measure whether models can be debiased while preserving legitimate demographic detection. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B can reduce bias without degrading recognition: ablating top-attribution features mitigates race and gender profession stereotypes (6вЂ“34%) while preserving name recognition accuracy, whereas ablating top-correlated features more effectively mitigates education bias. Qualitative analysis reveals that ablating top-attribution features causes ``prior collapse'' in education tasks, thus increasing bias in these settings. Our findings establish that demographic bias relies on distributed task-specific mechanisms rather than absolute demographic markers---but also that mechanistic interventions enable surgical debiasing while preserving general competence. This provides empirical evidence that inference-time bias mitigation need not compromise core model capabilities.",Zhengyang Shan; Aaron Mueller,Aaron Mueller,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Interpretability & Model Analysis,"Trustworthy, Safety, Privacy & Fairness",bias; demographic; ablating; recognition; attribution; education; features; preserving; mechanistic; mitigates
665-MAIN,A Survey on LLM-based Conversational User Simulation,"User simulation has long played a vital role in computer science due to its potential to support a wide range of applications. Language, as the primary medium of human communication, forms the foundation of social interaction and behavior. Consequently, simulating conversational behavior has become a key area of study. Recent advancements in large language models (LLMs) have significantly catalyzed progress in this domain by enabling high-fidelity generation of synthetic user conversation. In this paper, we survey recent advancements in LLM-based conversational user simulation. We introduce a novel taxonomy covering user granularity and simulation objectives. Additionally, we systematically analyze core techniques and evaluation methodologies. We aim to keep the research community informed of the latest advancements in conversational user simulation and to further facilitate future research by identifying open challenges and organizing existing work under a unified framework.",Bo Ni; Yu Wang; Leyao Wang; Branislav Kveton; Franck Dernoncourt; Yu Xia; Hongjie Chen; Reuben Luera; Samyadeep Basu; Subhojyoti Mukherjee; Puneet Mathur; Nesreen K. Ahmed; Junda Wu; Li Li; Huixin Zhang; Ruiyi Zhang; Tong Yu; Sungchul Kim; Jiuxiang Gu; Zhengzhong Tu; Alexa Siu; Zichao Wang; Seunghyun Yoon; Nedim Lipka; Namyong Park; Zihao Lin; Trung Bui; Yue Zhao; Tyler Derr; Ryan A. Rossi,Bo Ni,Oral,In-person,Pavillon  DE RABAT,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Dialogue, Conversational & Interactive NLP",Summarization & Generation,user simulation; simulation; user; conversational; advancements; recent advancements; survey; behavior; llm based; vital role
679-MAIN,Prompt-driven Detection of Offensive Urdu Language using Large Language Models,"Offensive language detection poses a significant challenge in modern social spaces, necessitating advanced solutions. Online media platforms have been known to escalate acts of violence and broader conflicts, and thus, an automated system to help counter offensive content is essential. Traditional NLP models have typically dominated the field of hate speech detection, but require careful model design and extensive tuning. Moreover, a notable resource gap exists for addressing offensive languages, particularly those transcribed in non-native scripts, such as Roman Urdu and Urdu. This study explores the potential of pre-trained LLMs in using prompt-based methods using different transcriptions of the Urdu language, particularly their efficacy in detecting offensive content in diverse linguistic contexts. Our study employs state-of-the-art open-source LLMs, including advanced variants of Llama, Qwen, Lughaat, and proprietary GPT-4, which are evaluated through prompting strategies in different under-resourced languages. Our findings show that pre-trained LLMs achieve performance comparable to traditional fine-tuned benchmarks in detecting hateful and offensive content.",Iffat Maab; Usman Haider; Junichi Yamagishi,Iffat Maab,Oral,In-person,SALLE  WALILI,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multimodal & Speech/Audio,"Linguistics, Syntax & Semantics",offensive; offensive content; pre trained llms; trained llms; detection; content; pre trained; advanced; detecting; traditional
680-MAIN,Zer0-Jack: A memory-efficient gradient-based jailbreaking method for black box Multi-modal Large Language Models,"Multi-modal large language models (MLLMs) have recently shown impressive capabilities but are also highly vulnerable to jailbreak attacks. While white-box methods can generate adversarial visual inputs via gradient-based optimization, such approaches fail in realistic black-box settings where model parameters are inaccessible. Zeroth-order (ZO) optimization offers a natural path for black-box attacks by estimating gradients from queries, yet its application to MLLMs is challenging due to sequence-conditioned objectives, limited feedback, and massive model scales. To address these issues, we propose Zer0-Jack, the first direct black-box jailbreak framework for MLLMs based on ZO optimization. Zer0-Jack focuses on generating malicious images and introduces a patch-wise block coordinate descent strategy that stabilizes gradient estimation and reduces query complexity, enabling efficient optimization on billion-scale models. Experiments show that Zer0-Jack achieves 98.2% success on MiniGPT-4 and 95% on the Harmful Behaviors Multi-modal dataset, while directly jailbreaking commercial models such as GPT-4o. These results demonstrate that ZO optimization can be effectively adapted to jailbreak large-scale multi-modal LLMs.Codes are provided in the supplement.",Tiejin Chen; Kaishen Wang; Hua Wei,Kaishen Wang,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Efficiency, Scaling & NLP Systems",box; multi modal; zo optimization; black box; black; modal; optimization; gradient; jailbreak; mllms
683-MAIN,RAGPPI: Retrieval-Augmented Generation Benchmark for ProteinвЂ“Protein Interactions in Drug Discovery,"Retrieving the biological impacts of protein-protein interactions (PPIs) is essential for target identification (Target ID) in drug development. Given the vast number of proteins involved, this process remains time-consuming and challenging. Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) frameworks have supported Target ID; however, no benchmark currently exists for identifying the biological impacts of PPIs. To bridge this gap, we introduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer benchmark of 4,420 question-answer pairs that focus on the potential biological impacts of PPIs. Through interviews with experts, we identified criteria for a benchmark dataset, such as a type of QA and source. We built a gold-standard dataset (500 QA pairs) through expert-driven data annotation. We developed an ensemble auto-evaluation LLM that incorporates expert labeling characteristics, average factвЂ“abstract similarity ($F_1$), and low-similarity fact counts ($F_2$), enabling the construction of a silver-standard dataset (3,720 QA pairs). We are committed to maintaining RAGPPI as a resource to support the research community in advancing RAG systems for drug discovery QA solutions.",Youngseung Jeon; Ziwen Li; Thomas Li; JiaSyuan Chang; Morteza Ziyadi; Xiang Anthony Chen,Youngseung Jeon,Oral,In-person,SALLE  WALILI,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)","LLM Evaluation, Benchmarks & Metrics",biological; drug; impacts; benchmark; rag; target; qa pairs; pairs; question answer; discovery
684-MAIN,"Don't Generate, Classify! Low-Latency Prompt Optimization with Structured Complementary Prompt","Large language models (LLMs) have demonstrated strong performance across diverse natural language processing tasks. However, their performance varies significantly across different prompts, requiring careful engineering for consistent results. Manual prompt engineering requires substantial human effort and limits reproducibility, while automatic prompt optimization techniques, though reducing manual labor, rely on costly autoregressive generation and incur significant latency overheads. To address these limitations, we present low-latency prompt optimization (LLPO), a novel framework that reframes prompt engineering as a classification problem. LLPO classifies structured prompt fields from user input through multi-task classification and populates a predefined template to generate an optimized system prompt with minimal latency. In LLM-based automatic evaluations across four question-answering benchmarks, LLPO improves answer quality by up to 26.5% in в€†win rate compared to prior automatic prompt optimization methods, while reducing latency by up to 1,956 times. Human evaluation shows that LLPO receives the highest proportion of top-ranked responses. Furthermore, ablation studies analyze the contribution of each structured prompt field to performance, highlighting the robustness of our framework.",Hee-Soo Kim; Junyoung Kim; Jeonghwan Lee; Seong-Jin Park; Kang-Min Kim,Hee-Soo Kim,Oral,In-person,SALLE  WALILI,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",prompt; latency; prompt optimization; optimization; engineering; structured prompt; automatic prompt; automatic prompt optimization; low latency; automatic
685-MAIN,CHROMIC: Chronological Reasoning Across Multi-Panel Comics,"Large-scale visionвЂ“language models (LVLMs) have achieved remarkable progress on various reasoning tasks. However, most studies focus on natural photographic images and pay limited attention to multi-panel visual narratives such as comics. This leaves a clear gap in our understanding of how well LVLMs perform chronological reasoning across comic panels. To address this, we introduce **ChrOMIC**, a new benchmark dataset for **chro**nological reasoning in multi-panel **comic**s. It covers six types of reasoning questions and spans both Western and Japanese comic styles. To ensure high-quality annotations, we customized a humanвЂ“AI collaborative annotation process tailored to the characteristics of the two comic styles. We further introduce three core tasks: Description Reordering and Panel Reordering, which jointly assess modelsвЂ™ ability to understand chronological order in panel sequences, and Multiple-Choice Question Answering (MCQA), which evaluates narrative-level reasoning. We evaluate a range of open-source and commercial LVLMs on ChrOMIC, and find that even the leading models struggle with panel-based chronological reasoning. Further analysis reveals key limitations, including weak visual action understanding and frequent hallucinations in fine-grained visual interpretation.",Bingxuan Hou; Jiayi Lin; Chenyang Zhang; Dapeng Yin; Shuyue Zhu; Qingqing Hong; Mengna Gao,Bingxuan Hou,Oral,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Reasoning, Planning & Agents",panel; chronological; lvlms; reasoning; comics; reordering; reasoning multi; styles; visual; multi
695-MAIN,GAST: Gradient-aligned Sparse Tuning of Large Language Models with Data-layer Selection,"Parameter-Efficient Fine-Tuning (PEFT) has become a key strategy for adapting large language models, with recent advances in sparse tuning reducing overhead by selectively updating key parameters or subsets of data. Existing approaches generally focus on two distinct paradigms: layer-selective methods aiming to fine-tune critical layers to minimize computational load, and data-selective methods aiming to select effective training subsets to boost training. However, current methods typically overlook the fact that different data points contribute varying degrees to distinct model layers, and they often discard potentially valuable information from data perceived as of low quality. To address these limitations, we propose Gradient-aligned Sparse Tuning (GAST), an innovative method that simultaneously performs selective fine-tuning at both data and layer dimensions as integral components of a unified optimization strategy. GAST specifically targets redundancy in information by employing a layer-sparse strategy that adaptively selects the most impactful data points for each layer, providing a more comprehensive and sophisticated solution than approaches restricted to a single dimension. Experiments demonstrate that GAST consistently outperforms baseline methods, establishing a promising direction for future research in PEFT strategies.",Kai Yao; Penglei Gao; Zhaorui Tan; Kaixin Wu; Danzhao Cheng; Yixin Ji; Zhenghan Song; mingjie zhong,Kaixin Wu,Oral,In-person,Pavillon  DE RABAT,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Efficiency, Scaling & NLP Systems",,layer; sparse; selective; tuning; subsets; aiming; peft; strategy; gradient; methods
696-MAIN,BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models,"We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., just), and also aggregates a shared-task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier reasoning model GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibits strong performance in arithmetic aspect of temporal reasoning, but they struggle with long-dependency reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation classification.",Chuyuan Li; Giuseppe Carenini,Chuyuan Li,Oral,In-person,SALLE  LE LIXUS,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Linguistics, Syntax & Semantics",Information Extraction & Structured Prediction,discourse; relation; discourse understanding; discourse relation; relation classification; reasoning language models; reasoning; reasoning language; era; parsing
705-MAIN,Confidence-Calibrated Small-Large Language Model Collaboration for Cost-Efficient Reasoning,"Large language models (LLMs) demonstrate superior reasoning capabilities compared to small language models (SLMs), but incur substantially higher costs. We propose COllaborative REAsoner (COREA), a system that cascades an SLM with an LLM to achieve a balance between accuracy and cost in complex reasoning tasks. COREA first attempts to answer questions using the SLM, which outputs both an answer and a verbalized confidence score. Questions with confidence below a predefined threshold are deferred to the LLM for more accurate resolution. We introduce a reinforcement learning-based training algorithm that aligns the SLM's confidence through an additional confidence calibration reward. Extensive experiments demonstrate that our method jointly improves the SLM's reasoning ability and confidence calibration across diverse datasets and model backbones. Compared to using the LLM alone, COREA reduces cost by 21.5% and 16.8% on out-of-domain math and non-math datasets, respectively, with only an absolute pass@1 drop within 2%.",Chuang Zhang; Zizhen Zhu; Yihao Wei; Bing Tian; Junyi Liu; Henan Wang; Wang Xavier; Yaxiao Liu,Chuang Zhang,Oral,In-person,SALLE  LE LIXUS,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",confidence; slm; confidence calibration; cost; calibration; math; reasoning; questions; small; compared
710-MAIN,Chat-Ghosting: Methods for Auto-Completion in Dialog Systems,"Ghosting, the ability to predict a user's intended text input for inline query auto-completion, is an invaluable feature for modern search engines and chat interfaces, greatly enhancing user experience. By suggesting completions to incomplete queries (or prefixes), ghosting aids users with slow typing speeds, disabilities, or limited language proficiency. Ghosting is a challenging problem and has become more important with the ubiquitousness of chat-based systems like ChatGPT, Copilot, etc. Despite the increasing prominence of chat-based systems utilizing ghosting, this challenging problem of Chat-Ghosting has received little attention from the NLP/ML research community. There is a lack of standardized benchmarks and relative performance analysis of deep learning and non-deep learning methods. We address this through an open and thorough study of this problem using four publicly available dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and two human-bot (Open Assistant and ShareGPT). We experiment with various existing query auto-completion methods (using tries), n-gram methods and deep learning methods, with and without dialog context. We also propose a novel entropy-based dynamic early stopping strategy. Our analysis finds that statistical n-gram models and tries outperform deep learning based models in terms of both model performance and inference efficiency for seen prefixes. For unseen queries, neural models like T5 and Phi-2 lead to better results. Adding conversational context leads to significant improvements in ghosting quality, especially for Open-Assistant and ShareGPT. We make code and data publicly available at https://anonymous.4open.science/r/Chat-AutoSuggest-Project-DF8F/",Anubhab Mandal; Sandeep Mishra; Bishal Santra; Tushar Abhishek; Pawan Goyal; Manish Gupta,Tushar Abhishek,Oral,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP","Efficiency, Scaling & NLP Systems",chat; deep learning; auto completion; deep; dialog; auto; completion; chat based; deep learning methods; challenging problem
711-MAIN,Attribution-Guided Multi-Object Hallucination and Bias Detection in Vision-Language Models,"Vision-Language Models excel in multi-modal tasks but often hallucinate objects or exhibit linguistic bias by over-repeating object names, especially in complex multi-object scenes. Existing methods struggle with multi-object grounding because language priors frequently dominate visual evidence, causing hallucinated or biased objects to produce attention distributions or similarity scores nearly indistinguishable from those of real objects. We introduce SHAPLENS, a Shapley valueвЂ“based attribution framework using Kernel SHAP and multi-layer fusion to detect hallucinated and biased objects. Evaluated on ADE and COCO datasets across four leading VLMs, SHAPLENS improves hallucination detection accuracy by 8вЂ“12% and F1 by 10вЂ“14% over the best baselines. It also achieves up to 6% higher bias detection performance across three distinct bias types on a curated HQH benchmark and exhibits minimal degradation (<0.03%) across partial and perturbed contexts.",Sirat Samyoun; Yingtai Xiao; Jian Du,Sirat Samyoun,Oral,In-person,Pavillon  DE RABAT,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Interpretability & Model Analysis,"Retrieval, Grounding & External Knowledge (RAG)",objects; object; bias; multi; hallucinated; biased; attribution; detection; hallucination; vision language models
714-MAIN,Word Surprisal Correlates with Sentential Contradiction in LLMs,"Large language models (LLMs) continue to achieve impressive performance on reasoning benchmarks, yet it remains unclear how their predictions capture semantic consistency between sentences. We investigate the important open question of whether word-level surprisal correlates with sentence-level contradiction between a premise and a hypothesis. Specifically, we compute surprisal for hypothesis words across a diverse set of experimental variants, and analyze its association with contradiction labels over multiple datasets and open-source LLMs. Because modern LLMs operate on subword tokens and can not directly produce reliable surprisal estimates, we introduce a token-to-word decoding algorithm that extends theoretically grounded probability estimation to open-vocabulary settings. Experiments show a consistent and statistically significant positive correlation between surprisal and contradiction across models and domains. Our analysis also provides new insights into the capabilities and limitations of current LLMs. Together, our findings suggest that surprisal can localize sentence-level inconsistency at the word level, establishing a quantitative link between lexical uncertainty and sentential semantics. We plan to release our code and data upon publication.",Ning Shi; Bradley Hauer; David Basil; John Zhang; Grzegorz Kondrak,Ning Shi,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Linguistics, Syntax & Semantics",surprisal; contradiction; word; sentential; word level; level; correlates; sentence level; hypothesis; open
715-MAIN,ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models,"Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a ""concept misalignment'' in their internal representations rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned modelsin generating soft refusals due to adversarial training. We make our codebase available at https://anonymous.4open.science/r/ARREST-LLM .",Sharanya Dasgupta; ARKAPRABHA BASU; Sujoy Nath; Swagatam Das,Sharanya Dasgupta,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",,safety; enhancing safety; adversarial; refusals; regulation; resilient; human cognition; cognition; misalignment; soft
717-MAIN,$\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts,"The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5% in accuracy over conventional MoE.",Shota Takashiro; Takeshi Kojima; Shohei Taniguchi; Yusuke Iwasawa; Yutaka Matsuo,Shota Takashiro,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Efficiency, Scaling & NLP Systems","Trustworthy, Safety, Privacy & Fairness",experts; moe; number; infinite; parameters; sampled; mixture experts; selects; conventional; mixture
718-MAIN,Beyond Tokens: Concept-Level Training Objectives for LLMs,"The next-token prediction (NTP) objective has been foundational in the development of modern large language models (LLMs), driving advances in fluency and generalization. However, NTP operates at the token level, treating deviations from a single reference continuation as errors even when alternative continuations are equally plausible or semantically equivalent. As a result, token-level loss can penalize valid abstractions, paraphrases, or conceptually correct reasoning paths, biasing models toward surface form rather than underlying meaning. This mismatch between the training signal and semantic correctness motivates learning objectives that operate over higher-level representations. We propose a shift from token-level to concept-level prediction, where concepts group multiple surface forms of the same idea (e.g., ""mom,"" ""mommy,"" ""mother"" $\rightarrow$ \textit{MOTHER}). We introduce various methods for integrating conceptual supervision into LLM training and show that concept-aware models achieve lower perplexity, improved robustness under domain shift, and stronger performance than NTP-based models on diverse NLP benchmarks. This suggests concept-level supervision as an improved training signal that better aligns LLMs with human semantic abstractions.",Laya Iyer; Pranav Somani; Alice Guo; Dan Jurafsky; Chen Shani,Laya Iyer,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Reasoning, Planning & Agents",,level; concept level; concept; token level; token; abstractions; signal; shift; objectives; training
721-MAIN,Re$^2$-DocRED: Revisiting Revisited-DocRED for Joint Entity and Relation Extraction,"Document-level Joint Entity and Relation Extraction (JERE) benchmarks such as DocRED, Re-DocRED, and DocGNRE suffer from pervasive False Negatives (FN), especially in long-tail relations, undermining training and evaluation. In this paper, we introduce SiftingLogic -- a training-free annotation pipeline that leverages LLMs with user-specifiable reasoning, enriched inverse/co-occurring relation schemas, and novel entity-level constraints to systematically address FN gaps. Applying SiftingLogic and our enriched schema of inverse and co-occurring relations, we add 29,580 verified triplets to Re-DocRED (train/dev, +27\%) and over 9,700 verified triplets to DocGNRE test (+49.89\%), yielding the enhanced Re$^2$-DocRED dataset. Beyond English datasets, we also apply our SiftingLogic to REDFM Mandarin test set, resulting in a significant increase in triplets from 663 to 1,391 (+109.8\%) demonstrating our pipeline's generalisability across languages and datasets. Our experiments show that recall scores of models trained on existing public datasets drop notably on our revised splits, whereas our enriched training set mitigates this, underscoring persistent FN gaps and motivating our proposed SiftingLogic and Re$^2$-DocRED. To facilitate further research and reproducibility, we release the revised datasets and our checkpoints.",Chen Kim Heng; Shao Wen Tong; Julian Wong Wei Sheng,Chen Kim Heng,Oral,In-person,SALLE  LE LIXUS,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Information Extraction & Structured Prediction,"Retrieval, Grounding & External Knowledge (RAG)",triplets; enriched; relation; revised; occurring; relation extraction; entity; inverse; joint; datasets
724-MAIN,Where Do LLMs Compose Meaning? A Layerwise Analysis of Compositional Robustness,"Understanding how large language models (LLMs) process compositional linguistic structures is integral to enhancing their reliability and interpretability. We present Constituent-Aware Pooling (CAP), a methodology grounded in compositionality, mechanistic interpretability, and information theory that intervenes in model activations by pooling token representations into linguistic constituents at various layers. Experiments across eight models (124M-8B parameters) on inverse definition modelling, hypernym and synonym prediction reveal that semantic composition is not localised to specific layers but distributed across network depth. Performance degrades substantially under constituent-based pooling, particularly in early and middle layers, with larger models showing greater sensitivity. We propose an information-theoretic interpretation: transformers' training objectives incentivise deferred integration to maximise token-level throughput, resulting in fragmented rather than localised composition. These findings highlight fundamental architectural and training constraints requiring specialised approaches to encourage robust compositional processing.",Nura Aljaafari; Danilo Carvalho; Andre Freitas,Nura Aljaafari,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Interpretability & Model Analysis,,pooling; compositional; layers; constituent; composition; interpretability; token; pooling token; mechanistic interpretability; deferred
729-MAIN,BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models,"As vision-language models (VLMs) are deployed globally, their ability to understand culturally situated knowledge becomes essential. Yet, existing evaluations largely assess static recall or isolated visual grounding, leaving unanswered whether VLMs possess robust and transferable cultural understanding. We introduce `BLEnD-Vis`, a multimodal, multicultural benchmark designed to evaluate the robustness of everyday cultural knowledge in VLMs across linguistic rephrasings and visual modalities. Building on the BLEnD dataset, `BLEnD-Vis` constructs 313 culturally grounded question templates spanning 16 regions and generates three aligned multiple-choice formats: (i) a text-only baseline querying from Region $\rightarrow$ Entity, (ii) an inverted text-only variant (Entity $\rightarrow$ Region), and (iii) a VQA-style version of (ii) with generated images. The resulting benchmark comprises 4,916 images and over 21,000 multiple-choice question (MCQ) instances, validated through human annotation. `BLEnD-Vis` reveals significant fragility in current VLM cultural knowledge; models exhibit performance drops under linguistic rephrasing and, whilst visual cues often aid performance, low cross-modal consistency highlights challenges in robustly integrating textual and visual understanding, particularly for lower-resource regions. `BLEnD-Vis` thus provides a crucial testbed for systematically analysing cultural robustness and multimodal grounding, exposing limitations and guiding the development of more culturally competent VLMs.",Bryan Chen Zhengyu Tan; Weihua Zheng; Zhengyuan Liu; Nancy F. Chen; Hwaran Lee; Kenny Tsu Wei Choo; Roy Ka-Wei Lee,Weihua Zheng,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",vis; cultural; vlms; culturally; visual; rightarrow; cultural knowledge; region; regions; multimodal
730-MAIN,Document-Level Zero-Shot Relation Extraction with Entity Side Information,"Document-Level Zero-Shot Relation Extraction (DocZSRE) aims to predict unseen relation labels in text documents without prior training on specific relations. Existing approaches rely on Large Language Models (LLMs) to generate synthetic data for unseen labels, which poses challenges for low-resource languages like Malaysian English. These challenges include the incorporation of local linguistic nuances and the risk of factual inaccuracies in LLM-generated data. This paper introduces Document-Level Zero-Shot Relation Extraction with Entity Side Information (DocZSRE-SI) to address limitations in the existing DocZSRE approach. The DocZSRE-SI framework leverages Entity Side Information, such as Entity Mention Descriptions and Entity Mention Hypernyms, to perform ZSRE without depending on LLM-generated synthetic data. The proposed low-complexity model achieves an average improvement of 11.6% in the macro F1-Score compared to baseline models and existing benchmarks. By utilising Entity Side Information, DocZSRE-SI offers a robust and efficient alternative to error-prone, LLM-based methods, demonstrating significant advancements in handling low-resource languages and linguistic diversity in relation extraction tasks. This research provides a scalable and reliable solution for ZSRE, particularly in contexts like Malaysian English news articles, where traditional LLM-based approaches fall short.",Mohan Raj; Lay-Ki Soon; Huey Fang Ong; Bhawani Selvaretnam,Mohan Raj Chanthran,Oral,Virtual,ZOOM,Virtual TBA,,,Information Extraction & Structured Prediction,Multilinguality & Low-Resource NLP,entity information; relation; relation extraction; entity; extraction; document level; document; mention; zero shot; zero
734-MAIN,Steering Large Language Models for Machine Translation Personalization,"Large language models have simplified the production of personalized translations reflecting predefined stylistic constraints. However, these systems still struggle when stylistic requirements are implicitly represented by a set of examples, such as texts produced by a specific human translator. In this work, we explore various strategies for personalizing automatically generated translations when few examples are available, with a focus on the challenging domain of literary translation. We begin by determining the feasibility of the task and how style information is encoded within model representations. Then, we evaluate various prompting strategies and inference-time interventions for steering model generations towards a personalized style, with a particular focus on contrastive steering with sparse autoencoder (SAE) latents to identify salient personalization properties. We demonstrate that contrastive SAE steering yields robust style conditioning and translation quality, resulting in higher inference-time computational efficiency than prompting approaches. We further examine the impact of steering on model activations, finding that layers encoding personalization properties are impacted similarly by prompting and SAE steering, suggesting a similar mechanism at play.",Daniel Scalena; Gabriele Sarti; Arianna Bisazza; Elisabetta Fersini; Malvina Nissim,Daniel Scalena,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Machine Translation,"Efficiency, Scaling & NLP Systems",steering; personalization; sae; steering model; style; translation; stylistic; prompting; translations; personalized
737-MAIN,Taxation Perspectives from Large Language Models: A Case Study on Additional Tax Penalties,"How capable are large language models (LLMs) in the domain of taxation? Although numerous studies have explored the legal domain, research dedicated to taxation remains scarce. Moreover, the datasets used in these studies are either simplified, failing to reflect the real-world complexities, or not released as open-source. To address this gap, we introduce PLAT, a new benchmark designed to assess the ability of LLMs to predict the legitimacy of additional tax penalties. PLAT comprises 300 examples: (1) 100 binary-choice questions, (2) 100 multiple-choice questions, and (3) 100 essay-type questions, all derived from 100 Korean court precedents. PLAT is constructed to evaluate not only LLMsвЂ™ understanding of tax law but also their performance in legal cases that require complex reasoning beyond straight forward application of statutes. Our systematic experiments with multiple LLMs reveal that (1) their baseline capabilities are limited, especially in cases involving conflicting issues that require a comprehensive understanding (not only of the statutes but also of the taxpayerвЂ™s circumstances), and (2) LLMs struggle particularly with the вЂњACвЂќ stages of вЂњIRACвЂќ even for advanced reasoning models like o3, which actively employ inference-time scaling.",Eunkyung Choi; Young Jin Suh; Siun Lee; Hongseok Oh; Juheon Kang; Won Hur; HUN PARK; Wonseok Hwang,Eunkyung Choi,Oral,In-person,SALLE  LE LIXUS,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Efficiency, Scaling & NLP Systems",tax; penalties; statutes; choice questions; questions; legal; choice; cases; additional; require
738-MAIN,Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing,"Knowledge editing (KE) has recently emerged as a promising technique to update specific facts in large language models (LLMs) without full retraining. While existing KE methods show promising results on general-domain benchmarks, their effectiveness in the medical domain remains largely unexplored. Medical knowledge editing poses unique challenges, requiring models not only to memorize new facts but also to internalize and generalize them for reliable and interpretable clinical decision-making. In this work, we propose MedEditBench, a rigorous evaluation framework for assessing medical knowledge editing. Our preliminary results reveal that current KE paradigm, which directly edits simple answers to the LLMs, often leads to superficial updates with poor generalization. To address this, we introduce Self-Generated Rationale Editing (SGR-Edit), which leverages model-generated rationales as editing targets, enabling deeper knowledge integration. Extensive experiments across diverse LLMs and KE methods demonstrate that SGR-Edit consistently improves editing efficacy and generalization. Furthermore, we examine the impact of sequential edits on in-domain medical knowledge, external-domain knowledge, as well as general model capabilities, offering practical insights for deploying KE in real-world medical applications.",Shigeng Chen; Linhao Luo; Zhangchi Qiu; Yanan Cao; Carl Yang; Shirui Pan,Shigeng Chen,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"LLM Evaluation, Benchmarks & Metrics",editing; medical knowledge; medical; knowledge editing; knowledge; ke methods; rigorous evaluation; edit; edits; domain
750-MAIN,Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding,"Large language models (LLMs) have emerged as strong contenders in machine translation. Yet, they still struggle to adequately handle discourse phenomena, such as pronoun resolution and lexical cohesion at the document level. In this study, we thoroughly investigate the discourse phenomena performance of LLMs in context-aware translation. We demonstrate that discourse knowledge is encoded within LLMs and propose the use of quality-aware decoding (QAD) to effectively extract this knowledge, showcasing its superiority over other decoding approaches through comprehensive analysis. Furthermore, we illustrate that QAD enhances the semantic richness of translations and aligns them more closely with human preferences.",Wafaa Mohammed; Vlad Niculae; Chrysoula Zerva,Wafaa Mohammed,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Machine Translation,Interpretability & Model Analysis,discourse; discourse phenomena; decoding; translation; phenomena; aware; llms; closely human preferences; encoded llms; llms quality
752-MAIN,Cross-lingual and Word-Independent Methods for Quantifying Degree of Grammaticalization,"Grammaticalization denotes a diachronic change of the grammatical category from content words to function words. One of the intensively explored directions in this area is to quantify the degree of grammaticalization. There have been a limited number of automated methods for this task and the existing, best-performing method is heavily language- and word-dependent. In this paper, we explore three methods for quantifying the degree of grammaticalization, which are applicable to a much wider variety of words and languages than before. The difficulty here is that training data is not available in the present task. We overcome this difficulty by using Positive-Unlabeled learning (PU-learning) or Cross-Validation-like learning (hereafter, CV-learning). Experiments show that the CV-learning-based method achieves middle to high correlations to human judgments in English deverbal prepositions and Japanese nouns being grammaticalized. With this method, we further explore words possibly being grammaticalized and counterexamples of the unidirectionality hypothesis.",Ryo Nagata; Daichi Mochihashi; Misato Ido; Yusuke Kubota; Naoki Otani; Yoshifumi Kawasaki; Hiroya Takamura,Ryo Nagata,Oral,In-person,SALLE  LE LIXUS,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multilinguality & Low-Resource NLP,,words; degree; learning; quantifying; word; difficulty; explore; methods; performing method; best performing method
758-MAIN,Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities,"Large Language Models (LLMs) are increasingly used for knowledge-based reasoning tasks, yet understanding when they rely on genuine knowledge versus superficial heuristics remains challenging. We investigate this question through entity comparison tasks by asking models to compare entities along numerical attributes (e.g., ``Which river is longer, the Danube or the Nile?''), which offer clear ground truth for systematic analysis. Despite having sufficient numerical knowledge to answer correctly, LLMs frequently make predictions which contradict this knowledge. We identify three heuristic biases that strongly influence model predictions: entity popularity, mention order, and semantic co-occurrence. For smaller models, a simple logistic regression using only these surface cues predicts model choices more accurately than the model's own numerical predictions, suggesting heuristics largely override principled reasoning. Crucially, we find that larger models (32B parameters) selectively rely on numerical knowledge when it is more reliable, while smaller models (7-8B parameters) show no such discrimination, which explains why larger models outperform smaller ones even when the smaller models possess more accurate knowledge. Chain-of-thought prompting steers all models towards using the numerical features across all model sizes.",Hans Hergen Lehmann; Jae Hee Lee; Steven Schockaert; Stefan Wermter,Hans Hergen Lehmann,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Interpretability & Model Analysis,numerical; smaller; knowledge; smaller models; predictions; models compare; heuristics; larger models; entities; compare
760-MAIN,Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLMs,"Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) across two quantizers (GPTQ, AWQ) on data from 10 different languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 perplexity gain. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static, one-size-fits-all calibration is suboptimal, and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.",Everlyn Asiko Chimoto; Mostafa Elhoushi; Bruce Bassett,Everlyn Asiko Chimoto,Oral,In-person,SALLE  LE LIXUS,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",Multilinguality & Low-Resource NLP,calibration; multilingual; perplexity; sets; tailoring; english; quantization; multilingual llms; largest; languages
763-MAIN,LaCoMSA: Language-Consistency Multilingual Self-Alignment with Latent Representation Rewarding,"Large Language Models (LLMs) have achieved impressive performance yet remain inconsistent across languages, often defaulting to high-resource outputs such as English. Existing multilingual alignment methods mitigate these issues through preference optimization but rely on external supervision, such as translation systems or English-biased signal. We propose Multilingual Self-Alignment (MSA), a targeted preference optimization framework that leverages an LLMвЂ™s own latent representations as intrinsic supervision signals, rewarding lower-resource language outputs based on their alignment with high-resource (English) counterparts in the ""semantic hub"". We further introduce Language-Consistency MSA (LaCoMSA), which augments MSA with a final-layer language-consistency factor to prevent off-target generation. Integrated with Direct Preference Optimization, LaCoMSA improves a Llama 3 8B-based model multilingual win rates by up to 6.8% absolute (55.0% relatively) on X-AlpacaEval and achieves consistent gains across benchmarks and models. Our findings demonstrate that LaCoMSA can serve as an effective and scalable mechanism, opening a new venue toward multilingual self-alignment.",Khanh-Tung Tran; Barry O'Sullivan; Hoang D. Nguyen,Khanh-Tung Tran,Oral,In-person,SALLE  LE LIXUS,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Interpretability & Model Analysis,Multilinguality & Low-Resource NLP,language consistency; multilingual self; multilingual; preference optimization; alignment; preference; rewarding; consistency; self; optimization
770-MAIN,Can you map it to English? The Role of Cross-Lingual Alignment in Multilingual Performance of LLMs,"Large language models (LLMs) can be prompted in many languages, despite being trained predominantly on English; yet, the mechanisms driving this generalization remain poorly understood. This work answers the question: Does an LLM's ability to align representations of non-English inputs to English impact its ability to solve NLU tasks? While prior works analyze alignment at the language-level and independently from the task, our study investigates the role of representation alignment in instance-level task decisions. We introduce the Discriminative Alignment Index ($\DALI$) to quantify instance-level alignment across 24 non-English languages and three distinct NLU tasks. Results show that incorrect predictions are strongly associated with lower representation alignment with English in the middle layers of the model. We further use activation patching to measure the extent to which representation (mis)alignment explains the correctness of answers in non-English languages.",Kartik Ravisankar; HyoJung Han; Sarah Wiegreffe; Marine Carpuat,Kartik Ravisankar,Oral,Virtual,ZOOM,Virtual TBA,,,Multilinguality & Low-Resource NLP,Interpretability & Model Analysis,english; alignment; non english; non english languages; english languages; nlu; instance level; representation; non; instance
771-MAIN,Recursive numeral systems are highly regular and easy to process,"Previous work has argued that recursive numeral systems optimise the trade-off between lexicon size and average morphosyntatic complexity (DeniД‡ and Szymanik, 2024). However, showing that only natural-language-like systems optimise this tradeoff has proven elusive, and the existing solution has relied on ad-hoc constraints to rule out unnatural systems (Yang and Regier, 2025). Here, we argue that this issue arises because the proposed trade-off has neglected regularity, a crucial aspect of complexity central to human grammars in general. Drawing on the Minimum Description Length (MDL) approach, we propose that recursive numeral systems are better viewed as efficient with regard to their regularity and processing complexity. We show that our MDL-based measures of regularity and processing complexity better capture the key differences between attested, natural systems and unattested but possible ones, including ``optimal'' recursive numeral systems from previous work, and that the ad-hoc constraints from previous literature naturally follow from regularity. Our approach highlights the need to incorporate regularity across sets of forms in studies that attempt to measure and explain optimality in language.",Ponrawee Prasertsom; Andrea Silvi; Jennifer Culbertson; Devdatt Dubhashi; Moa Johansson; Kenny Smith,Ponrawee Prasertsom,Oral,In-person,Pavillon  DE RABAT,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Efficiency, Scaling & NLP Systems",,recursive; systems; complexity; previous; ad hoc; hoc; previous work; constraints; trade; processing
772-MAIN,Bringing Emerging Architectures to Sequence Labeling in NLP,"Pretrained Transformer encoders are the dominant approach to sequence labeling. While some alternative architectures-such as xLSTMs, structured state-space models, diffusion models, and adversarial learning-have shown promise in language modeling, few have been applied to sequence labeling, and mostly on flat or simplified tasks. We study how these architectures adapt across tagging tasks that vary in structural complexity, label space, and token dependencies, with evaluation spanning multiple languages. We find that the strong performance previously observed in simpler settings does not always generalize well across languages or datasets, nor does it extend to more complex structured tasks.",Ana Ezquerro; Carlos GГіmez-RodrГ­guez; David Vilares,Carlos Gómez-Rodríguez,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Information Extraction & Structured Prediction,"LLM Evaluation, Benchmarks & Metrics",sequence labeling; labeling; sequence; architectures; does; space; structural complexity; spanning multiple languages; modeling applied; languages strong
773-MAIN,SEMIROUTER: Sparse-Data Enhanced Routing for Adaptive Multi-LLM System,"Large Language Models (LLMs) exhibit remarkable capabilities, but no single model optimally balances serving quality and deployment cost across diverse tasks. Multi-LLM systems address this challenge through intelligent routing mechanisms that dynamically allocate queries to the most appropriate model. However, existing routing methods suffer from two fundamental limitations: (i) dependence on extensive full-response datasets for training, and (ii) poor scalability when incorporating new models, typically necessitating retraining from scratch. In this paper, we propose SemiRouter, a novel LLM routing framework designed for data-sparse and evolving model environments. Our approach combines a data-efficient training methodology with an adaptive architecture that enables seamless integration of new models under limited supervision. As an extension, we also consider energy footprint as a potential deployment cost in our routing decision. Empirical evaluations demonstrate that our method improves data efficiency, adaptability, and routing accuracy compared to existing approaches, providing a scalable solution for dynamic multi-LLM deployment.",Zijie Wang; Xinyu Yan; CHE WANG; Zeng Zihao; Lei Xiao; Wei Yang Bryan Lim,WANG ZIJIE,Oral,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",,routing; multi llm; deployment; sparse; adaptive; llm; multi; cost; tasks multi; capabilities single
781-MAIN,DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation,"The promise of LLM watermarking rests on a core assumption that a specific watermark proves authorship by a specific model. We demonstrate that this assumption is dangerously flawed. We introduce the threat of watermark spoofing, a sophisticated attack that allows a malicious model to generate text containing the authentic-looking watermark of a trusted, victim model. This enables the seamless misattribution of harmful content, such as disinformation, to reputable sources. The key to our attack is repurposing watermark radioactivity, the unintended inheritance of data patterns during fine-tuning, from a discoverable trait into an attack vector. By distilling knowledge from a watermarked teacher model, our framework allows an attacker to steal and replicate the watermarking signal of the victim model. This work reveals a critical security gap in text authorship verification and calls for a paradigm shift towards technologies capable of distinguishing authentic watermarks from expertly imitated ones. Our code is available at https://anonymous.4open.science/r/ditto-8838/.",Hyeseon Ahn; Shinwoo Park; Suyeon Woo; Yo-Sub Han,Hyeseon An,Oral,In-person,SALLE  WALILI,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems","Trustworthy, Safety, Privacy & Fairness",watermark; attack; spoofing; watermarked; authorship; authentic; watermarking; assumption; allows; model demonstrate
782-MAIN,Boundary-Aware LLM Augmentation for Low-Resource Event Argument Extraction,"Event argument extraction (EAE) is a crucial task in information extraction. However, its performance heavily depends on expensive annotated data, making data scarcity a persistent challenge. Data augmentation serves as an effective approach to improving model performance in low-resource settings, yet research on applying LLMs for EAE augmentation remains preliminary. In this study, we pay attention to the boundary sensitivity of EAE and investigate four LLM-based augmentation strategies: argument replacement, adjunction rewriting, their combination, and annotation generation. We conduct comprehensive experiments across four benchmark datasets, employing GPT-4o-Mini and DeepSeek-R1-7B as data generators. Our results show that boundary-aware augmentation consistently leads to greater performance improvements over boundary-agnostic methods. In addition to performance gains, we provide a detailed analysis of augmentation quality from multiple perspectives, including uncertainty reduction, error types, data quality, and data scale. This work offers both empirical evidence and practical guidance for leveraging LLMs to enhance event argument extraction under low-resource conditions.",ZHAOYUE SUN; Gabriele Pergola; Yulan He,Zhaoyue Sun,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Information Extraction & Structured Prediction,augmentation; argument; boundary; extraction; event; low resource; resource; low; pay; provide detailed
784-MAIN,CASE вЂ“ Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement,"The meaning conveyed by a sentence often depends on the context in which it appears. Despite the progress of sentence embedding methods, it remains unclear as how to best modify a sentence embedding conditioned on its context. To address this problem, we propose Condition-Aware Sentence Embeddings (CASE), an efficient and accurate method to create an embedding for a sentence under a given condition. First, CASE creates an embedding for the condition using an Large Language Model (LLM) encoder, where the sentence influences the attention scores computed for the tokens in the condition during pooling. Next, a supervised method is learnt to align the LLM-based text embeddings with the Conditional Semantic Textual Similarity (C-STS) task. We find that subtracting the condition embedding will consistently improve the C-STS performance of LLM-based text embeddings and improve the isotropy of the embedding space. Moreover, our supervised projection method significantly improves the performance of LLM-based embeddings despite requiring a small number of embedding dimensions.",Gaifan Zhang; Yi Zhou; Danushka Bollegala,Gaifan Zhang,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Interpretability & Model Analysis,,condition; sentence; embedding; embeddings; case; performance llm based; sts; semantic textual similarity; textual similarity; semantic textual
786-MAIN,Evaluation and LLM-Guided Learning of ICD Coding Rationales,"ICD coding is the process of mapping unstructured text from Electronic Health Records (EHRs) to standardised codes defined by the International Classification of Diseases (ICD) system. In order to promote trust and transparency, existing explorations on the explainability of ICD coding models primarily rely on attention-based rationales and qualitative assessments conducted by physicians, yet lack a systematic evaluation across diverse types of rationales using consistent criteria and high-quality rationale-annotated datasets specifically designed for the ICD coding task. Moreover, there remains a scarcity of dedicated methods explicitly trained to generate plausible rationales. In this work, we present evaluations of the explainability of rationales in ICD coding, focusing on two fundamental dimensions: faithfulness and plausibility. To enable the systematic evaluation of plausibility, we construct a new rationale-annotated dataset specifically for ICD coding, constructed in accordance with the updating MIMIC-IV database and the ICD-10 coding system. This dataset features richer annotations with diverse levels of granularity. We conduct a comprehensive evaluation across three types of ICD coding rationales: entity-level mentions derived from entity linking dataset, LLM-generated rationales, and attention-based rationales. Building upon the strong plausibility exhibited by LLM-generated rationales, we further leverage them as distant supervision signals to develop rationale learning methods. Additionally, by prompting the LLM with few-shot human-annotated examples from our dataset, we achieve notable improvements in the plausibility of rationale generation in both the teacher LLM and the student rationale learning models.",Mingyang Li; Viktor Schlegel; Tingting Mu; Wuraola Oyewusi; Kai Kang; Goran Nenadic,Mingyang Li,Oral,In-person,SALLE  LE LIXUS,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),Information Extraction & Structured Prediction,icd; rationales; coding; rationale; plausibility; llm generated rationales; attention based; generated rationales; explainability; annotated
787-MAIN,Evaluating the Effect of Retrieval Augmentation on Social Biases,"Retrieval Augmented Generation (RAG) is a popular method for injecting up-to-date information into Large Language Model (LLM)-based Natural Language Generation (NLG) systems. While RAG can enhance factual accuracy, its effect on the social biases inherent in LLMs is not well understood. This paper systematically investigates how RAG modulates social biases across three languages (English, Japanese, and Chinese) and four categories (gender, race, age, and religion). By evaluating various generator LLMs on the BBQ benchmark, we analyse how document collections with controlled stereotypical content affect RAG outputs. We find that biases present in the retrieved documents are often significantly amplified in the generated texts, even when the base LLM itself has a low-level of intrinsic bias. These findings raise concerns about the social fairness of RAG systems, underscoring the urgent need for careful bias evaluation before real-world deployment.",Tianhui Zhang; Yi Zhou; Danushka Bollegala,Tianhui Zhang,Oral,In-person,SALLE  LE RIAD,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)","Trustworthy, Safety, Privacy & Fairness",rag; biases; social; effect; bias; gender race age; nlg; evaluating effect; language generation; systematically investigates
789-MAIN,Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions,"Existing challenges in misinformation exposure and susceptibility vary across demographics, as some populations are more vulnerable to misinformation than others. Large language models (LLMs) introduce new dimensions to these challenges through their ability to generate persuasive content at scale and reinforcing existing biases. This study investigates the bidirectional persuasion dynamics between LLMs and humans when exposed to misinformative content. We analyze human-to-LLM influence using human-stance datasets and assess LLM-to-human influence by generating LLM-based persuasive arguments. Additionally, we use a multi-agent LLM framework to analyze the spread of misinformation under persuasion among demographic-oriented LLM agents. Our findings show that demographic factors influence LLM susceptibility, with up to 15 percentage point differences in correctness across groups. Multi-agent LLMs also exhibit echo chamber behavior, aligning with human-like group polarization patterns. Therefore, this work highlights demographic divides in misinformation dynamics and offers insights for future interventions.",Angana Borah; Rada Mihalcea; Veronica Perez-Rosas,Angana Borah,Oral,In-person,SALLE  LE LIXUS,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Reasoning, Planning & Agents",,misinformation; demographic; persuasion; llm; dynamics; influence; human; susceptibility; persuasive; human llm
791-MAIN,Entropy-Gated Branching for Efficient Test-Time Reasoning,"Test-time compute methods can significantly improve the reasoning capabilities and problem-solving accuracy of large language models. However, these approaches require substantially more computational resources, with most computation wasted on exploring low-diversity branches where the model already exhibits high confidence. We observe that a small subset of uncertain reasoning steps has a disproportionately large impact on final prediction accuracy, and branching at these points tends to yield higher-quality and more diverse candidate reasoning steps. Therefore, we introduce EntropyвЂ‘Gated Branching: a novel inference technique that dynamically allocates computational resources by selectively expanding prediction sequences only at points of high uncertainty. Our method leverages entropy as a gating mechanism to identify when branching is most beneficial, coupled with an external feedback model to rank and prune candidate branches. Empirical results on mathematical and financial reasoning benchmarks show that this strategy improves accuracy by 22.6% over standard inference while operating 31%-75% faster across math benchmarks than test-time beam search with higher performance. Our results show that dynamic resource allocation during inference can substantially improve both efficiency and effectiveness, offering a more scalable pathway to enhanced LLM reasoning capabilities.",Xianzhi Li; Ethan Callanan; Abdellah Ghassel; Xiaodan Zhu,Xianzhi Li,Oral,In-person,SALLE  LE LIXUS,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",branching; test time; reasoning; gated; branches; computational resources; reasoning steps; test; entropy; candidate
793-MAIN,Decomposition-Enhanced Training for Post-Hoc Attributions in Language Models,"Large language models (LLMs) are increasingly used for long-document question answering, where reliable attribution to sources is critical for trust. Existing post-hoc attribution methods work well for extractive QA but struggle in multi-hop, abstractive, and semi-extractive settings, where answers synthesize information across passages. To address these challenges, we argue that post-hoc attribution can be reframed as a reasoning problem, where answers are decomposed into constituent units, each tied to specific context. We first show that prompting models to generate such decompositions alongside attributions improves performance. Building on this, we introduce DecompTune, a post-training method that teaches models to produce answer decompositions as intermediate reasoning steps. We curate a diverse dataset of complex QA tasks, annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and 14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards. Across extensive experiments and ablations, DecompTune substantially improves attribution quality, outperforming prior methods and matching or exceeding state-of-the-art frontier models.",Sriram Balasubramanian; Samyadeep Basu; Koustava Goswami; Ryan A. Rossi; Varun Manjunatha; Roshan Santhosh; Ruiyi Zhang; Soheil Feizi; Nedim Lipka,Sriram Balasubramanian,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Interpretability & Model Analysis,"Reasoning, Planning & Agents",post; attribution; post hoc; hoc; extractive; answers; rewards extensive experiments; curate diverse; outperforming prior methods; struggle multi
794-MAIN,INSURE-Dial: A Phase-Aware Conversational Dataset & Benchmark for Compliance Verification and Phase Detection,"Administrative phone tasks drain roughly $1 trillion annually from U.S. healthcare, with over 500 million insuranceвЂ“benefit verification calls manually handled in 2024. We introduce INSURE-Dial, to our knowledge the first public benchmark for developing and assessing compliance-aware voice agents. The corpus includes 50 de-identified, AI-initiated calls with live insurance representatives (mean 71 turns/call) and 1,000 synthetically generated calls that mirror the same workflow. All calls are annotated with a phase-structured JSON schema covering IVR navigation, patient identification, coverage status, medication checks (up to two drugs), and agent identification (CRN), and each phase is labeled for Information and Procedural compliance under explicit ask/answer logic. We define two novel evaluation tasks: (1) Phase Boundary Detection (span segmentation under phase-specific acceptance rules) and (2) Compliance Verification (IC/PC decisions given fixed spans). Per-phase scores are strong across small, low-latency baselines, but end-to-end reliability is constrained by span-boundary errors. On real calls, full-call exact segmentation is low, underscoring a gap between conversational fluency and audit-grade evidence. We include an anonymized package (data, schema, prompts, scoring) and reusable scripts for any phase-annotated call log.",Shubham Kulkarni; Alexander Lyzhov; Preetam Joshi; Shiva Chaitanya,Shubham Kulkarni,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Domain NLP (Biomedical/Clinical/Legal/Scientific),phase; calls; compliance; compliance verification; verification; span; boundary; segmentation; schema; identification
795-MAIN,Persuasion Tokens for Editing Factual Knowledge in LLMs,"In-context knowledge editing (IKE) is a promising technique for updating Large Language Models (LLMs) with new information. However, IKE relies on lengthy, fact-specific demonstrations which are costly to create and consume significant context window space. In this paper, we introduce persuasion tokens (P-Tokens) -- special tokens trained to replicate the effect of IKE demonstrations, enabling efficient knowledge editing without requiring fact-specific demonstrations. We evaluate P-Tokens across two editing datasets and three LLMs, demonstrating performance comparable to, and often exceeding, IKE. We further find that editing performance is robust to distractors with some negative effects to neighboring facts, and that increasing the number of P-Tokens improves performance. Our work addresses key limitations of IKE and provides a more practical and scalable alternative for editing LLMs.",Paul Youssef; JГ¶rg SchlГ¶tterer; Christin Seifert,Paul Youssef,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)",,editing; tokens; demonstrations; persuasion; knowledge editing; fact; knowledge; llms; special; special tokens
797-MAIN,"NLP for Social Good: A Survey and Outlook of Challenges, Opportunities and Responsible Deployment","Natural language processing (NLP) now shapes many aspects of our world, yet its social benefits are underexplored. This paper surveys work in NLP for Social Good (NLP4SG) across nine domains relevant to global development and risk agendas, summarizing principal tasks and challenges. We also analyze ACL Anthology trends, finding that inclusion and AI harms attract the most research, while domains such as poverty, peacebuilding, and environmental protection remain underexplored. Guided by our review, we outline opportunities for responsible and equitable NLP and conclude with a call for cross-disciplinary partnerships and human-centered approaches to ensure that future NLP advances the public good.",Antonia Karamolegkou; Angana Borah; Eunjung Cho; Sagnik Ray Choudhury; Martina Galletti; Pranav Gupta; Oana Ignat; Priyanka Kargupta; Neema Kotonya; Hemank Lamba; Sun-Joo Lee; Arushi Mangla; Ishani Mondal; Fatima Zahra Moudakir; Deniz Nazarova; Poli Nemkova; Dina Pisarevskaya; Naquee Rizwan; Nazanin Sabri; Keenan Samway; Dominik Stammbach; Anna Steinberg; David TomГЎs; Steven R Wilson; Jessica H Zhu; Arkaitz Zubiaga; Anders SГёgaard; Alexander Fraser; Zhijing Jin; Rada Mihalcea; Joel R. Tetreault; Daryna Dementieva,Daryna Dementieva,Oral,In-person,SALLE  WALILI,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Dialogue, Conversational & Interactive NLP",,nlp; good; nlp social; responsible; opportunities; social; underexplored; approaches ensure; responsible deployment; summarizing
799-MAIN,From Delegates to Trustees: How Optimizing for Long-Term Interests Shapes Bias and Alignment in LLMs,"Large language models (LLMs) have shown promising accuracy in predicting survey responses and policy preferences, which has increased interest in their potential to represent human interests in various domains. Most existing research has focused on ""behavioral cloning"", effectively evaluating how well models reproduce individualsвЂ™ expressed preferences. Drawing on theories of political representation, we highlight an underexplored design trade-off: whether AI systems should act as ""delegates"", mirroring expressed preferences, or as ""trustees"", exercising judgment about what best serves an individualвЂ™s interests. This trade-off is closely related to issues of LLM sycophancy, where models can encourage behavior or validate beliefs that may be aligned with a user's short-term preferences, but is detrimental to their long-term interests. Through a series of experiments simulating votes on various policy issues in the U.S. context, we apply a temporal utility framework that weighs short and long-term interests (simulating a trustee role) and compare voting outcomes to behavior-cloning models (simulating a delegate). We find that trustee-style predictions weighted toward long-term interests produce policy decisions that align more closely with expert consensus on well-understood issues, but also show greater bias toward modelsвЂ™ default stances on topics lacking clear agreement. These findings reveal a fundamental trade-off in designing AI systems to represent human interests. Delegate models better preserve user autonomy but may diverge from well-supported policy positions, while trustee models can promote welfare on well-understood issues yet risk paternalism and bias on subjective topics.",Suyash Fulay; Jocelyn Zhu; Michiel A. Bakker,Suyash Fulay,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Efficiency, Scaling & NLP Systems",interests; term; long term; policy; preferences; simulating; issues; long; delegates; trade
802-MAIN,Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection,"Multilingual Large Language Models (LLMs) offer powerful capabilities for cross-lingual fact-checking. However, these models often exhibit language bias, performing disproportionately better on high-resource languages such as English than on low-resource counterparts. We also present and inspect a novel concept - retrieval bias, when information retrieval systems tend to favor certain information over others, leaving the retrieval process skewed. In this paper, we study language and retrieval bias in the context of Previously Fact-Checked Claim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20 languages using a fully multilingual prompting strategy, leveraging the AMC-16K dataset. By translating task prompts into each language, we uncover disparities in monolingual and cross-lingual performance and identify key trends based on model family, size, and prompting strategy. Our findings highlight persistent bias in LLM behavior and offer recommendations for improving equity in multilingual fact-checking. To investigate retrieval bias, we employed multilingual embedding models and look into the frequency of retrieved claims. Our analysis reveals that certain claims are retrieved disproportionately across different posts, leading to inflated retrieval performance for popular claims while under-representing less common ones.",Ivan Vykopal; Antonia Karamolegkou; Jaroslav KopДЌan; Qiwei Peng; TomГЎЕЎ JavЕЇrek; Michal Gregor; Marian Simko,Ivan Vykopal,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multilinguality & Low-Resource NLP,"Retrieval, Grounding & External Knowledge (RAG)",bias; retrieval; multilingual; fact; claims; language retrieval; claim detection; prompting strategy; disproportionately; fact checking
807-MAIN,FFE-Hallu: Hallucinations in Fixed Figurative Expressions: A Benchmark of Idioms and Proverbs in the Persian Language,"Figurative language, especially fixed figurative expressions (FFEs) such as idioms and proverbs, poses unique challenges for large language models (LLMs). Unlike literal phrases, FFEs are culturally grounded and often non-compositional, making them vulnerable to figurative hallucination, the generation or acceptance of plausible-sounding but culturally invalid expressions. We introduce \textbf{FFE-Hallu}, the first comprehensive benchmark for evaluating LLMsвЂ™ ability to generate, detect, and translate FFEs in Persian, a linguistically rich but underrepresented language. FFE-Hallu includes 600 carefully curated examples spanning three tasks: FFE generation from meaning, detection of fabricated FFEs (across four controlled categories), and FFE-to-FFE translation from English to Persian. Our evaluation of six state-of-the-art multilingual LLMs reveals persistent weaknesses in both cultural grounding and figurative competence. While models like GPT-4.1 display relative strength in rejecting fabricated FFEs and retrieving authentic ones, most systems struggle to reliably distinguish real FFEs from high-quality fabrications and often hallucinate in translation. This work shows that LLMs still have important gaps in understanding and using figurative language, and that specialized benchmarks like FFE-Hallu are needed.",Faezeh Hosseini; Mohammadali Yousefzadeh; Yadollah Yaghoobzadeh,Faezeh Hosseini,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics",Machine Translation,figurative; persian; expressions; proverbs; figurative expressions; figurative language; idioms; fixed; culturally; translation
808-MAIN,MEVER: Multi-Modal and Explainable Claim Verification with Graph-based Evidence Retrieval,"Verifying the truthfulness of claims usually requires joint multi-modal reasoning over both textual and visual evidence, such as analyzing both textual caption and chart image for claim verification. In addition, to make the reasoning process transparent, a textual explanation is necessary to justify the verification result. However, most claim verification works mainly focus on the reasoning over textual evidence only or ignore the explainability, resulting in inaccurate and unconvincing verification. To address this problem, we propose a novel model that jointly achieves evidence retrieval, multi-modal claim verification, and explanation generation. For evidence retrieval, we construct a two-layer multi-modal graph for claims and evidence, where we design image-to-text and text-to-image reasoning for multi-modal retrieval. For claim verification, we propose token- and evidence-level fusion to integrate claim and evidence embeddings for multi-modal verification. For explanation generation, we introduce multi-modal Fusion-in-Decoder for explainability. Finally, since almost all the datasets are in general domain, we create a scientific dataset, AIChartClaim, in AI domain to complement claim verification community. Experiments show the strength of our model. Code and data are submitted. We will release them upon publication.",Delvin Ce Zhang; Suhan Cui; Zhelin Chu; Xianren Zhang; Dongwon Lee,Delvin Ce Zhang,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,verification; claim; multi modal; claim verification; modal; evidence; evidence retrieval; multi; textual; explanation
811-MAIN,DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding,"Arabic calligraphy represents one of the richest visual traditions of the Arabic language, blending linguistic meaning with artistic form. Although multimodal models have advanced across languages, their ability to process Arabic script, especially in artistic and stylized calligraphic forms, remains largely unexplored. To address this gap, we present DuwatBench, a benchmark of 1,050 curated samples containing about 1,400 words across 6 classical and modern calligraphic styles, each paired with sentence-level detection annotations. The dataset reflects real-world challenges in Arabic writing, such as complex stroke patterns, dense ligatures, and stylistic variations that often challenge standard text recognition systems. Using DuwatBench, we evaluated 13 leading Arabic and multilingual multimodal models and show that while they perform well in clean text, they struggle with calligraphic variation, artistic distortions, and precise visualвЂ“text alignment. By publicly releasing DuwatBench and its annotations, we aim to advance culturally grounded multimodal research, foster fair inclusion of Arabic language and visual heritage in AI systems, and support continued progress in this area. Our benchmark and the evaluation suit will be publicly available.",Shubham Patle; Sara Ghaboura; Hania Tariq; Mohammad Usman Khan; Omkar Thawakar; Rao Muhammad Anwer; Salman Khan,Shubham Patle,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,arabic; artistic; multimodal; arabic language; language visual; heritage; multimodal models; visual; publicly; text
815-MAIN,ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders,"The promise of LLM-based user simulators to improve conversational AI is hindered by a critical ""realism gap,"" leading to systems that are optimized for simulated interactions, but may fail in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to better understand this gap. Its unique dual-agent data collection protocol, using both ""good"" and ""bad"" recommenders, enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven methods consistently outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, indicating a more robust, if imperfect, user model.",Ofer Meshi; Krisztian Balog; Sally Goldman; Avi Caciularu; Guy Tennenholtz; Jihwan Jeong; Amir Globerson; Craig Boutilier,Ofer Meshi,Oral,In-person,SALLE  WALILI,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Dialogue, Conversational & Interactive NLP","LLM Evaluation, Benchmarks & Metrics",validation; counterfactual; user; realism; gap; conversational; driven methods; framework shows; statistical alignment; user satisfaction
816-MAIN,Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark,This paper presents a novel task of extracting Latin fragments from mixed-language historical documents with varied layouts. We benchmark and evaluate the performance of large foundation models against a multimodal dataset of 724 annotated pages. The results demonstrate that reliable Latin detection with contemporary models is achievable. Our study provides the first comprehensive analysis of these models' capabilities and limits for this task.,Yu Wu; Ke Shu; Jonas Fischer; Lidia Pivovarova; David Rosson; Eetu MГ¤kelГ¤; Mikko Tolonen,Yu Wu,Oral,In-person,SALLE  LA PALMERAIE,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Multimodal & Speech/Audio,Interpretability & Model Analysis,latin; models multimodal; historical; multimodal; fragments; achievable; multimodal benchmark; mixed language; provides comprehensive analysis; contemporary models
818-MAIN,"Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions","Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.",Pedro Henrique Luz de Araujo; Michael A. Hedderich; Ali Modarressi; Hinrich Schuetze; Benjamin Roth,Pedro Henrique Luz de Araujo,Oral,In-person,SALLE  LE RIAD,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Dialogue, Conversational & Interactive NLP","Trustworthy, Safety, Privacy & Fairness",persona; fidelity; persona fidelity; instruction following; following; dialogues; instruction; instruction following safety; following safety; assigned
819-MAIN,CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models,"With their growing capabilities, generative large language models (LLMs) are being increasingly investigated for complex medical tasks. However, their effectiveness in real-world clinical applications remains underexplored. To address this, we present CliniBench, the first benchmark that enables comparability of well-studied encoder-based classifiers and generative LLMs for discharge diagnosis prediction from admission notes in MIMIC-IV dataset. Our extensive study compares 12 generative LLMs and 3 encoder-based classifiers and demonstrates that encoder-based classifiers consistently outperform generative models in diagnosis prediction. We assess several retrieval augmentation strategies for in-context learning from similar patients and find that they provide notable performance improvements for generative LLMs.",Paul Grundmann; Dennis Fast; Jan Frick; Thomas Steffek; Felix Gers; Wolfgang Nejdl; Alexander LГ¶ser,Jan Frick,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Retrieval, Grounding & External Knowledge (RAG)",generative; encoder based; encoder; classifiers; prediction; diagnosis; clinical; based; llms; assess retrieval
827-MAIN,Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models,"Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often misalign with linguistic structure, amplify bias, and waste capacity across languages and domains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.",Sawsan Alqahtani; Mir Tafseer Nayeem; Md Tahmid Rahman Laskar; Tasnim Mohiuddin; M Saiful Bari,Sawsan Alqahtani,Oral,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics","LLM Evaluation, Benchmarks & Metrics",tokenization; core; design; pair encoding bpe; pair encoding; language models tokenization; byte pair; transparent reporting; byte pair encoding; stop
828-MAIN,DIVINE : Coordinating Multimodal Disentangled Representations for Oro-Facial Neurological Disorder Assessment,"In this study, we present a multimodal framework for predicting neuro-facial disorders by capturing both vocal and facial cues. We hypothesize that explicitly disentangling shared and modality-specific representations within multimodal foundation model embeddings can enhance clinical interpretability and generalization. To validate this hypothesis, we propose DIVINE (DIsentangled Variational INformation NEtwork), a fully disentangled multimodal framework that operates on representations extracted from state-of-the-art (SOTA) audio and video foundation models, incorporating hierarchical variational bottlenecks, sparse gated fusion, and learnable symptom tokens. DIVINE operates in a multitask learning setup to jointly predict diagnostic categories (Healthy Control, ALS, Stroke) and severity levels (Mild, Moderate, Severe). The model is trained using synchronized audio and video inputs and evaluated on the Toronto NeuroFace dataset under full (audio-video) as well as single-modality (audio-only and video-only) test conditions. Our proposed approach achieves SOTA results, with the DeepSeek-VL2 and TRILLsson combination reaching 98.26% accuracy and 97.51% F1-score. Under modality-constrained scenarios, the framework performs well, showing strong generalization when tested with video-only or audio-only inputs. It consistently yields superior performance compared to unimodal models and baseline fusion techniques. To the best of our knowledge, this is the first framework that combines cross-modal disentanglement, adaptive fusion, and multitask learning to comprehensively assess neurological disorders using synchronized speech and facial video. Code and model weights will be released upon the completion of the double-blind review process.",Mohd Mujtaba Akhtar; Girish; Muskaan Singh,Girish,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multimodal & Speech/Audio,,video; audio video; audio; fusion; modality; multimodal; variational; multitask; multimodal framework; disorders
829-MAIN,Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible,"Are large language models (LLMs) sensitive to the distinction between humanly possible languages and humanly impossible languages? This question is taken by many to bear on whether LLMs and humans share the same innate learning biases. Previous work has attempted to answer it in the positive by comparing LLM learning curves on existing language datasets and on ""impossible"" datasets derived from them via various perturbation functions. Using the same methodology, we examine this claim on a wider set of languages and impossible perturbations. We find that in most cases, GPT-2 learns each language and its impossible counterpart equally easily, in contrast to previous claims. We also apply a more lenient condition by testing whether GPT-2 provides any kind of separation between the whole set of natural languages and the whole set of impossible languages. By considering cross-linguistic variance in various metrics computed on the perplexity curves, we show that GPT-2 provides no systematic separation between the possible and the impossible. Taken together, these perspectives show that LLMs do not share the human innate biases that shape linguistic typology.",Imry Ziv; Nur Lan; Emmanuel Chemla; Roni Katzir,Imry Ziv,Oral,In-person,SALLE  LE LIXUS,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multilinguality & Low-Resource NLP,"Linguistics, Syntax & Semantics",impossible; possible; languages; innate; taken; gpt; separation; share; set; biases
831-MAIN,Bridging Attribution and Open-Set Detection using Graph-Augmented Instance Learning in Synthetic Speech,"We propose a unified framework for not only attributing synthetic speech to its source but also for detecting speech generated by synthe sizers that were not encountered during training. This requires methods that move beyond simple detection to support both detailed foren- sic analysis and open-set generalization. To address this, we introduce SIGNAL, a hybrid framework that combines speech foundation models (SFMs) with graph-based modeling and open-set-aware inference. Our framework integrates Graph Neural Networks (GNNs) and a k-Nearest Neighbor (KNN) classifier, allowing it to capture meaningful relationships between utterances and recognize speech that doesnвЂ™t belong to any known generator. It builds a dynamic graph where utterances are connected based on how closely their learned representations align, enabling the GNN to capture subtle relationships across samples. Meanwhile, the KNN branch handles open-set detection through confidence-based thresholding. We evaluate SIGNAL using the DiffSSD dataset, which offers a diverse mix of real speech and synthetic audio from both open-source and commercial diffusion-based TTS systems. To further assess generalization, we also test on the SingFake benchmark. Our results show that SIGNAL consistently improves performance across both tasks, with Mamba based embeddings delivering especially strong results. To the best of our knowledge, this is the first study to unify graph-based learning and open-set detection for tracing synthetic speech back to its origin.",Mohd Mujtaba Akhtar; Girish; Farhan Sheth; Muskaan Singh,Girish,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Multimodal & Speech/Audio,Interpretability & Model Analysis,open set; speech; synthetic speech; graph; open; set; signal; synthetic; detection; knn
837-MAIN,Detecting Non-Membership in LLM Training Data via Rank Correlations,"As large language models (LLMs) are trained on increasingly vast and opaque text corpora, determining which data contributed to training has become essential for copyright enforcement, compliance auditing, and user trust. While prior work focuses on detecting whether a dataset was used in training (membership inference), the complementary problemвЂ”verifying that a dataset was not used- has received little attention. We address this gap by introducing PRISM, a test that detects dataset-level non-membership using only grey-box access to model logits. Our key insight is that two models that have not seen a dataset exhibit higher rank correlation in their normalized token log probabilities than when one model has been trained on that data. Using this observation, we construct a correlation-based test that detects non-membership. Empirically, PRISM reliably rules out membership in training data across all datasets tested while avoiding false positives, thus offering a framework for verifying that specific datasets were excluded from LLM training.",Pranav Shetty; Mirazul Haque; Zhiqiang Ma; Xiaomo Liu,Pranav Shetty,Oral,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",Interpretability & Model Analysis,membership; dataset used; prism; training; detects; llm training; verifying; non; dataset; correlation
840-MAIN,Taming Object Hallucinations with Verified Atomic Confidence Estimation,"Multimodal Large Language Models (MLLMs) often suffer from hallucinations, particularly errors in object existence, attributes, or relations, which undermine their reliability. We introduce TACO (Verified Atomic Confidence Estimation), a simple framework that mitigates hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO decomposes responses into atomic queries, paraphrases them to reduce sensitivity to wording, and estimates confidence using self-consistency (black-box) or self-confidence (gray-box) aggregation, before refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (LLaVA-1.5-7B and CogVLM2) show that TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases, and improves confidence calibration, demonstrating its effectiveness in enhancing the faithfulness of MLLMs.",Jiarui Liu; Weihao Xuan; Zhijing Jin; Mona T. Diab,Jiarui Liu,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",,confidence; atomic; mllms; confidence estimation; confidence calibration; hallucinations; self; object; verified; box
842-MAIN,DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning,"Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the distribution of expert tool calls to ensure that every tool is being reliably used to help resolve disagreement.",Nithin Sivakumaran; Justin Chen; David Wan; Yue Zhang; Jaehong Yoon; Elias Stengel-Eskin; Mohit Bansal,Nithin Sivakumaran,Oral,In-person,SALLE  LE RIAD,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Reasoning, Planning & Agents",Multimodal & Speech/Audio,agent; multi agent; tool; tools; discussion; multi; disagreement; multi agent debate; agent debate; spatial reasoning
851-MAIN,ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers,"Tool calling has become increasingly popular for Large Language Models (LLMs). However, for large tool sets, the resulting tokens would exceed the LLM's context window limit, making it impossible to include every tool. Hence, an external retriever is used to provide LLMs with the most relevant tools for a query. Existing retrieval models rank tools based on the similarity between a user query and a tool description (TD). This leads to suboptimal retrieval as user requests are often poorly aligned with the language of TD. To remedy the issue, we propose ToolDreamer, a framework to condition retriever models to fetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e., description of tools that the LLM feels will be potentially useful for the query. The framework enables a more natural alignment between queries and tools within the language space of TD's. We apply ToolDreamer on the ToolRet dataset and show that our method improves the performance of sparse and dense retrievers with and without training, thus showcasing its flexibility. Through our proposed framework, our aim is to offload a portion of the reasoning burden to the retriever so that the LLM may effectively handle a large collection of tools without inundating its context window.",Saptarshi Sengupta; Zhengyu Zhou; Jun Araki; Xingbo Wang; Bingqing Wang; Suhang Wang; Zhe Feng,Zhengyu Zhou,Oral,In-person,Pavillon  DE RABAT,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents","Retrieval, Grounding & External Knowledge (RAG)",tools; tool; retriever; tools based; description; context window; query; window; retrievers; llm
855-MAIN,An Empirical Study of Speculative Decoding for Small Language Models,"Speculative decoding has emerged as a promising approach to accelerate Large Language Model inference. However, existing research has predominantly focused on 7B-70B parameters models, leaving a critical knowledge gap for small language models (1-2B parameters) that are increasingly important for edge computing and agentic AI systems. This paper presents the first comprehensive empirical study of speculative decoding techniques for small language models. We evaluate five distinct method categories across three representative model families and reveal that drafting overhead, rather than draft quality, becomes the primary bottleneck fundamentally limiting acceleration of small models. We demonstrate that traditional independent drafting fails completely due to the suboptimal architecture of available drafters, while self-drafting methods achieve meaningful acceleration only when employing sufficiently efficient draft modules. In contrast, retrieval-based methods with negligible computational overhead yield consistent gains. Based on these insights, we establish practical guidelines for effective small model acceleration.",Luca Mainardi; Selcuk Sandikci; Joaquin Vanschoren,Luca Mainardi,Oral,In-person,SALLE  LE LIXUS,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Efficiency, Scaling & NLP Systems","Retrieval, Grounding & External Knowledge (RAG)",acceleration; drafting; small; speculative; speculative decoding; small language models; small language; decoding; empirical study; draft
856-MAIN,Lost in Formatting: How Output Formats Skew LLM Performance on Information Extraction,"We investigate how the choice of output format influences the fine-tuning performance of large language models on sequence labelling- based information extraction tasks. Across more than 280 experiments spanning multiple benchmarks, model families, and formats, we find that the output format is a critical yet often overlooked hyperparameter. In some cases, the choice of format alone can shift F1 scores by over 40% on the same dataset and model. Further, we observe that no single format consistently dominates across settingsвЂ”the optimal choice depends on factors like model family, size and dataset characteristics. Overall, these results demonstrate that even informationally equivalent output formats can produce substantial performance variation, highlighting the need to treat output formatting as a key factor in building robust and reliable information extraction systems.",Rishi Ravikumar; Nuhu Ibrahim; Riza Batista-Navarro,Rishi Ravikumar,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Information Extraction & Structured Prediction,"Efficiency, Scaling & NLP Systems",output; format; information extraction; formats; choice; output format; formatting; output formats; extraction; information
859-MAIN,Pseudo-Likelihood Training for Reasoning Diffusion Language Models,"Policy-gradient reinforcement learning (PGRL) form the backbone of current methods used to improve language model reasoning. However, these methods are incompatible with diffusion based language models (dLLMs). Most attempts to apply PGRL to dLLMs, either are extremely unscalable or use unprincipled approximations. Our proposed framework uses a novel pseudo-likelihood based objective for alignment of dLLMs. Our objective has the same optima as PGRL based optimization, but does not need to evaluate exact likelihood from dLLMs, thus providing a principled and practical alternative for fine-tuning of dLLMs. Additionally other than the pseudo-likelihood based objective, we present two computationally efficient approximations to allow for scalable training. Experiments on multiple reasoning and coding benchmarks show that our proposals match or surpasses the performance of GRPO and related baselines.",Shiv Shankar,Shiv Shankar,Oral,In-person,SALLE  LE RIAD,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents","Trustworthy, Safety, Privacy & Fairness",likelihood; pseudo; approximations; based objective; objective; diffusion; based; reasoning; alternative fine tuning; training reasoning
860-MAIN,RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators without Human Test Sets,"LLMs are powerful generators of synthetic data, which are used for training smaller, specific models. This is especially valuable for low-resource languages, where human-labelled data is scarce but LLMs can still produce high-quality text. However, LLMs differ in how useful their outputs are for training. Selecting the best LLM as a generator is challenging because extrinsic evaluation requires costly human annotations (which are often unavailable for low-resource languages), while intrinsic metrics correlate poorly with downstream performance. We introduce Round-robin Synthetic data Evaluation (RoSE), a proxy metric for selecting the best LLM generator without human test sets. RoSE trains a small model on the outputs of a candidate generator (LLM) and then evaluates it on generated synthetic examples from all other candidate LLMs. The final RoSE score is the mean performance of this small model. Across six LLMs, eleven languages, and three tasks (sentiment, topic, intent), RoSE identifies the optimal generator more often than any other intrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within 0.76 percentage points of the optimal generator baseline. This result is measured in terms of downstream performance, obtained by training a small model on the chosen generatorвЂ™s outputs (optimal vs. proxy-metricвЂ“selected) and evaluating it on human-labelled test data. Additionally, RoSE is the only metric to achieve a positive correlation with performance on human test data.",Jan Cegin; Branislav Pecher; Ivan Srba; Jakub Simko,Jan Cegin,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",Multilinguality & Low-Resource NLP,generator; small model; selecting; intrinsic; synthetic; human; test; synthetic data; optimal; human labelled
867-MAIN,RotBench: Evaluating Multi-modal Large Language Models on Identifying Image Rotation,"We investigate to what extent Multi-modal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0В°, 90В°, 180В°, and 270В°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprised of lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show several state-of-the-art open and proprietary MLLMs do not reliably identify rotation in input images. Providing models with additional information -- including captions, segmentation maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate most models are able to reliably identify right-side up (0В°) images, while certain models are able to identify up-side down (180В°) images. None can reliably distinguish between 90В° and 270В°. Simultaneously showing the image rotated in different orientations leads to moderate performance gains in reasoning models, while a modified setup using voting improves performance in weaker models. We further show fine-tuning does not improve models' ability to distinguish 90В° and 270В°, despite substantially improving identification of 180В° images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.",Tianyi Niu; Jaemin Cho; Elias Stengel-Eskin; Mohit Bansal,Tianyi Niu,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents",Multimodal & Speech/Audio,images; mllms; identify; orientation; reliably; multi modal large; modal large; reliably identify; modal large language; image
870-MAIN,Multilingual Amnesia: On the Transferability of Unlearning in Multilingual LLMs,"As multilingual large language models become more widely used, ensuring their safety and fairness across diverse linguistic contexts presents unique challenges. While existing research on machine unlearning has mainly focused on monolingual settings, typically English, multilingual environments introduce additional complexities due to cross-lingual knowledge transfer and biases embedded in both pretraining and fine-tuning data. In this work, we address the problem of multilingual unlearning using the Aya-Expanse 8B model under two settings: (1) data unlearning and (2) concept unlearning. We extend benchmarks for factual knowledge and stereotypes into ten languages through translationвЂ”English, French, Arabic, Japanese, Russian, Farsi, Korean, Hindi, Hebrew, and IndonesianвЂ”spanning five language families and varying resource levels. Our experiments show that unlearning in high-resource languages tends to be more stable, with asymmetric transfer observed between typologically related languages. Moreover, analysis of linguistic distances reveals that syntactic similarity is the most predictive factor of cross-lingual unlearning effects.",Alireza Dehghanpour Farashah; Aditi Khandelwal; Marylou Fauchard; Zhuan Shi; Negar Rostamzadeh; Golnoosh Farnadi,Alireza Dehghanpour Farashah,Oral,Virtual,ZOOM,Virtual TBA,,,Multilinguality & Low-Resource NLP,"Trustworthy, Safety, Privacy & Fairness",unlearning; multilingual; languages; cross lingual; lingual; transfer; pretraining fine tuning; lingual knowledge transfer; unlearning using; safety fairness
872-MAIN,Beyond Math: Stories as a Testbed for Memorization-Constrained Reasoning in LLMs,"Memorization has been shown to greatly inflate Large Language ModelsвЂ™ (LLMs) performance on domains such as math and logic, where success should primarily rely on applying generalizable reasoning rules. In many real-world applications, however, memorization is not meant to be eliminated but selectively constrainedвЂ”for example, in story understanding, where background knowledge must be integrated with narrative context. Drawing on the cognitive science distinction between вЂњverbatimвЂќ (exact recall) and вЂњgistвЂќ (semantic abstraction) memorization, we propose a two-tier framework for analyzing how LLMs reason under different degrees of memory access. The Inductive (prompt-guided) Setting softly steers models to reason through selective, context-relevant recall, while the Restrictive Setting imposes stronger constraints by limiting verbatim memory access. Evaluating GPT-4o, LLaMA3.3-70B, and DeepSeek V3 on six character-centric story understanding benchmarks, we find up to a 45.2% accuracy drop under the Restrictive Setting, revealing strong dependence on surface recall. By contrast, the Inductive Setting maintains performance, indicating that prompting can align LLMs toward memorization-constrained reasoning.",Yuxuan Jiang; Francis Ferraro,Yuxuan Jiang,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Efficiency, Scaling & NLP Systems",memorization; setting; recall; inductive; story; access; math; reason; constrained; memory
873-MAIN,Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis,"Membership inference attacks (MIAs) reveal whether specific data was used to train machine learning models, serving as important tools for privacy auditing and compliance assessment. Recent studies have reported that MIAs perform only marginally better than random guessing against large language models, suggesting that modern pre-training approaches with massive datasets may be free from privacy leakage risks. Our work offers a complementary perspective to these findings by exploring how examining LLMs' internal representations, rather than just their outputs, may provide additional insights into potential membership inference signals. Our framework, \emph{memTrace}, follows what we call \enquote{neural breadcrumbs} extracting informative signals from transformer hidden states and attention patterns as they process candidate sequences. By analyzing layer-wise representation dynamics, attention distribution characteristics, and cross-layer transition patterns, we detect potential memorization fingerprints that traditional loss-based approaches may not capture. This approach yields strong membership detection across several model families achieving average AUC scores of 0.85 on popular MIA benchmarks. Our findings suggest that internal model behaviors can reveal aspects of training data exposure even when output-based signals appear protected, highlighting the need for further research into membership privacy and the development of more robust privacy-preserving training techniques for large language models.",Disha Makhija; Manoj Ghuhan Arivazhagan; Vinayshekhar Bannihatti Kumar; Rashmi Gangadharaiah,Disha Makhija,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,membership; privacy; membership inference; mias; signals; inference attacks; membership inference attacks; attention; inference; hidden
877-MAIN,Chat-TS: Enhancing Multi-Modal Reasoning Over Time-Series and Natural Language Data,"Large language models are being rapidly applied across many fields such as healthcare, finance, transportation, and energy, among many others. These applications often involve analyzing time-series data alongside contextual information in the form of natural language to support informed decisions. However, current time-series models are limited in their ability to perform reasoning that involves both time-series and their textual content. In this work, we address this gap by introducing Chat-TS, a large language model (LLM) based framework designed to support reasoning over time series and textual data. Unlike traditional models, Chat-TS integrates time-series tokens into LLMs' vocabulary, enhancing its reasoning ability over both modalities without compromising core natural language capabilities. To support learning and evaluation, we contribute new datasets: the TS Instruct Training Dataset (pairing diverse time-series data with relevant text instructions and responses for instruction tuning), the TS Instruct Question and Answer (QA) Gold Dataset (multiple-choice questions to evaluate multimodal reasoning), and a TS Instruct Quantitative Probing Set (a small subset of TS Instruct QA reasoning tasks alongside math and decision-making questions for LLM evaluation). We design a training strategy to preserve the inherent reasoning capabilities of LLMs while augmenting them for time-series reasoning. Experiments show that Chat-TS achieves state-of-the-art performance in multimodal reasoning tasks by maintaining strong natural language proficiency while improving time-series reasoning.",Paul Quinlan; Qingguo Li; Xiaodan Zhu,Paul Quinlan,Oral,In-person,SALLE  LE RIAD,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Dialogue, Conversational & Interactive NLP","Reasoning, Planning & Agents",time series; series; time; chat; reasoning; instruct; natural language; time series data; series data; natural
879-MAIN,Language Family Matters: Evaluating SpeechLLMs Across Linguistic Boundaries,"Large Language Model (LLM)-powered Automatic Speech Recognition (ASR) systems achieve strong performance with limited resources by linking a frozen speech encoder to a pretrained LLM via a lightweight connector. Prior work trains a separate connector per language, overlooking linguistic relatedness. We propose a novel connector-sharing strategy based on linguistic family membership, enabling one connector per family, and empirically validate its effectiveness across two multilingual LLMs and two real-world corpora spanning curated and crowd-sourced speech. Our results show that family-based connectors reduce parameter count while improving generalization across domains, offering a practical and scalable strategy for multilingual ASR deployment.",Yuchen Zhang; Ravi Shekhar; Haralambos Mouratidis,Yuchen Zhang,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multilinguality & Low-Resource NLP,Multimodal & Speech/Audio,family; speech; asr; linguistic; strategy; practical scalable; crowd; empirically validate effectiveness; limited resources; speech encoder
881-MAIN,Beyond Names: How Grammatical Gender Markers Bias LLM-based Educational Recommendations,"This paper investigates gender biases exhibited by LLM-based virtual assistants when providing educational recommendations, focusing on minimal gender indicators. Experimenting on Italian, a language with grammatical gender, we demonstrate that simply changing noun and adjective endings (e.g., from masculine ""-o"" to feminine ""-a"") significantly shifts recommendations. More specifically, we find that LLMs i) recommend STEM disciplines less for prompts with feminine grammatical gender and ii) narrow down the set of disciplines recommended to prompts with masculine grammatical gender; these effects persist across multiple commercial LLMs (from OpenAI, Anthropic, and Google). We show that grammatical gender cues alone trigger substantial distributional shifts in educational recommendations, and up to 76% of the bias exhibited when using prompts with proper names is already present with grammatical gender markers alone.Our findings highlight the need for robust bias evaluation and mitigation strategies before deploying LLM-based virtual assistants in student-facing contexts and the risks of using general purpose LLMs for educational applications, especially in languages with grammatical gender.",Luca Benedetto; Antonia Donvito; Alberto Lucchetti; Andrea Cappelli; Paula Buttery,Luca Benedetto,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",gender; grammatical; recommendations; educational; markers; disciplines; virtual; exhibited; prompts; bias
885-MAIN,ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images,"Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.",Mathieu Sibue; AndrГ©s MuГ±oz Garza; Samuel Mensah; Pranav Shetty; Zhiqiang Ma; Xiaomo Liu; Manuela Veloso,Mathieu Sibue,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Information Extraction & Structured Prediction,"LLM Evaluation, Benchmarks & Metrics",extraction; document; structured; extraction document; structured information; generalist; types; information extraction; vqa; schema
888-MAIN,WhatвЂ™s Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning,"Despite the impressive performance of vision-language models (VLMs) on downstream tasks, their ability to understand and reason about causal relationships in visual inputs remains unclear. Robust causal reasoning is fundamental to solving complex high-level reasoning tasks, yet existing benchmarks often include a mixture of reasoning questions, and VLMs can frequently exploit object recognition and activity identification as shortcuts to arrive at the correct answers, making it challenging to truly assess their causal reasoning abilities. To bridge this gap, we introduce VQA-Causal and VCR-Causal, two new benchmarks specifically designed to isolate and rigorously evaluate VLMsвЂ™ causal reasoning abilities. Our findings reveal that while VLMs excel in object and activity recognition, they perform poorly on causal reasoning tasks, often only marginally surpassing random guessing. Further analysis suggests that this limitation stems from a severe lack of causal expressions in widely used training datasets, where causal relationships are rarely explicitly conveyed. We additionally explore fine-tuning strategies with hard negative cases, showing that targeted fine-tuning can improve model's causal reasoning while maintaining generalization and downstream performance. Our study highlights a key gap in current VLMs and lays the groundwork for future work on causal understanding. We will release the code upon acceptance.",Zhaotian Weng; Haoxuan Li; Xin Eric Wang; Kuan-Hao Huang; Jieyu Zhao,"Zhaotian Weng, Kuan-Hao Huang, Jieyu Zhao",Oral,In-person,SALLE  LE RIAD,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Interpretability & Model Analysis,"Reasoning, Planning & Agents",causal; causal reasoning; reasoning; vlms; activity; causal relationships; object; reasoning abilities; relationships; reasoning tasks
890-MAIN,When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation,"The rapid evolution of large language models (LLMs) and the real world has outpaced the static nature of widely used evaluation benchmarks, raising concerns about their reliability for evaluating LLM factuality. While substantial works continue to rely on the popular but old benchmarks, their temporal misalignment with real-world facts and modern LLMs, and their effects on LLM factuality evaluation remain underexplored. Therefore, in this work, we present a systematic investigation of this issue by examining five popular factuality benchmarks and eight LLMs released across different years. An up-to-date fact retrieval pipeline and three metrics are tailored to quantify benchmark aging and its impact on LLM factuality evaluation. Experimental results and analysis illustrate that a considerable portion of samples in the widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality. We hope our work can provide a testbed to assess the reliability of a benchmark for LLM factuality evaluation and inspire more research on the benchmark aging issue.",Xunyi Jiang; Dingyi Chang; Julian McAuley; Xin Xu,Xin Xu,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics",,factuality; benchmarks; evaluation; misalignment; llm; popular; temporal; issue; widely used; reliability
896-MAIN,On the Additive Compositionality of Task Vectors in VisionвЂ“Language Models,"In-context learning (ICL) in large language models (LLMs) has been shown to operate through task vectorsвЂ”the representation that summarizes the mapping induced by in-context demonstrations and can be composed by simple arithmetic operations. While this phenomenon is well studied in LLMs, its extension to vision-language models (VLMs) remains underexplored. In this work, we systematically examine the additive compositionality of in-context task vectors in VLMs, extracted from text-side hidden representations. Specifically, we construct compositional visual reasoning tasks with clearly defined subtasks and extract task vectors from few-shot demonstrations. Empirical experiments show that the vector for a complex task can be approximated by adding the vectors of its constituent subtasks. Beyond this, we analyze token-level contextual embeddings and show that additive composition arises because complex-task representations emerge as the superposition of atomic subtask components, preserving semantic structure within the modelвЂ™s activation space.",Yuting SHI; Houjing WEI; Naoya Inoue,Yuting Shi,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Interpretability & Model Analysis,Multimodal & Speech/Audio,additive; task vectors; vectors; compositionality; subtasks; demonstrations; context; vlms; representations; systematically examine
900-MAIN,KidsArtBench: Multi-Dimensional ChildrenвЂ™s Art Evaluation with Attribute-Aware MLLMs,"Multimodal Large Language Models (MLLMs) show remarkable progress across many visualвЂ“language tasks; however, their capacity to evaluate artistic expression remains limited: aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach -- where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric -- with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.",Mingrui Ye; Chanjin Zheng; Zengyi Yu; Chenyu Xiang; Zhixue Zhao; Zheng Yuan; Helen Yannakoudakis,Mingrui Ye,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",,attribute; attribute aware; children; multi dimensional; rubric; dimensional; aware; supervision; mllms; dimensions
902-MAIN,Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions,"Model steering, which involves intervening on hidden representations at inference time, has emerged as a lightweight alternative to fine-tuning for precisely controlling large language models. While steering efficacy has been widely studied, evaluations of specificityвЂ”whether interventions alter only the intended propertyвЂ”remain limited, especially for potential degradation in behaviors related to the target one. We propose a framework that distinguishes three dimensions of specificity: general (preserving fluency and unrelated abilities), control (preserving related control properties), and robustness (preserving control properties under distribution shifts). We use overrefusal steering as a safety-critical case study and show that while steering consistently reduces overrefusal without harming general abilities and often preserves refusal on harmful queries, it fails on robustness: interventions substantially increase jailbreak vulnerability, even when safety is explicitly controlled. Our work provides the first systematic evaluation of specificity robustness in model steering, showing that standard efficacy and specificity checks are insufficient. Without robustness evaluation, steering methods that appear safe in-distribution may in fact compromise model safety.",Navita Goyal; Hal DaumГ© III,Navita Goyal,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,steering; specificity; robustness; interventions; model steering; control; preserving; efficacy; safety; properties
904-MAIN,Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation,"Multilingual domain adaptation (ML-DA) is widely used to learn new domain knowledge across languages into large language models (LLMs). Although many methods have been proposed to improve domain adaptation, the mechanisms of multilingual knowledge acquisition, how domain knowledge is learned within a language and transferred across languages, remain underexplored. This gap leads to suboptimal performance, particularly in low-resource settings. This work examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA studies often train and evaluate on datasets with mismatched knowledge coverage, we propose \textbf{AdaXEval}, an adaptive evaluation method that builds multiple-choice QA datasets from the same bilingual domain corpus used for training, thereby directly studying multilingual knowledge acquisition. Through continual training of LLMs with diverse data recipes, we track how LLMs acquire domain facts and pinpoint the mechanism behind the transformation process from domain training data to knowledge. Our experiments on a 13B English-Japanese bilingual LLM reveal that cross-lingual transfer remains challenging despite a high-quality bilingual corpus. The code has been released.",Xin Zhao; Naoki Yoshinaga; Yuma Tsuta; Akiko Aizawa,Xin Zhao,Oral,In-person,SALLE  LA PALMERAIE,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Multilinguality & Low-Resource NLP,,domain; knowledge acquisition; knowledge; acquisition; adaptation; domain adaptation; bilingual; multilingual; english japanese; domain knowledge
908-MAIN,Contextual morphologically-guided tokenization for Latin encoder models,"Tokenization is a critical component of language model pretraining, yet standard tokenization methods often prioritize information-theoretical goals like high compression and low fertility rather than linguistic goals like morphological alignment. In fact, they have been shown to be suboptimal for morphologically rich languages, where tokenization quality directly impacts downstream performance. In this work, we investigate morphologically-aware tokenization for Latin, a morphologically rich language that is medium-resource in terms of pretraining data, but high-resource in terms of curated lexical resources -- a distinction that is often overlooked but critical in discussions of low-resource language modeling. We find that morphologically-guided tokenization improves overall performance on four downstream tasks. Performance gains are most pronounced for out of domain texts, highlighting our models' improved generalization ability. Our findings demonstrate the utility of linguistic resources to improve language modeling for morphologically complex languages. For low-resource languages that lack large-scale pretraining data, the development and incorporation of linguistic resources can serve as a feasible alternative to improve LM performance.",Marisa Hudspeth; Patrick J. Burns; Brendan O'Connor,Marisa Hudspeth,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multilinguality & Low-Resource NLP,"Linguistics, Syntax & Semantics",morphologically; tokenization; pretraining; morphologically rich; resources; pretraining data; resource; latin; goals; language modeling
909-MAIN,Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning,"Curriculum learningвЂ”organizing training data from easy to hardвЂ”has improved efficiency across machine learning domains, yet remains underexplored for language model pretraining. We present the first systematic investigation of curriculum learning in LLM pretraining, training over 200 models across three strategies: vanilla curriculum learning, pacing-based sampling, and interleaved curricula. Using six difficulty metrics spanning linguistic and information-theoretic properties, we evaluate performance on eight benchmarks under three realistic scenarios: limited data, unlimited data, and continual training. Our experiments show that curriculum learning consistently accelerates convergence in early and mid-training phases, reducing training steps by 18-45% to reach baseline performance. When applied as a warmup strategy before standard random sampling, curriculum learning yields sustained improvements up to 3.5%. We identify compression ratio, lexical diversity (MTLD), and readability (Flesch Reading Ease) as the most effective difficulty signals. Our findings demonstrate that data orderingвЂ”orthogonal to existing data selection methodsвЂ”provides a practical mechanism for more efficient LLM pretraining.",Yang ZHANG; Amr Mohamed; Hadi Abdine; Guokan Shang; Michalis Vazirgiannis,Yang ZHANG,Oral,In-person,Pavillon  DE RABAT,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Efficiency, Scaling & NLP Systems","Linguistics, Syntax & Semantics",curriculum; curriculum learning; pretraining; learning; sampling; model pretraining; language model pretraining; llm pretraining; training; random
914-MAIN,ObjChangeVR: Object State Change Reasoning from Continuous Egocentric Views in VR Environments,"Recent advances in multimodal large language models (MLLMs) offer a promising approach for natural language-based scene change queries in virtual reality (VR). Prior work on applying MLLMs for object state understanding has focused on egocentric videos that capture the camera wearer's interactions with objects. However, object state changes may occur in the background without direct user interaction, lacking explicit motion cues and making them difficult to detect. Moreover, no benchmark exists for evaluating this challenging scenario. To address these challenges, we introduce ObjChangeVR-Dataset, specifically for benchmarking the question-answering task of object state change. We also propose ObjChangeVR, a framework that combines viewpoint-aware and temporal-based retrieval to identify relevant frames, along with cross-view reasoning that reconciles inconsistent evidence from multiple viewpoints. Extensive experiments demonstrate that ObjChangeVR significantly outperforms baseline approaches across multiple state-of-the-art MLLMs.",Shiyi Ding; SHAOEN WU; Ying Chen,Ying Chen,Oral,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",Multimodal & Speech/Audio,object; change; state; mllms; state art mllms; viewpoints; scene; reconciles; environments recent; approach natural
916-MAIN,Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge,"Large Language Models (LLMs) fail at reasoning when their parametric knowledge is outdated or incorrect. A common solution for mitigating outdated or incorrect information in models is to provide updated facts in-context or by knowledge editing, but such updates can conflict with LLMs' parametric knowledge and limit their reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall in isolated scenarios. In this work, we introduce TКЂбґЂбґ„бґ‹ (*Testing Reasoning Amid Conflicting Knowledge*), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TКЂбґЂбґ„бґ‹ introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TКЂбґЂбґ„бґ‹ reveal that providing updated facts yields limited performance gains and even worsens performance, compared with no facts provided. Performance further degrades with more updated facts. This failure stems from both inability to faithfully integrate updated facts and flawed reasoning even when knowledge is integrated. TКЂбґЂбґ„бґ‹ provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.",Yiyang Feng; Zeming Chen; Haotian Wu; Jiawei Zhou; Antoine Bosselut,Yiyang Feng,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",knowledge; updated; facts; reasoning; parametric knowledge; multi step reasoning; parametric; step reasoning; conflicting; multi step
918-MAIN,"Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs","Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on concept pairs that are linguistically distinct (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.",Arya Labroo; Ivaxi Sheth; Vyas Raina; Amaani Ahmed; Mario Fritz,Arya Labroo,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",Interpretability & Model Analysis,concept; control; concepts; persuasiveness; humor; fine grained; grained; attribute; dual; generative
922-MAIN,"Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance","Understanding emotion from speech requires sensitivity to both lexical and acoustic cues. However, it remains unclear whether large audio language models (LALMs) genuinely process acoustic information or rely primarily on lexical contents. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in Narratives), a controlled benchmark designed to disentangle lexical reliance from acoustic sensitivity in emotion understanding. Across evaluations of six state-of-the-art LALMs, we observe a consistent lexical dominance. Models predict вЂњneutralвЂќ when lexical cues are neutral or absent, show limited gains under cue alignment, and fail to classify distinct emotions under cue conflict. In paralinguistic settings, performance approaches chance. These results indicate that current LALMs largely вЂњtranscribeвЂќ rather than вЂњlisten,вЂќ relying heavily on lexical semantics while underutilizing acoustic cues. LISTEN offers a principled framework for assessing emotion understanding in multimodal models.",Jingyi Chen; Zhimeng Guo; Jiyun Chun; Pichao WANG; Andrew Perrault; Micha Elsner,Jingyi Chen,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,,lexical; acoustic; emotion; lalms; cues; emotion understanding; cue; reliance; understanding; audio
925-MAIN,CSPB: Conversational Speech Processing Benchmark for Self-supervised Speech Models,"Recent advances in self-supervised learning (SSL) have led to powerful speech representation models, yet their robustness in real-world conversational settings remains largely untested. Most existing benchmarks focus on clean, single-speaker, and single-channel audio, failing to reflect the complexities of natural human interactionвЂ”where overlapping speech, background noise, and reverberation are the norm. To bridge these critical gaps, we present the Conversational Speech Processing Benchmark (CSPB), a new benchmark designed to assess the robustness of SSL speech models in realistic conversational scenarios. CSPB is constructed from four multi-party datasetsвЂ”AMI, AliMeeting, MMCSG, and DiPCoвЂ”and supports both single-channel and multi-channel evaluation. By releasing CSPB as an open-source toolkit, we aim to establish a unified framework for evaluating and advancing robust, spatially-aware self-supervised speech models.",Zili Huang; Matthew Maciejewski; Leibny Paola Garcia Perera; Shinji Watanabe; Sanjeev Khudanpur,Zili Huang,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",speech; speech models; self supervised; channel; conversational; supervised; self; single; benchmark; processing
932-MAIN,Multi-Token Completion for Text Anonymization,"Text anonymization is a critical task for enabling research and development in high-stakes domains containing private data, like medicine, law, and social services. While much research has focused on redacting sensitive content from text, substantially less work has focused on what to replace redacted content with, which can enhance privacy and becomes increasingly important with greater levels of redaction. In this work, we formulate predicting replacements for sensitive spans as a research task with principled use-inspired evaluation criteria. We further propose a multi-token completion method for accomplishing this task that is designed to preserve consistency with low compute requirements, thus facilitating practitioners to anonymize data locally before sharing it externally. Human and automated annotations demonstrate that our approach produces more realistic text and better preserves utility than alternative infilling methods and differentially private mechanisms across multiple domains without retraining. Overall, our work explores the under-studied task of what to replace redacted content with and contributes grounded evaluations capturing utility, facilitating future work.",Pulkit Madaan; Krithika Ramesh; Lisa Bauer; Charith Peris; Anjalie Field,Pulkit Madaan,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",redacted; replace; multi token; private; anonymization; facilitating; text; completion; content; work
935-MAIN,MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder-LLM Integration in Cross-Lingual Reasoning,"Large language models (LLMs) excel in English but still struggle with complex reasoning in many low-resource languages (LRLs). Existing methods align LLMs with multilingual encoders, such as LangBridge and MindMerger, raising the accuracy for mid and high-resource languages, yet large performance gap remains for LRLs. We present MERLIN, a model-stacking framework that iteratively refines in 2-stages based on a curriculum strategy (from general to specific where general is bilingual bitext and specific is task-specific data) and adapts only a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves exact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini by 15.2 pp. It also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp), demonstrating effectiveness across both low and high-resource settings.",Kosei Uemura; David GuzmГЎn; Quang Phuoc Nguyen; Jesujoba Oluwadara Alabi; En-Shiun Annie Lee; David Ifeoluwa Adelani,Kosei Uemura,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multilinguality & Low-Resource NLP,,curriculum; resource; high resource; resource languages; specific; general; specific task; encoder llm; align llms multilingual; large performance
936-MAIN,Now You Hear Me: Audio Narrative Attacks Against Large AudioвЂ“Language Models,"Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.",Ye Yu; Haibo Jin; Yaoning Yu; Jun Zhuang; Haohan Wang,Ye Yu,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Trustworthy, Safety, Privacy & Fairness",audio; speech; narrative; text; safety; art models including; triage; vulnerabilities remain; 98 26; synthetic speech
939-MAIN,Evaluating Adversarial Robustness of Concept Representations in Sparse Autoencoders,"Sparse autoencoders (SAEs) are commonly used to interpret the internal activations of large language models (LLMs) by mapping them to human-interpretable concept representations. While existing evaluations of SAEs focus on metrics such as the reconstruction-sparsity tradeoff, human (auto-)interpretability, and feature disentanglement, they overlook a critical aspect: the robustness of concept representations to input perturbations. We argue that robustness must be a fundamental consideration for concept representations, reflecting the fidelity of concept labeling. To this end, we formulate robustness quantification as input-space optimization problems and develop a comprehensive evaluation framework featuring realistic scenarios in which adversarial perturbations are crafted to manipulate SAE representations. Empirically, we find that tiny adversarial input perturbations can effectively manipulate concept-based interpretations in most scenarios without notably affecting the base LLM's activations. Overall, our results suggest that SAE concept representations are fragile and without further denoising or postprocessing they might be ill-suited for applications in model monitoring and oversight.",Aaron J. Li; Suraj Srinivas; Usha Bhalla; Himabindu Lakkaraju,Aaron J. Li,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",concept representations; concept; representations; perturbations; robustness; input perturbations; adversarial; manipulate; saes; sae
940-MAIN,"Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?","Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs have an incomplete grasp of narrative coherence.",Karin De Langis; PГјren Г–ncel; Ryan Peters; Andrew Elfenbein; Laura Kristen Allen; Andreas Schramm; Dongyeop Kang,Karin de Langis,Oral,In-person,SALLE  LE LIXUS,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Interpretability & Model Analysis,"Reasoning, Planning & Agents",narratives; incoherent; llms; separate; narrative; coherent; coherence; reliably; internal; grasp
941-MAIN,"Strong Memory, Weak Control: An Empirical Study of Executive Functioning in LLMs","Working memory, or the ability to hold and manipulate information in the mind, is a critical component of human intelligence and executive functioning. It is correlated with performance on various cognitive tasks, including measures of fluid intelligence, which encompasses reasoning and problem solving. We use a comprehensive set of classic working memory tasks to estimate the working memory capacity of large language models (LLMs). We find that in most cases, LLMs exceed normative human scores. However, we do not find that the increased capacity of working memory is associated with higher performance on other executive functioning tasks or problem solving benchmarks. These results suggest that LLMs may have deficits in attentional control and cognitive flexibility, which result in difficulties with inhibiting automatic responses and adapting to shifting information. Our findings suggest that reasoning models, although they often do not currently fully compensate for these deficits, may have the potential to do so in the future.",Karin De Langis; Jong Inn Park; Khanh Chi Le; Andreas Schramm; Andrew Elfenbein; Michael C. Mensink; Dongyeop Kang,Karin de Langis,Oral,In-person,Pavillon  DE RABAT,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",working memory; working; memory; deficits; intelligence; problem solving; capacity; solving; cognitive; suggest
942-MAIN,How Do Language Models Acquire Character-Level Information?,"Large language models (LLMs) have been reported to implicitly encode character-level information, despite not being explicitly provided during training. However, the mechanisms underlying this phenomenon remain largely unexplored. To reveal the mechanisms, we analyze how models acquire character-level knowledge by comparing LLMs trained under controlled settings, such as specifying the pre-training dataset or tokenizer, with those trained under standard settings. We categorize the contributing factors into those independent of tokenization. Our analysis reveals that merge rules and orthographic constraints constitute primary factors arising from tokenization, whereas semantic associations of substrings and syntactic information function as key factors independent of tokenization.",Soma Sato; Ryohei Sasano,Soma Sato,Oral,In-person,SALLE  WALILI,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Interpretability & Model Analysis,,character level; character; tokenization; level information; models acquire; factors; acquire; independent; level; information
943-MAIN,Analysing the role of lexical and temporal information in turn-taking through predictability,"Turn-taking is a fundamental component of human communication and is signalled through complex cues distributed across lexical, temporal, and prosodic information. Full-duplex models of spoken dialogue integrate these information sources to produce impressive turn-taking behaviour; However, existing evaluations of their turn-taking capabilities do not address which information sources drive their predictions. We present a systematic analysis of the role of lexical-temporal features on the predictability of turn structure by examining PairwiseTurnGPT, a full-duplex model of spoken dialogue transcripts. Through PCA, mixed-effects modelling, and temporal surprisal analysis, we reveal context-dependent patterns: linguistic fluency paradoxically creates overconfidence at intermediate completion points, while turn-shift overlap dominates boundary detection. Our findings uncover where lexical-temporal information suffices and where additional cues become necessary, establishing a deeper understanding of how turn-taking cues are distributed and how to evaluate dialogue systems.",Sean Leishman; Sarenne Wallbridge; Peter Bell,Sean Leishman,Oral,In-person,SALLE  LE LIXUS,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Dialogue, Conversational & Interactive NLP",Interpretability & Model Analysis,turn; turn taking; taking; temporal; lexical; information; information sources; predictability; dialogue; cues
945-MAIN,Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances,"Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.",Jiyun Chun; Eric Fosler-Lussier; Michael White; Andrew Perrault,Jiyun Chun,Oral,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP","Reasoning, Planning & Agents",child; adult; independence; length; utterance; expansion; discourse; axes; context; age
948-MAIN,Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese,"Ancient people translated classical Chinese into Japanese by annotating around each character. We abstract this process as sequence tagging tasks and fit them into modern language technologies. The research of this annotation and translation system is a facing low-resource problem. We release this problem by introducing a LLM-based annotation pipeline and construct a new dataset from digitalized open-source translation data. We show that under the low-resource setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the training of sequence tagging tasks. We also evaluate the performance of large language models. They achieve high scores in direct machine translation, but they are confused when being asked to annotate characters. Our method could work as the reinforcement of LLMs.",Zilong Li; Jie Cao,Zilong Li,Oral,Virtual,ZOOM,Virtual TBA,,,Machine Translation,Multilinguality & Low-Resource NLP,translation; chinese; chinese japanese; tagging tasks; tagging; annotation; classical; japanese; introducing; sequence
949-MAIN,Extending Audio Context for Long-Form Understanding in Large Audio-Language Models,"Large Audio-Language Models (LALMs) are often constrained by short audio context windows, even when their text backbones support long contexts, limiting long-form audio understanding. Prior work has introduced context-extension methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains unexplored. First, building on RoPE-based context extension, we introduce Partial YaRN, a training-free, audio-only extension method that modifies only audio token positions, leaving text positions intact to preserve the base LLMвЂ™s text capabilities. Second, we propose Virtual Longform Audio Training (VLAT), a training strategy that extends Partial YaRN into a training-time positional augmentation. VLAT simulates diverse audio lengths during training, enabling generalization to inputs far longer than those seen in training and improving robustness for long-context audio understanding. Our experiments on SALMONN and Qwen2-Audio show that Partial YaRN outperforms the original models across wide range of settings, and VLAT training strategy provides substantial improvement, achieving strong performance on long audio of unseen lengths.",Yuatyong Chaichana; Pittawat Taveekitworachai; Warit Sirichotedumrong; Potsawee Manakul; Kunat Pipatanakul,Warit Sirichotedumrong,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,,audio; long; training; extension; partial; context; audio understanding; lalms; lengths; training strategy
951-MAIN,Common Sense or Ableism? Rethinking Commonsense Reasoning Through the Lens of Disability,"Commonsense reasoning (CSR) is a popular set of tasks used to evaluate large language model performance. However, what constitutes common sense might not apply to disabled people, leaving them out of AI progress by ignoring their identities. This paper investigates how datasets and models deal with disability in commonsense reasoning through a case study of three types of disabilities: autism, vision impairments, and long COVID. We first obtain annotations from disabled and non-disabled persons on $n = 300$ samples of three widely used CSR datasets (CommonsenseQA, Social IQA, Physical IQA) for $\textit{ableism}$, which we define as entries in which gold labels assume a lack of disability, finding that $\textasciitilde$15% of entries contain ableism. Then, we investigate LLMs' ability to flag ableism in CSR instances, and their flexibility to adapt to disability contexts---whether the CSR gold answer changes if the person is disabled. Our results show that the three LLMs tested have low sensitivity to human-detected ableism occurrences but still detect 5 to 25% of entries as ableist. Our findings call into question whether relying on such assumption-laden commonsense reasoning benchmarks risks leaving disabled people behind in the AI world.",Karina H Halevy; Kimi Wenzel; Seyun Kim; Kyle Dean Bauer; Bruno Neira; Mona T. Diab; Maarten Sap,Karina Halevy,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",,commonsense reasoning; commonsense; entries; common sense; people; gold; sense; leaving; reasoning; common
953-MAIN,Detecting Hallucinations in Vision-Language Models without Generating a Single Token,"Hallucinations remain a persistent challenge for visionвЂ“language models (VLMs), which often describe nonexistent objects or fabricate facts. Existing detection methods typically operate after text generation, making intervention both costly and untimely. We investigate whether hallucination risk can instead be predicted before any token is generated by probing a modelвЂ™s internal representations in a single forward pass. Across a diverse set of visionвЂ“language tasks and eight modern VLMs including Llama-3.2, Gemma-3, Phi-4, and Qwen2.5-VL we examine three families of internal representations: (i) visual-only features without multimodal fusion, (ii) vision-token representations within the text decoder, and (iii) query-token representations that integrate visual and textual information before generation. Probes trained on these representations achieve strong hallucination-detection performance without decoding, reaching up to 0.93 AUROC on Gemma-3-12B, Phi-4-VL, and Molmo. Late query-token states are the most predictive for most models, while visual or mid-layer features dominate in a few architectures (e.g., ~0.79 for Qwen2.5-VL using visual-only features). These results demonstrate that (1) hallucination risk is detectable pre-generation, (2) the most informative layer and modality vary across architectures, and (3) lightweight probes can enable early abstention, selective routing, and adaptive decoding to improve both safety and efficiency.",Sai Akhil Kogilathota; Sripadha Vallabha E G; Luzhe Sun; Jiawei Zhou,Sai Akhil Kogilathota; Sripadha Vallabha E G,Oral,In-person,Pavillon  DE RABAT,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Summarization & Generation,"Efficiency, Scaling & NLP Systems",representations; token; visual; hallucination; hallucination risk; visual features; phi; qwen2 vl; token representations; gemma
954-MAIN,Nanda Family: Open-Weights Generative Large Language Models for Hindi,"Large language models remain predominantly English-centric, which limits their usefulness for underrepresented languages. We help bridge this gap for Hindi with Nanda, a family of open-weights bilingual models (10B and 87B) adapted from Llama-3/3.1. Our approach integrates: (i) a tokenizer extending Llama's vocabulary with 20% Hindi-specific tokens, thus halving Hindi tokenization fertility while preserving English efficiency, (ii) Hindi-first parameter-efficient continuous pretraining using Llama Pro on a 65B-token corpus spanning Devanagari script, code-mixed, and romanized Hindi, and (iii) bilingual instruction and safety alignment on a large culturally grounded dataset. The resulting Nanda models outperform open-weights LLMs of comparable size in terms of generative performance and cultural knowledge, while being on par on MCQ benchmarks. Nanda-87B further demonstrates state-of-the-art performance on summarization, translation, transliteration, and instruction following. Moreover, both models show state-of-the-art performance in terms of safety. Our results demonstrate that careful tokenizer design, data curation, and expansion-based continual pretraining can yield capable and safe LLMs for resource-poor languages without compromising English performance.",Aaryamonvikram Singh; Debopriyo Banerjee; Dhruv Sahnan; Monojit Choudhury; Shivam Chauhan; Rocktim Jyoti Das; Xudong Han; Haonan Li; Alok Anil Jadhav; Utkarsh Agarwal; Mukund Choudhary; Fajri Koto; Junaid Hamid Bhat; Awantika Shukla; Samujjwal Ghosh; Samta Kamboj; Onkar Pandit; Lalit Pradhan; Rahul Pal; Sunil Kumar Sahu; Parvez Mullah; Ali El Filali; Zainul Abedien Ahmed Quraishi; Neha Sengupta; Gokulakrishnan Ramakrishnan; Rituraj Joshi; Gurpreet Gosal; Avraham Sheinin; Natalia Vassilieva; Preslav Nakov,Preslav Nakov; Aaryamonvikram Singh; Dhruv Sahnan; Ali El Filali;,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness","Dialogue, Conversational & Interactive NLP",hindi; nanda; weights; llama; tokenizer; bilingual; family; pretraining; english; open
957-MAIN,Wugnectives: Novel Entity Inferences of Language Models from Discourse Connectives,"The role of world knowledge has been particularly crucial to predict the discourse connective that marks the discourse relation between two arguments, with language models (LMs) being generally successful at this task. We flip this premise in our work, and instead study the inverse problem of understanding whether discourse connectives can inform LMs about the world. To this end, we present WUGNECTIVES, a dataset of 8,880 stimuli that evaluates LMs' inferences about *novel* entities in contexts where connectives link the entities to particular attributes. On investigating 17 different LMs at various scales, and training regimens, we found that tuning an LM to show reasoning behavior yields noteworthy improvements on most connectives. At the same time, there was a large variation in LMs' overall performance across connective type, with *all* models systematically struggling on connectives that express a concessive meaning. Our findings pave the way for more nuanced investigations into the functional role of language cues as captured by LMs.",Daniel Brubaker; William Sheffield; Junyi Jessy Li; Kanishka Misra,Daniel Brubaker,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Linguistics, Syntax & Semantics","Reasoning, Planning & Agents",connectives; lms; discourse; inferences; entities; role; findings pave way; functional role; novel entity; language cues
961-MAIN,Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval over haystacks,"Existing multilingual long-context benchmarks, often based on the popular needle-in-a-haystack test, primarily evaluate a model's ability to locate specific information buried within irrelevant texts. However, such a retrieval-centric approach is myopic and inherently limited, as successful recall alone does not indicate a model's capacity to reason over extended contexts. Moreover, these benchmarks are susceptible to data leakage, short-circuiting, and risk making the evaluation a priori identifiable. To address these limitations, we introduce MLRBench, a new synthetic benchmark for multilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes beyond surface-level retrieval by including tasks that assess multi-hop inference, aggregation, and epistemic reasoning. Spanning seven languages, MLRBench is designed to be parallel, resistant to leakage, and scalable to arbitrary context lengths. Our extensive experiments with an open-weight large language model (LLM) reveal a pronounced gap between high- and low-resource languages, particularly for tasks requiring the model to aggregate multiple facts or predict the absence of information. We also find that, in multilingual settings, LLMs effectively utilise less than 30% of their claimed context length. Although off-the-shelf Retrieval Augmented Generation helps alleviate this to a certain extent, it does not solve the long-context problem. We open-source MLRBench to enable future research in improved evaluation and training of multilingual LLMs.",Amey Hengle; Prasoon Bajpai; Soham Dan; Tanmoy Chakraborty,Amey Hengle,Oral,In-person,SALLE  LA PALMERAIE,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multilinguality & Low-Resource NLP,"LLM Evaluation, Benchmarks & Metrics",long context; context; multilingual; long; retrieval; leakage; extended; reason; does; benchmarks
963-MAIN,Beyond Accuracy: Benchmarking Abstention and Uncertainty in Large Language Models for Medical Question Answering,"Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA), integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain when uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.",Sravanthi Machcha; Sushrita Yerra; Sahil Gupta; Aishwarya Sahoo; Sharmin Sultana; hong yu; Zonghai Yao,Sravanthi Machcha,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",abstention; options; uncertain; trustworthy; perturbations; question; uncertainty; medical; explicit; accuracy
965-MAIN,MedQA-CS: Benchmarking Large Language Models Clinical Skills Using an AI-SCE Framework,"Artificial intelligence (AI) and large language models (LLMs) in healthcare require advanced clinical skills (CS), yet current benchmarks fail to evaluate these comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by medical education's Objective Structured Clinical Examinations (OSCEs), to address this gap. MedQA-CS evaluates LLMs through two instruction-following tasksвЂ”LLM-as-medical-student and LLM-as-CS-examinerвЂ”designed to reflect real clinical scenarios. Our contributions include developing MedQA-CS, a comprehensive evaluation framework with publicly available data and expert annotations, and providing the quantitative and qualitative assessment of LLMs as reliable judges in CS evaluation. Our experiments show that MedQA-CS is a more challenging benchmark for evaluating clinical skills than traditional multiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks, MedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities for both open- and closed-source LLMs.",Zonghai Yao; Zihao Zhang; Chaolong Tang; Xingyu Bian; Youxia Zhao; Zhichao Yang; Junda Wang; Huixue Zhou; Won Seok Jang; Feiyun Ouyang; hong yu,Zonghai Yao,Oral,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),"LLM Evaluation, Benchmarks & Metrics",medqa; clinical; skills; comprehensive evaluation; medical; llms; benchmarks; evaluation; assessment llms; skills using
966-MAIN,Machine translation Evaluation Eng-Thai MQM Ranking dataset,"We introduce MEET-MR (Machine Translation EnglishвЂ“Thai MQM and Ranking Dataset), a comprehensive benchmark for evaluating EnglishвЂ“Thai machine translation systems. The dataset is constructed using the Multidimensional Quality Metrics (MQM) annotation framework, providing fine-grained human judgments of translation quality. In addition, MEET-MR includes human preference rankings and reference translations, enabling both absolute and relative assessments of translation quality. The dataset spans 9 diverse domains ensuring rich linguistic and contextual diversity. By combining high-quality reference translations, objective MQM error annotations, and subjective preference rankings, MEET-MR serves as a valuable resource for studying translation quality estimation, model alignment with human evaluation, and cross-domain performance in EnglishвЂ“Thai machine translation.",Phichet Phuangrot; Natdanai Trintawat; Kanawat Vilasri; Yanapat Patcharawiwatpong; Pachara Boonsarngsuk; Nat Pavasant; Ekapol Chuangsuwanich,Nat Pavasant,Oral,In-person,SALLE  LA PALMERAIE,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Machine Translation,"LLM Evaluation, Benchmarks & Metrics",translation; englishвђ; meet; machine translation; translation quality; machine; quality; rankings; translations; reference
969-MAIN,Continual-learning for Modelling Low-Resource Languages from Large Language Models,"Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.",Santosh Srinath K; Mudit Somani; Varun Reddy Padala; Prajna Upadhyay; Abhijit Das,Santosh Srinath Kaza,Oral,In-person,SALLE  LE LIXUS,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multilinguality & Low-Resource NLP,Multimodal & Speech/Audio,catastrophic forgetting; catastrophic; modelling; forgetting; slm; continual learning; continual; low resource languages; resource languages; language
980-MAIN,Language-Grounded Multi-Domain Image Translation via Semantic Difference Guidance,"Multi-domain image-to-image translation requires grounding semantic differences expressed in natural language prompts into corresponding visual transformations, while preserving unrelated structural and semantic content. Existing methods struggle to maintain structural integrity and provide fine-grained, attribute-specific control, especially when multiple domains are involved. We propose LACE (Language-grounded Attribute-Controllable Translation), built on two components: (1) a GLIP-Adapter that fuses global semantics with local structural features to preserve consistency, and (2) a Multi-Domain Control Guidance mechanism that explicitly grounds the semantic delta between source and target prompts into per-attribute translation vectors, aligning linguistic semantics with domain-level visual changes. Together, these modules enable compositional multi-domain control with independent strength modulation for each attribute. Experiments on CelebA(Dialog) and BDD100K demonstrate that LACE achieves high visual fidelity, structural preservation, and interpretable domain-specific control, surpassing prior baselines. This positions LACE as a cross-modal content generation framework bridging language semantics and controllable visual translation. Code will be publicly available.",jongwon ryu; Joonhyung Park; Jaeho Han; Yeong-Seok Kim; Hye-Rin Kim; Sunjae Yoon; Junyeong Kim,Jongwon Ryu,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Linguistics, Syntax & Semantics",Machine Translation,multi domain; attribute; translation; domain; control; structural; visual; semantics; semantic; image
981-MAIN,Evaluation of Deontic Conditional Reasoning in Large Language Models: The Case of Wason's Selection Task,"As large language models (LLMs) advance in linguistic competence, growing attention has turned to their reasoning abilities. Humans excel at domain-specific reasoning, particularly in normative rather than purely formal contexts. While previous studies have compared LLM and human reasoning, the domain specificity of LLMs remains underexplored. This study examines LLMs' conditional reasoning with deontic rules using the Wason Selection Task, a classic paradigm in cognitive science. Building on findings that human errors in this task reflect a matching bias, namely a tendency to select cards matching the lexical content of the rule, we test whether similar biases appear in LLMs using a dataset of conditional rules with negation. Results show that, like humans, LLMs reason more effectively with deontic rules and display matching bias in their error patterns. These findings indicate that LLMs exhibit domain-specific reasoning and matching bias with respect to deontic rules, providing insights into how their reasoning relates to human cognition.",Hirohiko Abe; Kentaro Ozeki; Risako Ando; Takanobu Morishita; Koji Mineshima; Mitsuhiro Okada,Hirohiko Abe,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",deontic; rules; matching; conditional; reasoning; domain specific reasoning; specific reasoning; bias; llms; humans
986-MAIN,LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction,"Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.",Junior Cedric Tonga; Chen Cecilia Liu; Iryna Gurevych; Fajri Koto,Junior Cedric TONGA,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Reasoning, Planning & Agents","Trustworthy, Safety, Privacy & Fairness",cultural; llms cultural; cultural knowledge; knowledge; commonsense; commonsense knowledge; culture; chains; llms; knowledge graph
992-MAIN,"Nahw: A Comprehensive Benchmark of Arabic Grammar Understanding, Error Detection, Correction, and Explanation","Grammar comprehension is a critical capability for large language models (LLMs) to achieve fluency in a target language. In low-resource settings, such as Arabic, limited availability of high-quality data can lead to significant gaps in grammatical understanding, making systematic evaluation essential. We introduce Nahw, a comprehensive benchmark for Arabic grammar that covers both theoretical knowledge and practical applications, including grammatical error detection, correction, and explanation. We evaluate a range of LLMs on these tasks and find that many models still exhibit substantial deficiencies in Arabic grammar comprehension with GPT-4o achieving a score of 67%, ALLaM-7B achieving 42%, and Fanar-1-9B achieving 35% on average over all tasks. Our experiments also demonstrate that while fine-tuning with synthetic data can improve performance, it does not match the effectiveness of training on natural, high-quality data.",Hamdy Mubarak; Majd Hawasly; Abubakr Mohamed,Hamdy Mubarak,Oral,In-person,SALLE  LA PALMERAIE,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",Multilinguality & Low-Resource NLP,grammar; arabic; high quality data; error detection; quality data; achieving; comprehensive benchmark; comprehension; grammatical; correction
994-MAIN,Confidence Leaps in LLM Reasoning: Early Stopping and Cross-Model Transfer,"We challenge the common assumption that Large Language Models (LLMs) build confidence gradually during reasoning. Instead, we find that conviction is often reached in a discrete ``moment of insight'', characterized by a sudden and sharp increase in an answer's probability---a phenomenon we term a ""confidence leap"". Leveraging this discovery, we introduce a training-free, model-agnostic early-stopping heuristic that halts generation upon detecting such a leap, significantly reducing the generation length without sacrificing accuracy. We also demonstrate that the reasoning text leading up to this leap is semantically potent and transferable: feeding this partial reasoning to a different model family substantially boosts its performance. This suggests that the ""confidence leap"" marks a shared, interpretable reasoning milestone, not just a model-specific statistical artifact.",Pavel Tikhonov; Ivan Oseledets; Elena Tutubalina,Pavel Tikhonov,Oral,In-person,SALLE  LE RIAD,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Summarization & Generation,Multilinguality & Low-Resource NLP,leap; confidence; early stopping; stopping; reasoning; early; substantially boosts; introduce training free; interpretable reasoning; gradually
996-MAIN,Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations,"We systematically examine, analyze, and compare representative creativity measuresвЂ”creativity index, perplexity, syntactic templates, and LLM-as-a-JudgeвЂ”across diverse creative domains, including creative writing, unconventional problem-solving, and research ideation. Our analyses reveal that these metrics exhibit limited consistency, capturing different dimensions of creativity. We highlight key limitations, including the creativity index's focus on lexical diversity, perplexity's sensitivity to model confidence, and the inability of syntactic templates to capture conceptual creativity. Additionally, LLM-as-a-Judge shows instability and bias. Our findings underscore the need for more robust, generalizable evaluation frameworks that better align with human judgments of creativity.",Li-Chun Lu; Miri Liu; Pin Chun Lu; Yufei Tian; Shao-Hua Sun; Nanyun Peng,Li-Chun Lu,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",Interpretability & Model Analysis,creativity; index; templates; creative; perplexity; syntactic; better align human; key limitations including; bias findings; systematically examine
1007-MAIN,TReX: Tokenizer Regression for Optimal Data Mixture,"Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. While a tokenizerвЂ™s compression performance critically affects the efficiency of LLM training and inference, existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios. We introduce $\textbf{T}$okenizer $\textbf{Re}$gression for Optimal Data Mi$\textbf{X}$ture ($\text{TReX}$), a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training. $\text{TReX}$ trains small-scale proxy tokenizers on random mixtures, gathers their compression statistics, and learns to predict compression performance from data mixtures. This learned model enables scalable mixture search before large-scale tokenizer training, mitigating the accuracy-cost trade-off in multilingual tokenizer design. Tokenizers trained with TReXвЂ™s predicted mixtures outperform mixtures based on LLaMA3 and uniform distributions by up to 12% in both in- and out-of-distribution compression efficiency, demonstrating strong scalability, robustness, and practical effectiveness.",Inho Won; HanGyeol Yoo; Minkyung Cho; Jungyeul Park; Hoyun Song; KyungTae Lim,"Inho Won, Hangyeol Yoo",Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Efficiency, Scaling & NLP Systems",,tokenizer; compression; optimal; tokenizers; mixture; textbf; data mixture; regression; scale; large scale
1008-MAIN,CONGRAD: Conflicting Gradient Filtering for Multilingual Preference Alignment,"Naive joint training of large language models (LLMs) for multilingual preference alignment can suffer from negative interference. This is a known issue in multilingual training, where conflicting objectives degrade overall performance. However, the impact of this phenomenon in the context of multilingual preference alignment remains largely underexplored. To address this issue, we propose ConGrad, an effective and scalable filtering method that mitigates this interference by identifying and selecting preference samples that exhibit high cross-lingual affinity. Based on principles of multi-objective optimization, our approach computes an aggregated, cross-lingually beneficial gradient direction and uses this to filter for samples whose individual gradients align with this consensus direction. To ensure scalability for LLMs, we incorporate a sublinear gradient compression strategy that reduces memory overhead during gradient accumulation. We integrate ConGrad into a self-rewarding framework and evaluate on LLaMA3-8B and Gemma2-2B across 10 languages. Results show that ConGrad consistently outperforms strong baselines in both seen and unseen languages, with minimal alignment tax.",Jiangnan Li; Thuy-Trang Vu; Christian Herold; Amirhossein Tebbifakhr; Shahram Khadivi; Gholamreza Haffari,Thuy-Trang Vu,Oral,Virtual,ZOOM,Virtual TBA,,,Multilinguality & Low-Resource NLP,"Efficiency, Scaling & NLP Systems",gradient; preference alignment; preference; multilingual; alignment; interference; conflicting; filtering; direction; issue
1011-MAIN,Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs,"Large Language Models exhibit implicit personalities in their generation, but reliably controlling or aligning these traits to meet specific needs remains an open challenge. The need for effective mechanisms for behavioural manipulation of the model during generation is a critical gap in the literature that needs to be fulfilled. Personality-aware LLMs hold a promising direction towards this objective. However, the relationship between these psychological constructs and their representations within LLMs remains underexplored and requires further investigation. Moreover, it is intriguing to understand and study the use of these representations to steer the modelsвЂ™ behaviour. We propose a novel pipeline that extracts hidden state activations from transformer layers using the Big Five Personality Traits (Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism), which is a comprehensive and empirically validated framework to model human personality applies low-rank subspace discovery methods, and identifies trait-specific optimal layers across different model architectures for robust injection. The resulting personality-aligned directions are then operationalised through a flexible steering framework with dynamic layer selection, enabling precise control of trait expression in LLM outputs. Our findings reveal that personality traits occupy a low-rank shared subspace, and that these latent structures can be transformed into actionable mechanisms for effective steering through careful perturbations without impacting the fluency, variance and general capabilities, helping to bridge the gap between psychological theory and practical model alignment",Pranav Bhandari; Nicolas Fay; Sanjeevan Selvaganapathy; Amitava Datta; Usman Naseem; Mehwish Nasim,Pranav Bhandari,Oral,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,Summarization & Generation,personality; trait; traits; steering; layer selection; personality traits; subspace; low rank; psychological; needs
1016-MAIN,Speculative Decoding Speed-of-Light: Optimal Lower Bounds via Branching Random Walks,"Speculative generation has emerged as a promising technique to accelerate inference in large language models (LLMs) by leveraging parallelism to verify multiple draft tokens simultaneously. However, the fundamental limits on the achievable speedup remain poorly understood. In this work, we establish the first ``tight'' lower bounds on the runtime of any deterministic speculative generation algorithm. This is achieved by drawing a parallel between the token generation process and branching random walks, which allows us to analyze the optimal draft tree selection problem. We prove, under basic assumptions, that the expected number of tokens successfully predicted per speculative iteration is bounded as $\mathbb{E}[X] \leq (\mu + \mu_{(2)})\log(B )/\mu^2 + O(1)$, where $B$ is the verifier's batch size, $\mu$ is the expected entropy of the verifier's output distribution, and $\mu_{(2)}$ is this entropy's second moment. This result provides new insights into the limits of parallel token generation, and could guide the design of future speculative decoding systems. Empirical evaluations on Llama models validate our theoretical predictions, confirming the tightness of our bounds in practical settings.",Sergey Pankratov; Dan Alistarh,Sergey Pankratov,Oral,In-person,SALLE  LE LIXUS,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Efficiency, Scaling & NLP Systems",Summarization & Generation,speculative; bounds; branching; walks; random walks; verifier; token generation; draft; speculative decoding; expected
1019-MAIN,KG-CRAFT: Knowledge Graph-based Contrastive Reasoning with LLMs for Enhancing Automated Fact-checking,"Claim verification is a core module in automated fact-checking systems, tasked with determining claim veracity using retrieved evidence. This work presents KG-CRAFT, a novel knowledge graph-based contrastive reasoning method that enhances automatic claim verification by LLMs. Our approach first constructs a knowledge graph from claims and associated reports, then formulates contextually relevant contrastive questions based on the knowledge graph structure. These questions guide the distillation of evidence-based reports, which are synthesised into a concise summary for veracity assessment. Extensive evaluations on two real-world datasets (LIAR-RAW and RAWFC) demonstrate that our method achieves a new state-of-the-art in predictive performance. Comprehensive analyses validate in detail the effectiveness of our knowledge graph-based contrastive reasoning approach in improving LLMs' fact-checking capabilities.",VГ­tor LourenГ§o; Aline Paes; Tillman Weyde; Audrey Depeige; Mohnish Dubey,Vítor Nascimento Lourenço,Oral,In-person,SALLE  LA PALMERAIE,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Efficiency, Scaling & NLP Systems","Retrieval, Grounding & External Knowledge (RAG)",knowledge graph; contrastive reasoning; graph; knowledge graph based; based contrastive; contrastive; fact checking; claim; graph based; checking
1023-MAIN,"SciRAG: Adaptive, Citation-Aware, and Outline-Guided Retrieval and Synthesis for Scientific Literature","The accelerating growth of scientific publications has intensified the need for scalable, trustworthy systems to synthesize knowledge across diverse literature. While recent retrieval-augmented generation (RAG) methods have improved access to scientific information, they often overlook citation graph structure, adapt poorly to complex queries, and yield fragmented, hard-to-verify syntheses. We introduce SciRAG, an open-source framework for scientific literature exploration that addresses these gaps through three key innovations: (1) adaptive retrieval that flexibly alternates between sequential and parallel evidence gathering; (2) citation-aware symbolic reasoning that leverages citation graphs to organize and filter supporting documents; and (3) outline-guided synthesis that plans, critiques, and refines answers to ensure coherence and transparent attribution. Extensive experiments across multiple benchmarks such as QASA and ScholarQA demonstrate that SciRAG outperforms prior systems in factual accuracy and synthesis quality, establishing a new foundation for reliable, large-scale scientific knowledge aggregation.",Hang Ding; Yilun Zhao; Tiansheng Hu; Manasi Patwardhan; Arman Cohan,Hang Ding,Oral,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",Domain NLP (Biomedical/Clinical/Legal/Scientific),citation; scientific; synthesis; literature; outline; retrieval; adaptive; guided; experiments multiple benchmarks; knowledge diverse
1026-MAIN,Unintended Token-Level Memorization in Language Model Fine-Tuning,"Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacyвЂ“utility trade-offs, while differential privacy achieves the strongest reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.",Marton Szep; Jorge Marin Ruiz; Georgios Kaissis; Paulina Seidl; RГјdiger von Eisenhart-Rothe; Florian Hinterwimmer; Daniel Rueckert,Marton Szep,Oral,In-person,SALLE  LA PALMERAIE,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness",,privacy; memorization; pii; unintended; differential privacy; differential; privacy preserving; trade offs; leakage; offs
1029-MAIN,Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models,"With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements: achieving up to +20-point gains over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.",Francesco Maria Molfese; Momchil Hardalov; Rexhina Blloshmi; Bill Byrne; AdriГ de Gispert,Momchil Hardalov,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",cache compression; cache; kv cache; compression; context; long context; long; robustness; fine tuning; tuning
1046-MAIN,The Pluralistic Moral Gap: Understanding Moral Judgment and Value Differences between Humans and Large Language Models,"People increasingly rely on Large Language Models (LLMs) for moral advice, which may influence humans' decisions. Yet, little is known about how closely LLMs align with human moral judgments. To address this, we introduce the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a distribution of human moral judgments consisting of a binary evaluation and a free-text rationale. We treat this problem as a pluralistic distributional alignment task, comparing the distributions of LLM and human judgments across dilemmas. We find that models reproduce human judgments only under high consensus; alignment deteriorates sharply when human disagreement increases. In parallel, using a 60-value taxonomy built from 3,783 value expressions extracted from rationales, we show that LLMs rely on a narrower set of moral values than humans. These findings reveal a pluralistic moral gap--a mismatch in both the distribution and diversity of values expressed. To close this gap, we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method that conditions model outputs on human-derived value profiles. DMP improves alignment by 64.3% and enhances value diversity, offering a step toward more pluralistic and human-aligned moral guidance.",Giuseppe Russo; Debora Nozza; Paul RГ¶ttger; Dirk Hovy,Giuseppe Russo,Oral,In-person,Pavillon  DE RABAT,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",moral; pluralistic; value; judgments; human; humans; dilemmas; human judgments; values; gap
1050-MAIN,CoReTab: Improving Multimodal Table Understanding with Code-driven Reasoning,"Existing datasets for multimodal table understanding, such as MMTab, primarily provide short factual answers without explicit multi-step reasoning supervision. Models trained on these datasets often generate brief responses that offers insufficient accuracy and limited interpretability into how these models arrive at the final answer. We introduce CoReTab, a code-driven reasoning framework that produces scalable, interpretable, and automatically verifiable annotations by coupling natural-language explanations with executable Python code. Using the CoReTab framework, we curate a dataset of 115K verified samples averaging 529 tokens per response and fine-tune open-source MLLMs through a three-stage pipeline. We evaluate the resulting model trained on CoReTab across 17 MMTab benchmarks spanning table question answering, fact verification, and table structure understanding. Our model achieves significant gains of +6.5%, +6%, and +25.6%, respectively, over MMTab-trained baselines, while producing transparent and verifiable reasoning traces. These results establish CoReTab as a robust and generalizable supervision framework for improving multi-step reasoning in multimodal table understanding.",Van-Quang Nguyen; Takayuki Okatani,Van-Quang Nguyen,Oral,In-person,SALLE  LE RIAD,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Reasoning, Planning & Agents",Multimodal & Speech/Audio,table; table understanding; driven reasoning; understanding; multi step reasoning; reasoning; step reasoning; verifiable; multimodal; trained
1051-MAIN,Explaining Generalization of AI-Generated Text Detectors Through Linguistic Analysis,"AI-text detectors achieve high accuracy on in-domain benchmarks, but often struggle to generalize across different generation conditions such as unseen prompts, model families, or domains. While prior work has reported these generalization gaps, there are limited insights about the underlying causes. In this work, we present a systematic study aimed at explaining generalization behavior through linguistic analysis. We construct a comprehensive benchmark that spans 6 prompting strategies, 7 large language models (LLMs), and 4 domain datasets, resulting in a diverse set of human- and AI-generated texts. Using this dataset, we fine-tune classification-based detectors on various generation settings and evaluate their cross-prompt, cross-model, and cross-dataset generalization. To explain the performance variance, we compute correlations between generalization accuracies and feature shifts of 80 linguistic features between training and test conditions. Our analysis reveals that generalization performance for specific detectors and evaluation conditions is significantly associated with linguistic features such as tense usage and pronoun frequency.",Yuxi Xia; Kinga StaЕ„czak; Benjamin Roth,Yuxi Xia,Oral,In-person,SALLE  LA PALMERAIE,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics",Summarization & Generation,generalization; detectors; conditions; linguistic analysis; explaining; linguistic; linguistic features; ai generated; cross; analysis
1052-MAIN,Elections go bananas: a first large-scale multilingual study of pluralia tantum using LLMs,"In this paper, we are interested in the expansion of pluralia tantum, i.e., defective nouns which lack a singular form, like scissors. We base our work on a qualitatively developed annotation framework. On a corresponding hand-annotated testset, we show that the OpenAI and DeepSeek models are all well equipped to annotate semantic, syntactic and sense categories, ranging from 51вЂ“89% in accuracy. Next, we turn to a large-scale investigation of pluralia tantum. Using dictionaries, we extract candidate words for Italian, Russian and English and keep those for which the changing ratio of singular and plural form is evident in a corresponding reference corpus. We use an LLM to annotate each instance from the reference corpus according to the annotation framework. We show that the large amount of automatically annotated sentences can be used to perform in-depth linguistic analysis, both to study the change in meaning of nouns, but also to update dictionary classifications using modern reference corpora.",Elena Spaziani; Kamyar Zeinalipour; Pierluigi Cassotti; Nina Tahmasebi,Elena Spaziani,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Linguistics, Syntax & Semantics",Interpretability & Model Analysis,reference; annotation framework; nouns; annotate; corresponding; corpus; form; annotation; annotated; large scale
1053-MAIN,Post-ASR Correction in Hindi: Comparing Language Models and Large Language Models in Low-Resource Scenarios,"Automatic Speech Recognition (ASR) systems for low-resource languages like Hindi often produce erroneous transcripts due to limited annotated data and linguistic complexity. **Post-ASR correction** using language models (LMs) and large language models (LLMs) offers a promising approach to improve transcription quality. In this work, we compare fine-tuned LMs (mT5, ByT5), fine-tuned LLMs (Nanda 10B), and instruction-tuned LLMs (GPT-4o-mini, LLaMA variants) for post-ASR correction in Hindi. Our findings reveal that **smaller, fine-tuned models** consistently **outperform larger LLMs** in both fine-tuning and in-context learning (ICL) settings. We observe a **U-shaped inverse scaling** trend under zero-shot ICL, where mid-sized LLMs degrade performance before marginal recovery at extreme scales, yet still fall short of fine-tuned models. **ByT5 is more effective for character-level corrections** such as transliteration and word segmentation, while **mT5 handles broader semantic inconsistencies**. We also identify performance drops in out-of-domain settings and propose **mitigation strategies** to preserve domain fidelity. In particular, we observe similar trends in **Marathi and Telugu**, indicating the broader applicability of our findings across low-resource Indian languages.",Rishabh Kumar; Amrith Krishna; Ganesh Ramakrishnan; Preethi Jyothi,Rishabh Kumar,Oral,In-person,SALLE  LE LIXUS,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,Multilinguality & Low-Resource NLP,asr; tuned; hindi; fine tuned; correction; post; mt5; fine; tuned llms; low resource
1059-MAIN,CacheNotes: Task-Aware Key-Value Cache Compression for Reasoning-Intensive Knowledge Tasks,"Integrating external knowledge into Large Language Models (LLMs) is crucial for many real-world applications, yet current methods like Retrieval-Augmented Generation (RAG) face limitations with broad, multi-source queries, while long-context models are computationally prohibitive. We introduce CacheNotes: Task-Aware Key-Value Cache Compression. Given a task description and a corpus, CacheNotes first generates a sequence of Compression-Planning-Tokens (CPTs), an offline task-focused distillation pass that identifies and organizes key information from the corpus. These CPTs are then used to guide a one-time compression of the corpus into a compact, reusable KV cache, which is then used alone at inference time to efficiently answer diverse, reasoning-intensive queries, eliminating repeated retrieval or context expansion. Experiments on LongBench show that CacheNotes outperforms RAG by over 8 F1 points on Question-Answering tasks at a 0.95 compression and reduces latency by over 4x. On RULER, it surpasses previous query-agnostic compression methods by 55 points, narrowing the gap to query-aware compression approaches. Additional results on real-world enterprise and synthetic datasets demonstrate its strong performance on multi-hop and broad-coverage queries.",Giulio Corallo; Orion Weller; Fabio Petroni; Paolo Papotti,Giulio Corallo,Oral,In-person,SALLE  LE LIXUS,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Efficiency, Scaling & NLP Systems","Retrieval, Grounding & External Knowledge (RAG)",compression; cache; cache compression; key value; reasoning intensive; corpus; task aware; queries; key; broad
1063-MAIN,Beyond Blind Following: Evaluating Robustness of LLM Agents under Imperfect Guidance,"Large language models (LLMs) have shown strong capabilities as task-solving agents across interactive domains. However, in complex environments, these agents may need to rely on auxiliary guidance to reduce the search space or make up for limited domain-specific knowledge. Such guidance includes human-provided manuals and demonstrations, retrieved examples from memory or external tools, high-level heuristics, and agent-acquired knowledge from prior interactions. However, this guidance may be imperfect. For example, due to changes in the environment, ambiguous or simplified language, or retrieval errors from external sources, guidance can be incomplete, outdated, or contextually mismatched, potentially causing errors or failures during task execution. To address this, we introduce MIRAGE, a benchmark for MeasurIng Robustness of LLM Agents under Imperfect GuidancE. MIRAGE includes procedurally generated environments in navigation, cooking, and gaming, where both the environment and the auxiliary guidance vary in fidelity and relevance. We further extend MIRAGE to realistic web tasks via WebArena, using noisy or underspecified instructions extracted from demonstrations. Our findings reveal critical failure modes in current LLM agents and motivate future work on improving their robustness under imperfect guidance.",Yao Fu; Ran Qiu; Xinhe Wang; Jacob Sansom; Sathvika Ayyappa Prabhu; Huijie Tang; Jaekyeom Kim; Sungryull Sohn; Honglak Lee,Yao Fu,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Retrieval, Grounding & External Knowledge (RAG)",guidance; imperfect; mirage; agents; llm agents; demonstrations; environment; auxiliary; includes; robustness
1078-MAIN,How Do LLMs Generate Contrastive Sentiments? A Mechanistic Perspective,"This paper presents a mechanistic investigation into the ability of large language models (LLMs) to generate contrastive sentiments. We define this task as transforming the sentiment of a given text (e.g., positive to negative) with minimal changes. We identify two core mechanisms: (1) a preservation mechanism that preserves the input sentiment, primarily mediated by specific attention heads, and (2) a sentiment transformation mechanism, which integrates a representation of the target sentiment label with the original valenced words using a circuit containing both MLP and attention layers. Building on these findings, we propose and validate a novel mechanistic intervention. By modifying key attention heads, we successfully steer the LLM toward more effective contrastive generation, significantly increasing the sentiment flip rate without sacrificing distance preservation. Our work not only deepens the understanding of how LLMs perform this task but also introduces a promising new direction for steering LLM behavior through targeted, mechanistic interventions.",Van Bach Nguyen; Christin Seifert; JГ¶rg SchlГ¶tterer,Van Bach Nguyen,Oral,In-person,SALLE  LE RIAD,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Interpretability & Model Analysis,,sentiment; mechanistic; contrastive; attention heads; attention; preservation; heads; llms generate; mechanism; steer llm
1083-MAIN,Continual Neural Topic Model,"In continual learning, our aim is to learn a new task without forgetting what was learned previously. In topic models, this translates to learning new topic models without forgetting previously learned topics. Previous work either considered Dynamic Topic Models (DTMs), which learn the evolution of topics based on the entire training corpus at once, or Online Topic Models, which are updated continuously based on new data but do not have long-term memory. To fill this gap, we propose the Continual Neural Topic Model (CoNTM), which continuously learns topic models at subsequent time steps without forgetting what was previously learned. This is achieved using a global prior distribution that is continuously updated. In our experiments, CoNTM consistently outperformed the dynamic topic model in terms of topic quality and predictive perplexity while being able to capture topic changes online. The analysis reveals that CoNTM can learn more diverse topics and better capture temporal changes than existing methods.",Charu Karakkaparambil James; Waleed Mustafa; Marcio Monteiro; Marius Kloft; Sophie Fellenz,Charu Karakkaparambil James,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Interpretability & Model Analysis,"Efficiency, Scaling & NLP Systems",topic; topic models; topic model; forgetting; continuously; previously; continual; topics; learned; learn
1086-MAIN,MAQuA: Multi-outcome Adaptive Question-Asking for Mental Health using Item Response Theory,"Recent advances in LLMs offer new opportunities for scalable, interactive mental health assessment, but excessive querying burdens users and is inefficient for real-world screening across transdiagnostic symptom profiles. We introduce MAQuA, a multi-outcome modeling and adaptive question-asking framework for simultaneous, multidimensional mental health screening. Combining multi-outcome modeling on language responses with item response theory (IRT) and factor analysis, MAQuA selects the questions with most informative responses across multiple dimensions at each turn to optimize diagnostic information, improving accuracy and potentially reducing response burden. Empirical results on a novel dataset reveal that MAQuA reduces the number of assessment questions required for score stabilization by 50вЂ“87% compared to random ordering (e.g., achieving stable depression scores with 71% fewer questions and eating disorder scores with 85% fewer questions). MAQuA demonstrates robust performance across both internalizing (depression, anxiety) and externalizing (substance use, eating disorder) domains, with early stopping strategies further reducing patient time and burden. These findings position MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive mental health screening, advancing the integration of LLM-based agents into real-world clinical workflows.",Vasudha Varadarajan; Hui Xu; Rebecca Astrid BГ¶hme; Mariam Marlen MirstrГ¶m; Sverker SikstrГ¶m; H. Schwartz,Vasudha Varadarajan,Oral,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Dialogue, Conversational & Interactive NLP",mental health; mental; screening; health; outcome; questions; eating; item response; response theory; item response theory
1091-MAIN,Principled Self-Correction in Discrete Diffusion: A UCB-Guided Framework for Text Generation,"Inspired by their success in image synthesis, diffusion models offer a flexible, iterative alternative to rigid left-to-right text generation. However, a fundamental training-inference discrepancy hinders their performance: models train on random noise but must correct their own structured, semantic errors during inference. To bridge this gap, we propose a unified framework. First, Deeper Self-Prediction (DSP) is a multi-step training objective to that teaches robust self-correction by forcing the model to denoise its own intermediate outputs. Second, UCB-guided Decoding is a principled inference algorithm that frames token re-masking as a multi-armed bandit problem, using the Upper Confidence Bound (UCB) to balance exploration and exploitation. Experiments on text generation tasks demonstrate consistent improvements over existing diffusion baselines. The framework achieves higher faithfulness and coherence according to both automatic metrics and LLM-as-a-Judge evaluations.",Masaki Asada; Makoto Miwa,Masaki Asada,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Summarization & Generation,"Reasoning, Planning & Agents",diffusion; text generation; self correction; self; correction; principled; inference; text; generation; guided
1092-MAIN,ConLID: Supervised Contrastive Learning for Low-Resource Language Identification,"Language identification (LID) is a critical step in curating multilingual LLM pretraining corpora from web crawls. While many studies on LID model training focus on collecting diverse training data to improve performance, low-resource languages -- often limited to single-domain data, such as the Bible -- continue to perform poorly. To resolve these class imbalance and bias issues, we propose a novel supervised contrastive learning (SCL) approach to learn domain-invariant representations for low-resource languages. We show that our approach improves LID performance on out-of-domain data for low-resource languages by 3.2 percentage points, while maintaining its performance for the high-resource languages.",Negar Foroutan; Jakhongir Saydaliev; Grace Kim; Antoine Bosselut,"Jakhongir Saydaliev, Negar Foroutan",Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multilinguality & Low-Resource NLP,"Trustworthy, Safety, Privacy & Fairness",lid; resource languages; resource; low resource; supervised contrastive; supervised contrastive learning; low resource languages; low; language identification; languages
1096-MAIN,CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations,"The medical adoption of NLP tools requires interpretability by end users, yet traditional XAI methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to (i) predict psychosis risk and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation and BERT classification, achieving above 90% accuracies across all three BERT variants, outperforming baseline models. Evaluation of explanation formats by twenty-eight clinical experts demonstrated strong preference for our novel concept-guided explanations, especially hybrid graph and text summary formats, over standard SHAP word-level bar plots and heat map visualisations. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.",Stephanie Fong; Guilherme C Oliveira; Xiangyu Zhao; Yiwen Jiang; Zimu Wang; Jiahe Liu; Beau-Luke Colton; Scott W. Woods; Martha Shenton; Barnaby Nelson; Zongyuan Ge; Dominic Dwyer,Stephanie Fong,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Domain NLP (Biomedical/Clinical/Legal/Scientific),Interpretability & Model Analysis,clinical; formats; shap; semi structured; international; clinician; semi; nlp; bert; explanation
1098-MAIN,"Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection","Logical fallacies are common in public communication and can mislead audiences; fallacious arguments may still appear convincing despite lacking soundness, because convincingness is inherently subjective. We present the first computational study of how emotional framing interacts with fallacies and convincingness, using large language models (LLMs) to systematically change emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting emotional appeal into fallacious arguments while preserving their logical structures, then use the best models to generate stimuli for a human study. Our results show that LLM-driven emotional framing reduces human fallacy detection in F1 by 14.5% on average. Humans perform better in fallacy detection when perceiving enjoyment than fear or sadness, and these three emotions also correlate with significantly higher convincingness compared to neutral or other emotion states. Our work has implications for AI-driven emotional manipulation in the context of fallacious argumentation.",Yanran Chen; Lynn Greschner; Roman Klinger; Michael Klenk; Steffen Eger,Yanran Chen,Oral,In-person,SALLE  LE RIAD,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",,emotional; fallacy; framing; arguments; fallacies; ai driven; driven; detection; logical; human
1099-MAIN,"Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization","Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarityвЂ”qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zeroвЂ‘shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at <redacted>.",Mizanur Rahman; Mohammed Saidul Islam; Md Tahmid Rahman Laskar; Shafiq Joty; Enamul Hoque,Mizanur Rahman,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Summarization & Generation,Multimodal & Speech/Audio,visualization; execution; code; relative; post; execution feedback; reinforcement learning framework; executable; multi objective; learning framework
1100-MAIN,Offline Preference Optimization via Maximum Marginal Likelihood Estimation,"Aligning Large Language Models (LLMs) with human preferences is crucial, but standard methods like Reinforcement Learning from Human Feedback (RLHF) are often complex and unstable. In this work, we propose a new, simpler approach that recasts alignment through the lens of Maximum Marginal Likelihood (MML) estimation. Our new MML-based Preference Optimization (MMPO) maximizes the marginal log-likelihood of a preferred text output, using the preference pair as samples for approximation, and forgoes the need for both an explicit reward model and entropy maximization. We theoretically demonstrate that MMPO implicitly performs preference optimization, producing a weighted gradient that naturally up-weights chosen responses over rejected ones. Across models ranging from 135M to 8B parameters, we empirically show that MMPO: 1) is more stable with respect to the hyperparameter compared to alternative baselines, and 2) achieves competitive or superior preference alignment while better preserving the base modelвЂ™s general language capabilities. Through a series of ablation experiments, we show that this improved performance is indeed attributable to MMPO's implicit preference optimization within the gradient updates.",Saeed Najafi; Alona Fyshe,Saeed Najafi,Oral,In-person,SALLE  LE LIXUS,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Dialogue, Conversational & Interactive NLP","Trustworthy, Safety, Privacy & Fairness",preference; preference optimization; marginal; likelihood; optimization; maximum; gradient; estimation; series ablation; recasts
1102-MAIN,The Relevance of Value Systems for Offensive Language Detection,"We examine in how far a person's value system has an impact on their perception of offensiveness. For instance, a scholar is likely to be offended by being accused of reporting unverified claims whereas many non-scholars would not feel that way. Thus, we move away from the assumption that offensiveness can be defined through a universal perspective. Ultimately, such research aims to support personalized approaches to content moderation. Our main contribution is the introduction of a dataset consisting of neutrally-phrased sentences on controversial topics, evaluated by individuals from 4 different value systems. This allows us to identify offensiveness patterns across value systems and conduct classification experiments.",Michael Wiegand; Elisabeth Eder; Josef Ruppenhofer,Michael Wiegand,Oral,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",,value systems; value; systems; feel; offensive language detection; classification experiments; unverified; language detection; offensive language; away
1104-MAIN,Instruction Tuning with and without Context: Behavioral Shifts and Downstream Impact,"Instruction tuning is a widely used approach to improve the instruction-following ability of large language models (LLMs). Instruction-tuning datasets typically include a mixture of context-augmented and context-free examples, yet prior work has largely combined these data types without examining their distinct effects. In this paper, we investigate how training LLMs with or without context affects model behavior and downstream performance. First, in the text domain, we show that LLMs trained with context attend more strongly to the provided knowledge, achieving better grounding. We also observe that context-augmented training shifts how LLMs use knowledge: models store and leverage less on parametric knowledge and instead depend more on the provided context. Second, we observe that using LLM trained with context-augmented data as the backbone for vision-language models reduces hallucination and improves grounding in the visual domain. Finally, we explore practical strategies for real-world deployments where context availability varies. We show that maintaining separate context-augmented and context-free models and routing inputs between them yields more robust overall performance than training a single mixed model, as it better preserves their complementary strengths.",Hyunji Lee; Seunghyun Yoon; Yunjae Won; Hanseok Oh; Geewook Kim; Trung Bui; Franck Dernoncourt; Elias Stengel-Eskin; Mohit Bansal; Minjoon Seo,Hyunji Lee,Oral,In-person,SALLE  LA PALMERAIE,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",Multimodal & Speech/Audio,context; instruction tuning; instruction; augmented; provided; shifts; observe; tuning; free; grounding
1108-MAIN,RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models,"The ability of language models in RAG systems to selectively refuse to answer based on flawed context is critical for safety, yet remains a significant failure point. Our large-scale study reveals that even frontier models struggle in this setting, with refusal accuracy dropping below 50% on multi-document tasks, while exhibiting dangerous over-confidence or over-caution. Static benchmarks fail to reliably evaluate this capability, as models exploit dataset-specific artifacts and memorize test instances. We introduce RefusalBench, a generative methodology that programmatically creates diagnostic test cases through controlled linguistic perturbation. Our framework employs 176 distinct perturbation strategies across six categories of informational uncertainty and three intensity levels. Evaluation of over 30 models uncovers systematic failure patterns: refusal comprises separable detection and categorization skills, and neither scale nor extended reasoning improves performance. We find that selective refusal is a trainable, alignment-sensitive capability, offering a clear path for improvement. We release two benchmarksвЂ”RefusalBench-NQ (single-document) and RefusalBench-GaRAGe (multi-document), and our complete generation framework to enable continued, dynamic evaluation of this critical capability.",Aashiq Muhamed; Leonardo F. R. Ribeiro; Markus Dreyer; Virginia Smith; Mona T. Diab,Aashiq Muhamed,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",refusal; capability; multi document; document; perturbation; selective; failure; generative; test; evaluation
1109-MAIN,Query Decomposition for RAG: Balancing Exploration-Exploitation,"Retrieval-augmented generation (RAG) systems address complex user requests by decomposing them into subqueries, retrieving potentially relevant documents for each, and then aggregating them to generate an answer. Efficiently selecting informative documents requires balancing a key trade-off: (i) retrieving broadly enough to capture all the relevant material, and (ii) limiting retrieval to avoid excessive noise and computational cost. We formulate query decomposition and document retrieval in an exploitation-exploration setting, where retrieving one document at a time builds a belief about the utility of a given sub-query and informs the decision to continue exploiting or exploring an alternative. We experiment with a variety of bandit learning methods and demonstrate their effectiveness in dynamically selecting the most informative sub-queries. Our main finding is that estimating document relevance using rank information and human judgments yields a 35% gain in document-level precision, 15% increase in О±-nDCG, and better performance on the downstream task of long-form generation. Code is available on GitHub.",Roxana Petcu; Kenton Murray; Daniel Khashabi; Evangelos Kanoulas; Maarten de Rijke; Dawn Lawrie; Kevin Duh,Roxana Petcu,Oral,In-person,Pavillon  DE RABAT,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,retrieving; document; selecting informative; exploitation; query; sub; decomposition; selecting; exploration; balancing
1115-MAIN,Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation in VLMs,"Vision-Language Models (VLMs) have shown strong multimodal reasoning capability on Visual-Question-Answering (VQA) benchmarks. However, their robustness against textual misinformation remains under-explored. While existing research has extensively studied the effect of misinformation in text-only domains, it is not clear how VLMs arbitrate between contradictory information from different modalities. To bridge the gap, we first propose the ConText-VQA (i.e. Conflicting Text) dataset, consisting of image-question pairs together with systematically generated persuasive prompts that deliberately conflict with visual evidence. Then, a thorough testing framework is designed and executed to benchmark the susceptibility of various models to these conflicting textual inputs. Comprehensive experiments over 11 state-of-the-art VLMs reveal that these models are indeed vulnerable to misleading prompts, often overriding clear visual evidence in favor of the conflicting text, and show an average performance drop of over 48.2% after only one round of persuasive conversation. Our findings highlight a critical limitation in current VLMs and underscore the need for improved robustness against textual manipulation.",Chi Zhang; Wenxuan Ding; Jiale Liu; Mingrui Wu; Qingyun Wu; Ray Mooney,Chi Zhang,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multimodal & Speech/Audio,,vlms; misinformation; conflicting; textual; visual evidence; persuasive; vqa; visual; clear; effect
1116-MAIN,Sycophancy Hides Linearly in the Attention Heads,"We find that correct-to-incorrect sycophancy signals are most linearly accessible within multi-head attention activations. Motivated by the linear representation hypothesis, we train linear probes across the residual stream, multilayer perceptron (MLP), and attention layers to analyze where these signals emerge. Although separability appears in the residual stream and MLPs, steering using these probes is most effective in a sparse subset of middle-layer attention heads. Using TruthfulQA as the base dataset, we find that probes trained on it transfer effectively to other factual QA benchmarks. Furthermore, comparing our discovered direction to previously identified вЂњtruthfulвЂќ directions reveals limited overlap, suggesting that factual accuracy, and deference resistance, arise from related but distinct mechanisms. Attention-pattern analysis further indicates that the influential heads attend disproportionately to expressions of user doubt, contributing to sycophantic shifts. Overall, these findings suggest that sycophancy can be mitigated through simple, targeted linear interventions that exploit the internal geometry of attention activations. Code will be released upon publication.",Rifo Ahmad Genadi; Munachiso Samuel Nwadike; Nurdaulet Mukhituly; Tatsuya Hiraoka; Hilal AlQuabeh; Kentaro Inui,Rifo Ahmad Genadi,Oral,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,,attention; sycophancy; heads; probes; linear; stream; linearly; residual; attention heads; activations
1122-MAIN,AICD Bench: A Challenging Benchmark for AI-Generated Code Detection,"Large language models (LLMs) are increasingly capable of generating functional source code, raising questions about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce AICD Bench, the most comprehensive benchmark for AI-generated code detection. It spans 2M samples, 77 models across 11 families, and 9 programming languages, including new reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: (i) Robust Binary Classification under distribution shifts in language and domain; (ii) Model Family Attribution, grouping generators by architectural lineage; and (iii) Fine-Grained Human-Machine Classification across human, machine, hybrid, and adversarial code. We evaluate neural and classical detectors and show that performance remains far below practical usability, underscoring task difficulty. We release a unified, challenging evaluation suite to drive the next generation of robust approaches for AI-generated code detection",Daniil Orel; Dilshod Azizov; Indraneil Paul; Yuxia Wang; Iryna Gurevych; Preslav Nakov,Daniil Orel,Oral,In-person,SALLE  LE RIAD,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",generated code; ai generated; human machine; code; bench; detection; generated; machine; binary; classification
1123-MAIN,Safeguarding Language Models via Self-Destruct Trapdoor,"The potential misuse and misalignment of language models (LMs) is a central safety concern. This work presents Self-Destruct, a novel mechanism to restrict specific behaviors in LMs by leveraging overlooked properties of the underlying hardware. We observe that the LM frameworks use limited-precision formats (e.g., FP32), which are vulnerable to overflow errors during matrix multiplications. Exploiting this property, Self-Destruct replaces selected weights in pre-trained LM layers with values that act as traps, triggering a system error only when the model engages in targeted behaviors, such as harmful text generation, while leaving normal functionality unaffected. Unlike post-hoc filters, this safeguard is embedded directly within the model, introduces neither inference overhead nor auxiliary models, and requires only a set of examples for calibration. Experiments on Llama-3.2 and Qwen-2.5 demonstrate that Self-Destruct provides competitive protection against jailbreak attacks while preserving accuracy on standard benchmarks. Our results highlight the potential of hardware-aware safeguards as an efficient, low-overhead complement to existing LM defenses.",Shahar Katz; Bar Alon; Ariel Shaulov; Lior Wolf; Mahmood Sharif,Bar Alon,Oral,In-person,SALLE  LE RIAD,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness",Summarization & Generation,self; hardware; lms; overhead; behaviors; potential; triggering; set examples; benchmarks results; overflow
1124-MAIN,"Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity","Large language models (LLMs) are known to ""hallucinate"" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.",Prakhar Ganesh; Reza Shokri; Golnoosh Farnadi,Prakhar Ganesh,Oral,In-person,SALLE  LE RIAD,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics","Retrieval, Grounding & External Knowledge (RAG)",multiplicity; harms; consistency; hallucination; mitigation; detection mitigation; correctness; prompt; detection; hallucinations
1126-MAIN,Hype or not? Formalizing Automatic Promotional Language Detection in Biomedical Research,"In science, promotional language ('hype') is increasing and can undermine objective evaluation of evidence, impede research development, and erode trust in science. In this paper, we introduce the task of automatic detection of hype, which we define as hyperbolic or subjective language that authors use to glamorize, promote, embellish, or exaggerate aspects of their research. We propose formalized guidelines for identifying hype language and apply them to annotate a portion of the National Institutes of Health (NIH) grant application corpus. We then evaluate traditional text classifiers and language models on this task, comparing their performance with a human baseline. Our experiments show that formalizing annotation guidelines can help humans reliably annotate candidate hype adjectives and that using our annotated dataset to train machine learning models yields promising results. Our findings highlight the linguistic complexity of the task and the potential need for domain knowledge. While some linguistic works address hype detection, to the best of our knowledge, we are the first to approach it as a natural language processing task.",Bojan Batalo; Erica K. Shimomoto; Dipesh Satav; Neil Millar,Bojan Batalo,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"LLM Evaluation, Benchmarks & Metrics",formalizing; annotate; guidelines; detection; research; language; automatic; science; processing task; knowledge approach
1131-MAIN,"H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs","The alignment of pre-trained LLMs continues to draw significant attention from both industry and academia, aiming to ensure responses that are helpful, harmless, and honest. However, identifying a point in the modelвЂ™s representation subspace that simultaneously satisfies all these properties remains challenging. H3Fusion addresses this challenge by introducing a mixture-of-experts (MoE)-based fusion mechanism that models alignment as a controllable drift within the subspace, guided by a drift-regularization loss to balance competing alignment dimensions. Furthermore, we formulate the alignment by finding a dual objective of harnessing the distance of generated embeddings and alignment embeddings, and introduce a gating loss by canalizing the activations on the contributing experts. Extensive evaluations of three benchmark datasets show that H3Fusion is more helpful, less harmful, and more honest in three aspects: it outperforms each individually aligned model by 11.37, and provides stronger robustness compared to the state-of-the-art LLM ensemble approaches by 13.77 and model-merging approaches by 6.18. Code is available at https://anonymous.4open.science/r/h3fusion-F45E/.",Selim Furkan Tekin; Fatih Ilhan; Sihao Hu; Tiansheng Huang; Yichang Xu; Zachary Yahn; Ling Liu,Selim Furkan Tekin,Oral,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Trustworthy, Safety, Privacy & Fairness",honest; helpful; alignment; helpful harmless honest; harmless honest; harmless; helpful harmless; drift; subspace; fusion
1132-MAIN,Revisiting Generalization Across Difficulty Levels: ItвЂ™s Not So Easy,"We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",Yeganeh Kordi; Nihal V. Nayak; Max Zuo; Ilana Nguyen; Stephen Bach,Nihal V. Nayak,Oral,In-person,SALLE  LE LIXUS,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",Interpretability & Model Analysis,difficulty; difficulties; different llms; easier harder; easier; harder; generalization; easy; different; llms
1139-MAIN,BLUR: A Bi-Level Optimization Approach for LLM Unlearning,"Enabling large language models (LLMs) to unlearn knowledge and capabilities acquired during training has proven vital for ensuring compliance with data regulations and promoting ethical practices in generative AI. Although there are growing interests in developing various unlearning algorithms, it remains unclear how to best formulate the unlearning problem. The most popular formulation uses a weighted sum of forget and retain loss, but it often leads to performance degradation due to the inherent trade-off between forget and retain losses. In this work, we argue that it is important to model the hierarchical structure of the unlearning problem, where the forget problem (which \textit{unlearns} certain knowledge and/or capabilities) takes priority over the retain problem (which preserves model utility). This hierarchical structure naturally leads to a bi-level optimization formulation where the lower-level objective focuses on minimizing the forget loss, while the upper-level objective aims to maintain the model's utility. Based on this new formulation, we propose a novel algorithm, termed Bi-Level UnleaRning (\texttt{BLUR}), which not only possesses strong theoretical guarantees but more importantly, delivers superior performance. In particular, our extensive experiments demonstrate that \texttt{BLUR} consistently outperforms all the state-of-the-art algorithms across various unlearning tasks, models, and metrics.",Hadi Reisizadeh; Jinghan Jia; Zhiqi Bu; Bhanukiran Vinzamuri; Anil Ramakrishna; Kai-Wei Chang; Volkan Cevher; Sijia Liu; Mingyi Hong,Hadi Reisizadeh,Oral,In-person,Pavillon  DE RABAT,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)",,unlearning; retain; blur; formulation; level; problem; model utility; hierarchical structure; knowledge capabilities; level optimization
1148-MAIN,DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal Understanding,"Hyperscaling of data and parameter count in LLMs is yielding diminishing improvement, especially when weighed against training costs, underlining a growing need for more efficient finetuning and inference without sacrificing performance. This is especially so for multimodal models (MLMs), where the overhead of processing multimodal tokens alongside language data often limits the practical viability of these systems. Parallely, recent work has uncovered implicit cross-modal alignment in the deeper layers of large MLMs, improving our understanding of how MLMs process and encode information. Motivated by this, and our observation that MLMs naturally defer most cross-modal token interactions to deeper layers of the model, we propose a simple modification. Instead of concatenation with the language prompt at the start, we insert multimodal tokens directly into the middle, allowing them to entirely bypass the early layers. Our results with diverse modalities, (i) LLaVA & BLIP for vision, (ii) LTU for audio, and (iii) MoLCA for molecular data, and broad model sizes, starting from 350M and going upto 13B parameters, indicate that our method reduces both training and inference costs, while at least preserving, if not surpassing the performance of existing baselines.",Moulik Choraria; Xinbo Wu; Akhil Bhimaraju; Nitesh Sekhar; Yue Wu; Xu Zhang; Prateek Singhal; Lav R. Varshney,Moulik Choraria,Oral,In-person,SALLE  LE RIAD,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Multimodal & Speech/Audio,"Efficiency, Scaling & NLP Systems",mlms; multimodal; layers; bypass; cross modal; deeper; early; modal; costs; especially
1151-MAIN,Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory,"Despite their impressive performance on complex tasks, current language models (LMs) typically operate in a vacuum: Each input query is processed separately, without retaining insights from previous attempts. Here, we present Dynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM with a persistent, evolving memory. Rather than repeatedly re-discovering or re-committing the same solutions and mistakes, DC enables models to store and reuse accumulated strategies, code snippets, and general problem-solving insights at inference time. This test-time learning enhances performance substantially across a range of tasks without needing explicit ground-truth labels or human feedback. Leveraging DC, Claude 3.5 SonnetвЂ™s accuracy more than doubled on AIME math exams once it began retaining algebraic insights across questions. Similarly, GPT-4oвЂ™s success rate on the Game of 24 puzzle increased from about 10% to 99% after the model discovered and reused a Python-based solution. In tasks prone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o and Claude to reach near-perfect accuracy by recalling previously validated code, whereas their baselines stagnated around 50%. Beyond arithmetic challenges, DC yields notable accuracy gains on knowledge-demanding tasks. Claude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro Engineering and Physics problems. Crucially, DCвЂ™s memory is self-curated, focusing on concise, transferable snippets rather than entire transcripts, thereby facilitating meta-learning and avoiding context ballooning. Unlike fine-tuning or static retrieval methods, DC adapts LMsвЂ™ problem-solving skills on the fly, without modifying their underlying parameters, and offers a practical approach for continuously refining responses and cutting routine errors. Overall, our findings present DC as a promising approach for augmenting LMs with persistent memory, bridging the divide between isolated inference events and the cumulative, experience-driven learning characteristic of human cognition.",Mirac Suzgun; Mert Yuksekgonul; Federico Bianchi; Dan Jurafsky; James Zou,Mirac Suzgun,Oral,In-person,Pavillon  DE RABAT,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems","Retrieval, Grounding & External Knowledge (RAG)",claude; memory; mistakes; test time learning; time learning; snippets; retaining; learning; insights; arithmetic
1153-MAIN,Evidential Semantic Entropy for LLM Uncertainty Quantification,"Quantifying uncertainty in large language models (LLMs) is crucial for applications where safety is a concern, as it helps identify factually incorrect LLM answers, commonly referred to as hallucinations. Recently, advancements have been made in quantifying uncertainty, specifically by incorporating the semantics of sampled answers to estimate entropy. These methods typically rely on a normalized probability that is calculated using a limited number of sampled answers. However, we note these estimation methods fail to account for the effects of the semantics that are possible to be obtained as answers, but are not observed in the sample. This is a significant oversight, since a heavier tail of unobserved answer probabilities indicates a higher level of overall uncertainty. To alleviate this issue, we propose Evidential Semantic Entropy (EVSE), which leverages evidence theory to represent both total ignorance arising from unobserved answers and partial ignorance stemming from the semantic relationships among the observed answers. Experiments show that EVSE significantly improves uncertainty quantification performance. Our code is available at: \url{https://anonymous.4open.science/r/EvidentialSemanticEntropy-6371}.",Lucie Kunitomo-Jacquin; Edison Marrese-Taylor; Ken Fukuda; Masahiro Hamasaki,Lucie Kunitomo-Jacquin,Oral,In-person,SALLE  WALILI,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics","Linguistics, Syntax & Semantics",answers; uncertainty; entropy; semantic entropy; uncertainty quantification; sampled; quantification; quantifying; observed; semantic
1154-MAIN,LLMs Know More About Numbers than They Can Say,"We show that despite their ability to solve mathematical problems, state-of-the-art LLMs such as GPT-4o struggle with seemingly simpler numerical comparisons when they mix notations: ""Which is larger, $5.7 \times 10^2$ or $580$?’' This raises a fundamental question: do LLMs even know how big these numbers are? We linearly probe several LLMs' hidden states, and find that they internally encode numerals to about 2.7\% relative error (on restricted synthetic text) or 22\% (on scientific papers), representing the log-magnitudes of both decimal and scientific-notation in the same 1-dimensional subspace. Furthermore, the hidden state after reading a *pair* of numerals encodes their *ranking* with over 90\% accuracy. Yet surprisingly, when explicitly asked to rank pairs of numerals, LLMs achieve only up to 70\% accuracy, and models with less effective probes show even worse performance. Finally, we show that incorporating classification probing loss as an auxiliary objective during finetuning brings an additional 9.18\% improvement in verbalized accuracy over base models, demonstrating that improving models' internal magnitude representations can enhance their numerical reasoning capabilities.",Fengting Yuchi; Li Du; Jason Eisner,Fengting Yuchi,Oral,In-person,SALLE  LE RIAD,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Domain NLP (Biomedical/Clinical/Legal/Scientific),Interpretability & Model Analysis,llms know; numbers; know; numerical; hidden; scientific; llms; accuracy; 18 improvement; ability solve
1157-MAIN,SCENEBench: An Audio Understanding Benchmark Grounded in Assistive and Industrial Use Cases,"Advances in large language models (LLMs) have enabled significant capabilities in audio processing, resulting in state-of-the-art models now known as Large Audio Language Models (LALMs). However, minimal work has been done to measure audio understanding beyond automatic speech recognition (ASR). This paper closes that gap by proposing a benchmark suite, SCENEBench (Spatial, Cross-lingual, Environmental, Non-speech Evaluation), that targets a broad form of audio comprehension across four real-world categories: background sound understanding, noise localization, cross-linguistic speech understanding, and vocal characterizer recognition. In addition to performance, we also measure model latency. The purpose of this benchmark suite is to assess the audio beyond just what words are said--- rather, in how they are said and the non-speech components of the audio. To strengthen ecological validity, we include a small human-recorded evaluation split per category. Based on the needs articulated by audio understanding use-cases of accessibility technology and industrial noise monitoring, this benchmark reveals critical gaps in current LALMs. The performance in each task is quite varied, with some tasks having performance far below random chance and others with high accuracy. We also provide a structured error taxonomy to characterize standard failure modes across tasks. These results provide direction for targeted improvements in model capabilities.",Laya Iyer; Angelina Wang; Sanmi Koyejo,Laya Iyer,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",audio; audio understanding; speech; understanding; said; non speech; benchmark suite; lalms; use cases; industrial
1158-MAIN,Incentivizing Strong Reasoning from Weak Supervision,"Large language models (LLMs) have demonstrated impressive performance on reasoning-intensive tasks, but enhancing their reasoning abilities typically relies on either reinforcement learning (RL) with verifiable signals or supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT) demonstrations, both of which are expensive. In this paper, we study a novel problem of incentivizing the reasoning capacity of LLMs without expensive high-quality demonstrations and reinforcement learning. We investigate whether the reasoning capabilities of LLMs can be effectively incentivized via supervision from significantly weaker models. We further analyze when and why such weak supervision succeeds in eliciting reasoning abilities in stronger models. Our findings show that supervision from significantly weaker reasoners can substantially improve student reasoning performance, recovering close to 94\% of the gains of expensive RL at a fraction of the cost. Experiments across diverse benchmarks and model architectures demonstrate that weak reasoners can effectively incentivize reasoning in stronger student models, consistently improving performance across a wide range of reasoning tasks. Our results suggest that this simple weak-to-strong paradigm is a promising and generalizable alternative to costly methods for incentivizing strong reasoning capabilities at inference-time in LLMs. Code is at https://github.com/W2SR-ARR/Code.",Yige Yuan; Teng Xiao; Shuchang Tao; Xue Wang; Jinyang Gao; Bolin Ding; Bingbing Xu,Yige Yuan,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",reasoning; weak; incentivizing; supervision; expensive; reasoners; strong reasoning; weaker; demonstrations; reasoning abilities
1161-MAIN,DivMerge: A divergence-based model merging method for multi-tasking,"Merging fine-tuned models is a promising alternative to costly multi-task training, but task interference remains a challenge, especially as the number of tasks grows. We present DivMerge, a reference-free method that merges models trained on different tasks by minimizing Jensen-Shannon divergence between their outputs and those of the merged model, automatically balancing task importance. While the method exhibits strong theoretical properties, experiments on classification and generative tasks with autoregressive models show that DivMerge consistently outperforms prior work, and remains robust when scaling to more tasks.",Brahim Touayouch; LoГЇc Fosse; GГ©raldine Damnati; GwГ©nolГ© LecorvГ©,Loïc Fosse,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Interpretability & Model Analysis,,divergence; merging; tasks; reference free; costly multi; divergence based; merging fine; shannon; alternative costly; strong theoretical
1163-MAIN,A Reinforcement Learning Framework for Robust and Secure LLM Watermarking,"Watermarking has emerged as a promising solution for tracing and authenticating text generated by large language models (LLMs). A common approach to LLM watermarking is to construct a green/red token list and assign higher or lower generation probabilities to the corresponding tokens, respectively. However, most existing watermarking algorithms rely on heuristic green/red token list designs, as directly optimizing the list design with techniques such as reinforcement learning (RL) comes with several challenges. First, desirable watermarking involves multiple criteria, i.e., detectability, text quality, robustness against removal attacks, and security against spoofing attacks. Directly optimizing for these criteria introduces many partially conflicting reward terms, leading to an unstable convergence process. Second, the vast action space of green/red token list choices is susceptible to reward hacking. In this paper, we propose an end-to-end RL framework for robust and secure LLM watermarking. Our approach adopts an anchoring mechanism for reward terms to ensure stable training and introduces additional regularization terms to prevent reward hacking. Experiments on standard benchmarks with two backbone LLMs show that our method achieves a state-of-the-art trade-off across all criteria, with notable improvements in resistance to spoofing attacks without degrading other criteria.",Li An; Yujian Liu; Yepeng Liu; Yuheng Bu; Yang Zhang; Shiyu Chang,Li An,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Summarization & Generation,watermarking; list; criteria; llm watermarking; reward; red; spoofing; attacks; terms; framework robust
1164-MAIN,Agent-Testing Agent: A Meta-Agent for Automated Testing and Evaluation of Conversational AI Agents,"LLM agents are increasingly deployed to plan, retrieve, and write with tools, yet evaluation still leans on static benchmarks and small human studies. We present the Agent-Testing Agent (ATA), a meta-agent that combines static code analysis, developer interrogation, literature mining, and persona-driven adversarial test generation whose difficulty adapts via judge feedback. Each dialogue is scored with an LLM-as-a-Judge (LAAJ) rubric and used to steer subsequent tests toward the agent's weakest capabilities. On a travel planner and a Wikipedia writer, the ATA surfaces more diverse and severe failures than expert annotators while matching severity, and finishes in 20--30 minutes versus ten-annotator rounds that took days. Ablating code analysis and web search increases variance and miscalibration, underscoring the value of evidence-grounded test generation. The ATA outputs quantitative metrics and qualitative bug reports for developers. We release the full open-source implementation.",Sameer Komoravolu; Khalil Mrini,Unknown,Oral,In-person,SALLE  WALILI,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Dialogue, Conversational & Interactive NLP","Reasoning, Planning & Agents",agent; ata; testing; test generation; meta agent; meta; judge; static; test; agents
1166-MAIN,User-Centric Evidence Ranking for Attribution and Fact Verification,"Attribution and fact verification are critical challenges in natural language processing for assessing information reliability. While automated systems and Large Language Models (LLMs) aim to retrieve and select concise evidence to support or refute claims, current inaccuracies often present users with either insufficient or overwhelming information, leading to inefficient and error-prone verification. To address this, we propose User-Centric Evidence Ranking, a novel task that prioritizes presenting sufficient information as early as possible in a ranked list, aligning with a userвЂ™s sequential verification process. This minimizes user reading effort while keeping all candidate evidence accessible. We introduce a novel evaluation framework inspired by information retrieval metrics and construct a unified benchmark by aggregating existing fact verification datasets. Our extensive experiments with diverse models show that incremental ranking strategies better capture complementary evidence and that LLM-based methods outperform shallower baselines, while still facing challenges in balancing sufficiency and redundancy. A supporting user study further confirms that incremental ranking reduces reading effort and improves verification outcomes compared to standard evidence selection. This work establishes a foundational step toward more interpretable, efficient, and user-aligned verification systems.",Guy Alt; Eran Hirsch; Serwar Basch; Ido Dagan; Oren Glickman,Guy Alt,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Retrieval, Grounding & External Knowledge (RAG)",verification; evidence; ranking; fact verification; user; fact; evidence ranking; reading; information; effort
1170-MAIN,Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language,"We present a comprehensive evaluation of large language modelsвЂ™ (LLMs) ability to process culturally grounded language, specifically to understand and pragmatically use figurative expressions that encode local knowledge and social nuance. Using figurative language as a proxy for cultural nuance and local knowledge, we design evaluation tasks for contextual understanding, pragmatic use, and connotation interpretation across Arabic and English. We evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms, multidialectal Arabic proverbs, and English proverbs. Results show a consistent hierarchy: accuracy on Arabic proverbs is 4.29% lower than on English proverbs, and performance on Egyptian idioms is 10.28% lower than on Arabic proverbs. On the pragmatic use task, accuracy drops by 14.07% relative to understanding, though providing idiomsвЂ™ contextual sentences improves accuracy by 10.66%. Models also struggle with connotative meaning, reaching at most 85.58% agreement with human annotators on idioms with full inter-annotator agreement. Figurative language thus serves as an effective diagnostic for cultural reasoning, revealing that while LLMs often interpret figurative meaning, they still face major challenges in using it appropriately. To support future research, we release Kinayat, the first dataset of Egyptian Arabic idioms designed for both figurative understanding and pragmatic use evaluation.",Mena Attia; Aashiq Muhamed; Mai Alkhamissi; Thamar Solorio; Mona T. Diab,Mena Attia,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",figurative; proverbs; arabic; pragmatic; idioms; egyptian; figurative language; egyptian arabic; cultural; use
1178-MAIN,VietMix: A Naturally-Occurring Parallel Corpus and Augmentation Framework for Vietnamese-English Code-Mixed Machine Translation,"Machine translation (MT) systems universally degrade when faced with code-mixed text. This problem is more acute for low-resource languages that lack dedicated parallel corpora. This work directly addresses this gap for Vietnamese-English, a language context characterized by challenges including orthographic ambiguity and the frequent omission of diacritics in informal text. We introduce VietMix, the first expert-translated, naturally occurring parallel corpus of Vietnamese-English code-mixed text. We establish VietMix's utility by developing a data augmentation pipeline that leverages iterative fine-tuning and targeted filtering. Experiments show that models augmented with our data outperform strong back-translation baselines by up to +3.5 xCOMET points and improve zero-shot models by up to +11.9 points. Our work delivers a foundational resource for a challenging language pair and provides a validated, transferable framework for building and augmenting corpora in other low-resource settings.",Hieu Tran; Phuong-Anh Nguyen-Le; Huy Nghiem; Quang-Nhan Nguyen; Wei Ai; Marine Carpuat,Phuong-Anh Nguyen-Le,Oral,In-person,SALLE  LE LIXUS,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Machine Translation,Multilinguality & Low-Resource NLP,code mixed; vietnamese; mixed; mixed text; parallel; english code; naturally occurring; parallel corpus; translation; occurring
1183-MAIN,Do You See Me?: A Diagnostic Benchmark for Evaluating Visual Perception in Multimodal Language Models,"Multimodal Large Language Models (MLLMs) show reasoning promise, yet their visual perception is a critical bottleneck. Paradoxically, MLLMs sometimes produce correct answers while misinterpreting crucial visual elements, masking these underlying perception failures. Our preliminary analysis on a joint perception-reasoning dataset revealed that 29% of correct reasoning answers from a leading MLLM contained perception errors. To systematically study visual perception abilities of MLLMs, we introduce \textbf{Do You See Me}- a scalable, programmatically generated benchmark with 1758 images and 2612 questions across seven core subtasks spanning 2D and 3D variants (twelve total tasks) providing parametric control over difficulty levels. The benchmark tasks are inspired by human psychology. Our evaluation of eleven leading MLLMs reveals a stark deficit: humans achieve 96.49% accuracy, while top MLLMs average below 50%. This performance gap widens drastically as task complexity increases. Further diagnostics show: (1) supervised finetuning offers only modest gains, (2) models tend to exploit task вЂњshortcutsвЂќ like MCQ formats over detailed visual analysis, and (3) Chain-of-Thought prompting can degrade complex visual tasks by verbalizing images into lossy text. These findings expose the foundational perception limits in current MLLMs and highlight the need for robust visual perception improvements in MLLMs. The benchmark dataset, source code and evaluation scripts are available at\footnote{\url{https://anonymous.4open.science/r/DoYouSeeMe-F52E/README.md}}.",Aditya Sanjiv Kanade; Tanuja Ganu,Aditya Kanade,Oral,In-person,Pavillon  DE RABAT,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",perception; mllms; visual perception; visual; benchmark; correct; images; answers; leading; multimodal
1185-MAIN,An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents,"Large Language Models (LLMs) increasingly mediate our social, cultural, and political interactions. While they can simulate some aspects of human behavior and decision-making, it is still underexplored whether repeated interactions with other agents amplify their biases or lead to exclusionary behaviors. To this end, we study Chirper.aiвЂ”an LLM-driven social media platformвЂ”analyzing 7M posts and interactions among 32K LLM agents over a year. We start with homophily and social influence among LLMs, learning that similar to humans', their social networks exhibit these fundamental phenomena. Next, we study the toxic language of LLMs, its linguistic features, and their interaction patterns, finding that LLMs show different structural patterns in toxic posting than humans. After studying the ideological leaning in LLMs posts, and the polarization in their community, we focus on how to prevent their potential harmful activities. We present a simple yet effective method, called Chain of Social Thought (CoST), that reminds LLM agents to avoid harmful posting.",Farnoosh Hashemi; Michael Macy,Farnoosh Hashemi,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Linguistics, Syntax & Semantics",social; agents; posting; interactions; toxic; posts; llm agents; humans; harmful; behaviors
1186-MAIN,Detecting Subtle Biases: An Ethical Lens on Underexplored Areas in AI Language Models Biases,"Large Language Models (LLMs) are increasingly embedded in the daily lives of individuals across diverse social classes. This widespread integration raises urgent concerns about the subtle, implicit biases these models may contain. In this work, we investigate such biases through the lens of ethical reasoning, analyzing model responses to scenarios in a new dataset we propose comprising 1,020 scenarios, systematically categorized into ethical, unethical, and neutral types. Our study focuses on dimensions that are socially influential but less explored, including (i) residency status, (ii) political ideology, (iii) Fitness Status, (iv) educational attainment, and (v) attitudes toward AI. To assess LLMsвЂ™ behavior, we propose a baseline and employ one statistical test and one metric: a permutation test that reveals the presence of bias by comparing the probability distributions of ethical/unethical scenarios with the probability distribution of neutral scenarios on each demographic group, and a tendency measurement that captures the magnitude of bias with respect to the relative difference between probability distribution of ethical and unethical scenarios. Our evaluations of 12 prominent LLMs reveal persistent and nuanced biases across all four attributes, and Llama models exhibited the most pronounced biases. These findings highlight the need for refined ethical benchmarks and bias-mitigation tools in LLMs.",Shayan Bali; Farhan Farsi; Mohammad Hosseini; Adel Khorramrouz; Ehsaneddin Asgari,Shayan Bali,Oral,In-person,SALLE  WALILI,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",ethical; biases; scenarios; probability; probability distribution; status; bias; neutral; lens; subtle
1188-MAIN,HarfoSokhan: A Comprehensive Parallel Dataset for Transitions between Persian Colloquial and Formal Variations,"A wide array of NLP/NLU models have been developed for the Persian language and have shown promising results. However, the performance of such models drops significantly when applied to the colloquial form of Persian. This challenge arises from the substantial differences between colloquial and formal Persian and the lack of parallel data facilitating the robustness of the model to the colloquial data or to transform the data to formal Persian. In addressing this gap, our research is dedicated to the development of the HarfoSokhan dataset, a large-scale colloquial to formal Persian parallel dataset of 6M sentence pairs. Our proposed dataset is a critical resource for training models that can effectively bridge the linguistic variations between colloquial and formal Persian. To illustrate the utility of our dataset, we used it to train a GPT2 model, which exhibited remarkable proficiency in colloquial to formal text style transfer, outperforming both OpenAI's GPT-3.5-turbo model and a leading rule-based system in this task. This conclusion is supported by our proposed ranking-based human evaluation. The results underscore the significance of the HarfoSokhan dataset in enhancing the performance of natural language processing models in the challenging task of colloquial to formal Persian conversion.",Hamid Jahad Sarvestani; Vida Ramezanian; Saee Saadat; Neda Taghizadeh Serajeh; Maryam Sadat Razavi Taheri; Shohreh Kasaei; MohammadAmin Fazli; Ehsaneddin Asgari,Ehsaneddin Asgari,Oral,In-person,SALLE  WALILI,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics",Summarization & Generation,persian; formal; dataset; parallel dataset; parallel; variations; proposed; text style transfer; results performance; resource training
1194-MAIN,JointCal: Efficient and Effective Domain-adapted Compression,"Large language models (LMs) excel across diverse domains, yet require substantial computational resources during inference. Model compression has proved a successful direction towards reducing these costs, exemplified by the performance preservation on general-purpose benchmarks. However, model compression methods can substantially degrade performance for specialized domains, such as law and healthcare. We therefore present JointCal, a novel approach that simultaneously models the importance of weights to both specialized and general capabilities. This leverages a unique layer-wise reconstruction loss formulation that captures the activations from different domains and the interactions between them. Using a battery of experiments across tasks and models, we empirically show that JointCal offers consistent improvements on specialized benchmarks, while preserving overall performance. In contrast to prior work towards domain-adapted compression, our approach does not require any resource-intensive model training procedures, reducing the duration of compression from hours to minutes.",Miles Williams; George Chrysostomou; Vitor Amancio Jeronymo; Nikolaos Aletras,Miles Williams,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Efficiency, Scaling & NLP Systems",Domain NLP (Biomedical/Clinical/Legal/Scientific),compression; model compression; specialized; adapted; domains; reducing; require; general; activations different; experiments tasks
1195-MAIN,GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences,"Personalization in LLMs often relies on costly human feedback or interaction logs, limiting scalability and neglecting deeper user attributes. We introduce \approach (\textbf{G}enerative \textbf{R}esponse with \textbf{A}ligned \textbf{V}alues, \textbf{I}nterests, and \textbf{T}raits of \textbf{Y}ou), a framework for generating \textbf{synthetic, profile-grounded preference data} that captures usersвЂ™ interests, values, beliefs, and personality traits. By integrating demographic, cultural, and psychological frameworksвЂ”including HofstedeвЂ™s cultural dimensions, SchwartzвЂ™s basic values, the World Values Survey, and Big Five OCEAN traitsвЂ”\approach synthesizes chosen/rejected preference pairs to guide personalized content generation. We evaluate \approach on book descriptions for 400 Amazon users, comparing it to prompt-based conditioning, standard fine-tuning, and naive synthetic pair generation. Profile-grounded synthetic data consistently improves generation, especially across multiple cultures (USA, Brazil, Japan, India), achieving over 4% higher preference gains across baselines, with user studies showing that \approach outputs are preferred over 86% of the time. Our results show that scenario-grounded synthetic data can capture richer user variation, reduce reliance on costly annotation, and produce more engaging, user-centered content, offering a scalable path for LLM personalization. \textit{Code and datasets will be released upon acceptance.}",Priyanka Dey; Daniele Rosa; Wenqing Zheng; Daniel Barcklow; Jieyu Zhao; Emilio Ferrara,Priyanka Dey,Oral,In-person,SALLE  LA PALMERAIE,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Summarization & Generation,"Trustworthy, Safety, Privacy & Fairness",textbf; grounded synthetic; synthetic; profile; grounded; values; user; preference; personalization; generation
1197-MAIN,On the Mathematical Relationship Between Layer Normalization and Dynamic Activation Functions,"Layer normalization (LN) is an essential component of modern neural networks. While many alternative techniques have been proposed, none of them have succeeded in replacing LN so far. The latest suggestion in this line of research is a dynamic activation function called Dynamic Tanh (DyT). Although it is empirically well-motivated and appealing from a practical point of view, it lacks a theoretical foundation. In this work, we shed light on the mathematical relationship between LN and dynamic activation functions. In particular, we derive DyT from the LN variant RMSNorm, and show that a well-defined decoupling in derivative space as well as an approximation are needed to do so. By applying the same decoupling procedure directly in function space, we are able to omit the approximation and obtain the exact element-wise counterpart of RMSNorm, which we call Dynamic Inverse Square Root Unit (DyISRU). We demonstrate numerically that DyISRU reproduces the normalization effect on outliers more accurately than DyT does.",Felix Stollenwerk,Felix Stollenwerk,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",,dynamic activation; dynamic; normalization; activation; rmsnorm; decoupling; approximation; functions; relationship; function
1203-MAIN,Multimodal Conversation Structure Understanding,"While large language models (LLMs) excel at dialogue, whether they can adequately parse the structure of conversationвЂ”resolve the reply-to relationship between utterances, or attribute roles like speakers and addresseesвЂ”remains underexplored, especially in multimodal settings. To address this, we introduce a suite of tasks for multimodal conversation understanding and release TV-MMPC, a new human-annotated dataset of conversational roles and threading in television dialogue. Our evaluation reveals that while all multimodal LLMs outperform our heuristic baseline, even the best-performing model we consider experiences a substantial drop in performance when character identities of the conversation are anonymized. Beyond evaluation, we carry out a sociolinguistic analysis of 350,842 utterances in TVQA. We find that while female characters initiate conversations at rates in proportion to their speaking time, they are 1.2 times more likely than men to be cast as an addressee or side-participant, and the presence of side-participants shifts the conversational register from personal to social.",Kent K. Chang; Mackenzie Hanh Cramer; Anna Ho; Ti Ti Nguyen; Yilin Yuan; David Bamman,Kent K. Chang,Oral,In-person,SALLE  WALILI,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Dialogue, Conversational & Interactive NLP",Multimodal & Speech/Audio,conversation; multimodal; roles; utterances; conversational; dialogue; structure; understanding release; structure understanding; likely men
1206-MAIN,A Review of Incorporating Psychological Theories in LLMs,"This paper reviews how psychological theories inform large language model (LLM) development across data, pre-training, post-training, and evaluation/application. We synthesize insights from cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics. Our stage-wise analysis reveals patterned but uneven adoption: cognitive constructs appear across stages; behavioral principles dominate post-training and alignment; and social/personality/psycholinguistic theories are most prevalent in evaluation and application. We highlight underused frameworks (e.g., identity and influence in social psychology; schema theory in cognition) and ongoing debates (e.g., reliability of ToM/personality measures, theoryвЂ“method mismatches). To aid readers, we add impact cues for seminal works and an appendix table of high-impact milestones by stage. We conclude with recommendations for theory-grounded evaluation and closer collaboration between NLP and psychology to achieve more robust, ethically aligned LLMs.",Zizhou Liu; Ziwei Gong; Lin Ai; Zheng Hui; Run Chen; Colin Wayne Leach; Michelle R. Greene; Julia Hirschberg,Ziwei Gong,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",Interpretability & Model Analysis,theories; psychology; personality; social; psychological; post training; behavioral; cognitive; application; theory
1209-MAIN,How Robust Are Router-LLMs? Analysis of the Fragility of LLM Routing Capabilities,"Large language model (LLM) routing has emerged as a crucial strategy for balancing computational costs with performance by dynamically assigning queries to the most appropriate model based on query complexity. Despite recent advances showing that preference-data-based routers can outperform traditional methods, current evaluation benchmarks remain limitedвЂ”they largely focus on general model capabilities while overlooking task-specific behaviors and critical concerns such as privacy, safety, and potential backdoor vulnerabilities introduced through preference data. In response, we propose the DSC benchmark: Diverse, simple, and categorized, an evaluation framework that categorizes router performance across a broad spectrum of query typesвЂ”including coding, translation, mathematics, human instructions, general knowledge, and LLM jailbreakingвЂ”and integrates privacy and safety assessments to reveal hidden risks. Our experiments on three preference-based routers and two commercial counterparts demonstrate that while these systems improve efficiency, they often make suboptimal, category-driven decisions; for instance, a BERT-based router directs all coding and mathematics queries to the most powerful LLMвЂ”even when simpler models would sufficeвЂ”while routing jailbreaking attempts to weaker models, thereby elevating safety risks.",Aly M. Kassem; Bernhard SchГ¶lkopf; Zhijing Jin,Aly Kassem,Oral,In-person,SALLE  LE RIAD,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems","Trustworthy, Safety, Privacy & Fairness",router; routing; preference data; llm routing; preference; mathematics; safety; coding; risks; privacy
1212-MAIN,NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering,"Diet plays a central role in human health, and Nutrition Question Answering (QA) offers a promising path toward personalized dietary guidance and the prevention of diet-related chronic diseases. However, existing methods face two fundamental challenges: the limited reasoning capacity of single-agent systems and the complexity of designing effective multi-agent architectures, as well as contextual overload that hinders accurate decision-making. We introduce Nutritional-Graph Router (NG-Router), a novel framework that formulates nutritional QA as a supervised, knowledge-graphвЂ“guided multi-agent collaboration problem. NG-Router integrates agent nodes into heterogeneous knowledge graphs and employs a graph neural network to learn task-aware routing distributions over agents, leveraging soft supervision derived from empirical agent performance. To further address contextual overload, we propose a gradient-based subgraph retrieval mechanism that identifies salient evidence during training, thereby enhancing multi-hop and relational reasoning. Extensive experiments across multiple benchmarks and backbone models demonstrate that NG-Router consistently outperforms both single-agent and ensemble baselines, offering a principled approach to domain-aware multi-agent reasoning for complex nutritional health tasks.",Kaiwen Shi; Zheyuan Zhang; Zhengqing Yuan; Keerthiram Murugesan; Vincent Galassi; Chuxu Zhang; Yanfang Ye,Kaiwen Shi,Oral,In-person,SALLE  LA PALMERAIE,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents","Retrieval, Grounding & External Knowledge (RAG)",router; agent; nutritional; multi agent; nutrition; multi; multi agent collaboration; agent collaboration; single agent; graph
1213-MAIN,Verification-Aware Planning for Multi-Agent Systems,"Large language model (LLM) agents are increasingly deployed to tackle complex tasks, often necessitating collaboration among multiple specialized agents. However, multi-agent collaboration introduces new challenges in planning, coordination, and verification. Execution failures frequently arise not from flawed reasoning alone, but from subtle misalignments in task interpretation, output format, or inter-agent handoffs. To address these challenges, we present VeriMAP, a framework for multi-agent collaboration with verification-aware planning. The VeriMAP planner decomposes tasks, models subtask dependencies, and encodes planner-defined passing criteria as subtask verification functions (VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets, demonstrating that it outperforms both single- and multi-agent baselines while enhancing system robustness and interpretability. Our analysis highlights how verification-aware planning enables reliable coordination and iterative refinement in multi-agent systems, without relying on external labels or annotations.",Tianyang Xu; Dan Zhang; Kushan Mitra; Estevam Hruschka,Tianyang Xu,Oral,In-person,Pavillon  DE RABAT,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Reasoning, Planning & Agents",Interpretability & Model Analysis,multi agent; verification; agent; planning; collaboration; multi; multi agent collaboration; subtask; agent collaboration; coordination
1216-MAIN,Zero-Shot Open-Schema Entity Structure Discovery,"Entity structure extraction, which aims to extract entities and their associated attributeвЂ“value structures from text, is an essential task for text understanding and knowledge graph construction. Existing methods based on large language models (LLMs) typically rely heavily on predefined entity attribute schemas or annotated datasets, often leading to incomplete extraction results. To address these challenges, we introduce ZOES, a novel approach to entity structure extraction that does not require any schema or annotated samples. ZOES operates via a principled mechanism of enrichment, refinement, and unification, based on the insight that an entity and its associated structure are mutually reinforcing. Experiments demonstrate that ZOES consistently enhances LLMs' ability to extract more complete entity structures across three different domains, showcasing both the effectiveness and generalizability of the method. These findings suggest that such an enrichment, refinement, and unification mechanism may serve as a principled approach to improving the quality of LLM-based entity structure discovery in various scenarios.",Xueqiang Xu; Jinfeng Xiao; James Barry; Mohab Elkaref; Jiaru Zou; Pengcheng Jiang; Yunyi Zhang; Maxwell J Giammona; Geeth De Mel; Jiawei Han,Xueqiang Xu,Oral,Virtual,ZOOM,Virtual TBA,,,Information Extraction & Structured Prediction,,entity; structure; enrichment; extraction; discovery; schema; associated; principled; refinement; extract
1217-MAIN,Beyond Semantics: How Temporal Biases Shapes Retrieval in Transformer and State-Space Models,"In-context learning is governed by both temporal and semantic relationships, shaping how Large Language Models (LLMs) retrieve contextual information. Analogous to human episodic memory, where the retrieval of specific events is enabled by separating events that happened at different times, this work probes the ability of various pretrained LLMs, including transformer and state-space models, to differentiate and retrieve temporally separated events. Specifically, we prompted models with sequences containing multiple presentations of the same token, which reappears at the sequence end. By fixing the positions of these repeated tokens and permuting all others, we removed semantic confounds and isolated temporal effects on next-token prediction. Across diverse sequences, models consistently placed the highest probabilities on tokens following a repeated token, but with a notable bias for those nearest the beginning or end of the input. An ablation experiment linked this phenomenon in transformers to induction heads. Extending the analysis to unique semantic contexts with partial overlap further demonstrated that memories embedded in the middle of a prompt are retrieved less reliably. Despite architectural differences, state-space and transformer models showed comparable temporal biases. Our findings deepen the understanding of temporal biases in in-context learning and offer an illustration of how these biases can enable temporal separation and episodic retrieval.",Anooshka Bajaj; Deven Mahesh Mistry; Sahaj Singh Maini; Yash Aggarwal; Zoran Tiganj,Anooshka Bajaj,Oral,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",temporal; state space; biases; events; transformer; episodic; space models; state space models; repeated; space
1220-MAIN,Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies,"Large-scale Vision Language Models (LVLMs) are increasingly applied to a wide range of real-world multimodal tasks, involving complex visual and linguistic reasoning. As these models become more integrated as practical application, they are expected to handle complex aspects of human interaction. Among these, color perception is a fundamental yet highly variable aspect of visual understanding. It diverges across individuals due to biological factors such as Color Vision Deficiencies (CVDs), as well as differences in culture and language. Despite its importance, perceptual diversity has received limited attention. In our study, we use the Ishihara Test, a widely adopted diagnostic tool for detecting CVDs, to investigate the ability of LVLMs to handle individual-level perceptual diversity from three complementary perspectives: generation, confidence, and internal representation. Our experimental analysis shows that while LVLMs possess linguistic factual knowledge about the Ishihara Test and CVDs and can explain them, they still fail to simulate how people with CVDs perceive colors. These findings highlight the need for multimodal systems that account for color perceptual diversity and broader discussions on inclusiveness and fairness in multimodal AI.",Kazuki Hayashi; Shintaro Ozaki; Yusuke Sakai; Hidetaka Kamigaito; Taro Watanabe,Kazuki Hayashi,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Interpretability & Model Analysis,Multimodal & Speech/Audio,color; perceptual; lvlms; vision; deficiencies; diversity; multimodal; handle; perception; vision language models
1221-MAIN,Tokenizer-Aware Cross-Lingual Adaptation of Decoder-Only LLMs through Embedding Relearning and Swapping,"Extending Large Language Models (LLMs) to new languages is challenging, with most methods proposed suffering from high computational cost and catastrophic forgetting of original model capabilities. Embedding relearning~\citep{artetxe-etal-2020-cross}, a technique that creates new tokenizers and tunes embeddings on fixed model weights for target language adaptation, is both light-weight and performant. However, it has only been shown to work for older generation encoder-only models and for high resource languages. In this paper, we extend this framework to decoder-only LLMs focusing on joint adaptation to many languages, including low-resource ones. We experiment in three language groups over 100 languages each. We adapt a pre-trained LLM via switching to a customized tokenizer, and relearning the embedding layer. Across 96 diverse languages spanning both classification and generation tasks, we show embedding relearning improves \texttt{Gemma2} models by up to 20%, being highly competitive with full-weight updating baselines while vastly more computationally efficient and mitigating catastrophic forgetting. This translates into better results in transferring the improved multilingual performance to tasks that build on core English abilities (e.g., multilingual math reasoning), compared to various baselines. Further analysis reveals the critical role of customizing tokenizers in achieving effective language transfer, particularly for non-Latin script languages.",Fan Jiang; Honglin Yu; Grace Y Chung; Trevor Cohn,Fan Jiang,Oral,Virtual,ZOOM,Virtual TBA,,,Multilinguality & Low-Resource NLP,,languages; embedding; adaptation; decoder llms; catastrophic; catastrophic forgetting; forgetting; tokenizers; tokenizer; weight
1222-MAIN,Active Generalized Category Discovery with Diverse LLM Feedback,"Generalized Category Discovery (GCD) is a practical and challenging open-world task that aims to recognize both known and novel categories in unlabeled data using limited labeled data from known categories. Due to the lack of supervision, previous GCD methods face significant challenges, such as difficulty in rectifying errors for confusing instances, and inability to effectively uncover and leverage the semantic meanings of discovered clusters. Therefore, additional annotations are usually required for real-world applicability. However, human annotation is extremely costly and inefficient. To address these issues, we propose DeLFGCD , a unified framework for generalized category discovery that actively learns from diverse and collaborative LLM feedback. Our approach leverages three different types of LLM feedback to: (1) improve instance-level contrastive features, (2) generate category descriptions, and (3) align uncertain instances with LLM-selected category descriptions. Extensive experiments demonstrate the superior performance of DeLFGCD over state-of-the-art models across diverse datasets, metrics, and supervision settings. Our code is available at https://anonymous.4open.science/r/GCD-LLM-0DB9",Henry Peng Zou; Siffi Singh; Yi Nian; Jianfeng He; Jason Cai; Saab Mansour; Hang Su,Saab Mansour,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Dialogue, Conversational & Interactive NLP",,category; gcd; generalized; discovery; feedback; descriptions; instances; llm; known; supervision
1231-MAIN,RAFFLES: Reasoning-based Attribution of Faults for LLM Systems,"The advent of complex, interconnected long-horizon LLM systems has made it incredibly tricky to identify where and when these systems break down. Evaluation capabilities that currently exist today are limited in that they often focus on simple metrics, end-to-end outcomes, and are dependent on the perspectives of humans. In order to match the increasing complexity of these many component systems, evaluation frameworks must also be able to reason, probe, iterate, and understand the nuanced logic passing through these systems. In this paper, we present RAFFLES, an evaluation architecture that incorporates reasoning and iterative refinement. Specifically, RAFFLES operates as an iterative, multi-component pipeline, using a central Judge to systematically investigate faults and a set of specialized Evaluators to assess not only the system's components but also the quality of the reasoning by the Judge itself. We evaluated RAFFLES with several benchmarks - the Who&When dataset to identify step-level faults in agentic systems and the ReasonEval datasets to diagnose step-level mathematical reasoning errors. RAFFLES outperforms strong baselines, achieving an accuracy of over 20% and 50% on the Who&When Hand-Crafted and Algorithm datasets, and over 80% on the ReasonEval datasets. These results demonstrate a key step towards introducing automated fault detection for autonomous systems over labor-intensive manual human review.",Chenyang Zhu; Spencer Hong; Jingyu Wu; Kushal Chawla; Yuhui Tang; Youbing Yin; Nathan Wolfe; Erin Babinsky; Daben Liu,Chenyang Zhu,Oral,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","LLM Evaluation, Benchmarks & Metrics",systems; step level; llm systems; step; iterative; reasoning; component; judge; datasets; end
1235-MAIN,Jailbreaks as Inference-Time Alignment: A Framework for Understanding Safety Failures in LLMs,"Large language models (LLMs) are safety-aligned to prevent harmful response generation, yet still remain vulnerable to jailbreak attacks. While prior works have focused on improving jailbreak attack effectiveness, they offer little explanation for why safety alignment fails. We address this gap by framing jailbreaks as inference-time alignment, connecting attack design and safety alignment within a unified optimization framework. This framing allows us to extend best-of-N inference-time alignment to the adversarial setting, called LIAR (Leveraging Inference-time Alignment to jailbReak), and derive suboptimality bounds that show LIAR provably approaches an optimal jailbreak as compute scales. Interestingly, our framework allows us to develop the notion of a Safety-Net, a measure of how vulnerable an LLM is to jailbreaks, which helps to explain why safety alignment can fail. Empirically, LIAR produces natural, hard-to-detect prompts that achieve a competitive attack success rate while running 10 to 100x faster than prior suffix-based jailbreaks.",James Beetham; Souradip Chakraborty; Mengdi Wang; Furong Huang; Amrit Singh Bedi; Mubarak Shah,James Beetham,Oral,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",,inference time alignment; time alignment; jailbreaks; safety; alignment; jailbreak; liar; inference time; safety alignment; attack
1240-MAIN,Over-Searching in Retrieval-Augmented Large Language Models,"Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. Yet they often over-search вЂ” unnecessarily invoking search even when it does not improve response quality, which inflates computational cost and can lead to hallucinations by introducing misleading evidence. In this work, we conduct a systematic evaluation of over-searching across query types, model categories, retrieval conditions, and single- vs. multi-turn settings. Our findings show: (i) search improves answer accuracy on answerable queries but harms abstention accuracy on unanswerable ones; (ii) the effect is stronger for reasoning-style and deep research systems, under noisy retrieval, and in multi-turn conversations where search behavior snowballs across turns; and (iii) the composition of retrieved evidence significantly governs abstention, with negative evidence improving abstention when present. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. We investigate mitigation approaches at both query and retrieval levels and release OverSearchQA to foster continued research into improving efficiency in search-augmented LLMs.",Roy Xie; Deepak Gopinath; David Qiu; Dong Lin; Haitian Sun; Saloni Potdar; Bhuwan Dhingra,Roy Xie,Oral,In-person,SALLE  LE RIAD,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)","Reasoning, Planning & Agents",search; abstention; searching; retrieval; augmented; augmented llms; evidence; multi turn; turn; cost
1255-MAIN,LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing,"Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. To address this gap, we introduce LitBench, a large-scale benchmark for creative writing evaluation, featuring a training corpus of 43,827 story pairs and a 2,480-pair test set curated from Reddit. Using LitBench, we benchmark existing LLM judges and train specialized reward models. Our analysis reveals that the strongest OTS judge, Claude-3.7-Sonnet, achieves only 73% agreement with human preferences. In contrast, our trained Bradley-Terry and generative reward models both reach 78% accuracy, outperforming all OTS judges. An online human study further validates our models, showing their rankings of newly generated stories align more closely with human preferences. Our work provides the first reliable benchmark and specialized reward models for creative writing, establishing a crucial foundation for the future development of more capable verifiers.",Daniel Fein; Sebastian Russo; Violet Xiang; Kabir Jolly; Rafael Rafailov; Nick Haber,Daniel Fein,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,creative writing; creative; writing; reward models; judges; reward; human preferences; benchmark; preferences; specialized
1258-MAIN,H-Mem: Hybrid Multi-Dimensional Memory Management for Long-Context Conversational Agents,"Long-context conversational agents require robust memory, but existing frameworks struggle to organize information effectively across dimensions like time and topic, leading to poor retrieval. To address this, we introduce H-Mem, a novel Hybrid Multi-Dimensional Memory architecture. H-Mem stores conversational facts in two parallel, hierarchical data structures: a temporal tree that organizes information chronologically and a semantic tree that organizes it conceptually. This dual-tree design enables a hybrid retrieval mechanism managed by an intelligent Mode Controller. Based on the query, the controller dynamically chooses between a sequential search using semantic anchors and an intersective search combining both hierarchies. Our experiments on long-context QA datasets demonstrate that H-Mem provides a more flexible approach to memory management, leading to significant improvements of over 8.4\% compared to other state-of-the-art systems.",Zihe Ye; Jingyuan Huang; Weixin Chen; Yongfeng Zhang,Zihe Ye,Oral,In-person,SALLE  WALILI,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",mem; memory; tree; long context; hybrid; hybrid multi; controller; conversational; conversational agents; multi dimensional
1263-MAIN,"``Yuki Gets Sushi, David Gets Steak?'': Uncovering Gender and Racial Biases in LLM-Based Meal Recommendations","Group bias in Large Language Models (LLMs) is a well-documented issue, its impact in high-stakes domains such as personalized nutritional advice remains under explored. This study introduces the USChainMains dataset to systematically evaluate LLMs, prompting them with names associated with specific racial and gender groups and rigorously quantifying the healthfulness of the generated meal recommendations against established dietary standards. The findings demonstrate that LLMs systematically recommend meals with significantly higher levels of adverse nutrients for names associated with Black, Hispanic, or male individuals, thereby reflecting and potentially reinforcing detrimental dietary stereotypes. Furthermore, our analysis of two common mitigation strategies reveals their limitations. While model scaling improves overall recommendation healthfulness, it is insufficient to eliminate the healthfulness gap between demographic groups. Notably, while augmented reasoning was effective in mitigating gender bias, it did not mitigate racial disparities. This work underscores the necessity of developing more nuanced, group-aware debiasing techniques to ensure AI-driven systems advance, rather than hinder, health equity.",Xuefeng Wei; Xuan Zhou; Yusuke Sakai; Taro Watanabe,Xuefeng Wei,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Efficiency, Scaling & NLP Systems","Trustworthy, Safety, Privacy & Fairness",gender; dietary; names; groups; recommendations; group; associated; bias; systematically; bias large
1265-MAIN,Happiness is Sharing a Vocabulary: A Study of Transliteration Methods,"Transliteration has emerged as a promising means to bridge the gap between various languages in multilingual NLP, showing promising results especially for languages using non-Latin scripts. We investigate the degree to which shared script, overlapping token vocabularies, and shared phonology contribute to performance of multilingual models. To this end, we conduct controlled experiments using three kinds of transliteration (romanization, phonemic transcription, and substitution ciphers) as well as orthography. We evaluate each model on two downstream tasks---named entity recognition (NER) and natural language inference (NLI)---and find that romanization significantly outperforms other input types in 7 out of 8 evaluation settings, largely consistent with our hypothesis that it is the most effective approach. We further analyze how each factor contributed to the success, and suggest that having longer (subword) tokens shared with pre-trained languages leads to better utilization of the model.",Haeji Jung; Jinju Kim; Kyungjin Kim; Youjeong Roh; David R. Mortensen,Haeji Jung,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Information Extraction & Structured Prediction,Multilinguality & Low-Resource NLP,transliteration; shared; romanization; languages; promising; kinds; vocabularies; tasks named; tasks named entity; languages leads
1266-MAIN,SCALAR: Scientific Citation-based Live Assessment of Long-context Academic Reasoning,"Long-context understanding has emerged as a critical capability for large language models (LLMs). However, evaluating this ability remains challenging. We present SCALAR, a benchmark designed to assess citation-grounded long-context reasoning in academic writing. SCALAR leverages academic papers and their citation structure to automatically generate high-quality ground-truth labels without human annotation. It features controllable difficulty levels and a dynamic updating mechanism that mitigates data contamination. The benchmark includes two tasks: a multiple-choice QA format and a cloze-style citation prediction. We evaluate a range of state-of-the-art LLMs and find that the multiple-choice task effectively distinguishes model capabilities---while human experts achieve over 90% accuracy, most models struggle. The cloze-style task is even more challenging, with no model exceeding 40% accuracy. SCALAR provides a domain-grounded, continuously updating framework for tracking progress in citation-based long-context understanding. Code and data will be publicly released.",Renxi Wang; Honglin Mu; Liqun Ma; Lizhi Lin; Yunlong Feng; Timothy Baldwin; Xudong Han; Haonan Li,Renxi Wang,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Retrieval, Grounding & External Knowledge (RAG)",Domain NLP (Biomedical/Clinical/Legal/Scientific),citation; scalar; long context; academic; long; cloze style; cloze; context; updating; multiple choice
1268-MAIN,Look Before You Leap: A Lookahead Reasoning Quality Gate for Speculative Decoding,"We present a lookahead quality gate (verifier) for speculative decoding for reasoning or chain-of-thought language models. The gate accepts the longest reliable prefix of each k-token lookahead (block-wise) draft. Unlike token-level likelihood search, which is myopic and often rewards verbosity, or tree-level sampling methods that trade accuracy for latency, our approach works at an intermediate granularity. It uses only the base model's hidden states to compute a geometry-based quality score for each prefix, then accepts the longest prefix whose score exceeds a quantile-calibrated threshold estimated from unlabeled prompts. The method integrates seamlessly with speculative/blockwise decoding and adds minimal runtime overhead, requiring no auxiliary heads, reward models, or finetuning. On math and science benchmarks, it improves accuracy over sampling baselines while achieving $2.6-7.9Г—$ faster generation.",Hiroaki Kingetsu; Kaoru Yokoo; Kenji Fukumizu; Manohar Kaul,Hiroaki Kingetsu,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",prefix; speculative; decoding; speculative decoding; sampling; quality; score; token; trade accuracy; leap
1285-MAIN,{AnonGuard}: A Culturally-Aware Moderation Filter for Arabic Language Models,"Content moderation filters are a critical safeguard against alignment failures in language models. Yet most existing filters focus narrowly on general safety and overlook cultural context. In this work, we introduce {AnonGuard} (Masked for anonymity), a bilingual moderation filter that evaluates both safety and cultural alignment in Arabic and English. We construct a dataset of over 468K prompt and response pairs, drawn from synthetic and public datasets, scored by a panel of LLM judges on harmlessness and cultural awareness, and use it to train two filter variants. To rigorously evaluate cultural alignment, we further develop the first benchmark targeting Arabic cultural contexts, comprising 1008 norm-sensitive prompts with LLM-generated responses annotated by human raters. Results show that {AnonGuard} achieves stronger agreement with human annotations than inter-annotator reliability, while matching the performance of state-of-the-art filters on safety benchmarks. These findings highlight the importance of integrating cultural awareness into moderation and establish {AnonGuard} as a practical step toward more context-sensitive safeguards.",Masoomali Fatehkia; Enes Altinisik; Husrev Taha Sencar,Masoomali Fatehkia,Oral,In-person,SALLE  LE RIAD,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",cultural; moderation; filters; filter; arabic; awareness; safety; alignment; sensitive; cultural context
1286-MAIN,BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation,"Multi-LLM systems enhance the creativity of large language models by simulating human collective intelligence but suffer from significant drawbacks, such as high computational costs and inference latency. To address these limitations, we propose BILLY (BlendIng persona vectors for Large Language model creativitY), a training-free framework that captures the benefits of multi-LLM collaboration, i.e. inducing diverse perspectives and specialized expertise, within a single model. BILLY operates by extracting and blending multiple distinct persona vectors directly in the model's activation space. We steer the model's generation process with this merged vector while inference, enabling multi-perspective output without explicit multi-LLM communication. Our experiments across creativity-oriented benchmarks demonstrate that BILLY surpasses single model prompting and traditional multi-LLM approaches, while substantially reducing inference time and computational costs. Our analyses further reveal that distinct persona vectors can be blended to achieve both effective control over complementary aspects of generation and greater interpretability.",Tsung-Min Pai; Jui-I Wang; Li-Chun Lu; Shao-Hua Sun; Hung-yi Lee; Kai-Wei Chang,"Tsung-Min Pai, Li-Chun Lu",Oral,In-person,Pavillon  DE RABAT,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems",Interpretability & Model Analysis,multi llm; persona; vectors; creativity; multi; blending; single model; computational costs; inference; costs
1288-MAIN,Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story,"Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID ($\sim 8$), encyclopedic content medium ID ($\sim 9$), and creative/opinion writing high ID ($\sim 10.5$) across all models tested. This reveals that contemporary LLMs find scientific text ""representationally simple"" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively ""easy"", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.",Pedashenko Vladislav; Laida Kushnareva; Yana Khassan Nibal; Eduard Tulchinskii; Kristian Kuznetsov; Vladislav Zharchinskii; Yury Maximov; Irina Piontkovskaya,Laida Kushnareva,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Interpretability & Model Analysis,Domain NLP (Biomedical/Clinical/Legal/Scientific),sim; scientific; degrees freedom; freedom; opinion; saes; contemporary; degrees; creative; writing
1299-MAIN,Efficient Uncertainty Quantification of Language Models through Token Clustering,"Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, yet their limited truthfulness and tendency toward overconfidence restrict their reliability when needing to state facts. Uncertainty quantification offers a promising approach to identify and manage unreliable outputs from LLMs. However, most existing uncertainty quantification methods typically require multiple sampling or external inference models, resulting in increased computational overhead. To address these limitations, we propose an efficient uncertainty quantification method that leverages semantic information inherently encoded within LLMs. Specifically, we cluster tokens into semantically coherent groups based on embedding similarity and prefix matching, and compute a cluster-based uncertainty score at each decoding step. Our approach requires only a single deterministic generation and does not rely on external inference models. Experiments across multiple datasets and models show that our method achieves performance comparable to existing baselines while significantly reducing computational overhead.",Qi Cao; Andrew Gambardella; Takeshi Kojima; Yutaka Matsuo; Yusuke Iwasawa,Qi Cao,Oral,In-person,SALLE  WALILI,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",uncertainty; uncertainty quantification; quantification; inference models; cluster; computational overhead; overhead; external; computational; encoded llms
1302-MAIN,Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models,"Large vision-language models (LVLMs) have demonstrated outstanding performance in many downstream tasks. However, LVLMs are trained on large-scale datasets, which can pose privacy risks if training images contain sensitive information. Therefore, it is important to detect whether an image is used to train the LVLM. Recent studies have investigated membership inference attacks (MIAs) against LVLMs, including detecting image-text pairs and single-modality content. In this work, we focus on detecting whether a target image is used to train the target LVLM. We design simple yet effective Image Corruption-Inspired Membership Inference Attacks (ICIMIA) against LLVLMs, which are inspired by LVLM's different sensitivity to image corruption for member and non-member images. We first perform an MIA method under the white-box setting, where we can obtain the embeddings of the image through the vision part of the target LVLM. The attacks are based on embedding similarity between the image and its corrupted version. We further explore a more practical scenario where we have no knowledge about target LVLMs and we can only query the target LVLMs with an image and a question. We then conduct the attack by utilizing the output text embeddings' similarity. Experiments on existing datasets validate the effectiveness of our proposed attack methods under two different settings.",Zongyu Wu; Minhua Lin; Zhiwei Zhang; Fali Wang; Xianren Zhang; Xiang Zhang; Suhang Wang,Minhua Lin,Oral,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Trustworthy, Safety, Privacy & Fairness",image; lvlms; target; corruption; membership inference attacks; inference attacks; attacks; membership inference; membership; inspired
1305-MAIN,Becoming Experienced Judges: Selective Test-Time Learning for Evaluators,"Most evaluation methods like LLM-as-a-judge treat each test example independently, overlooking the potential to learn from previous evaluations. We introduce **Learning While Evaluating** (LWE), a framework that enables evaluators to improve sequentially during testing without parameter updates. LWE maintains an evolving *meta-prompt* that (i) produces sample-specific evaluation instructions and (ii) updates itself using self-generated feedback after each batch. While sequential updating improves performance, processing every sample incurs substantial computational overhead. We therefore propose ***Selective* LWE**, which updates the meta-prompt only for cases where the evaluator is uncertain, focusing computation on the most informative samples. On multimodal pairwise evaluation benchmarks, *Selective* LWE outperforms baselines and achieves comparable accuracy to full sequential updates while significantly reducing token costs.",Seungyeon Jwa; Daechul Ahn; Reokyoung Kim; Dongyeop Kang; Jonghyun Choi,Seungyeon Jwa,Oral,In-person,SALLE  LE RIAD,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,updates; selective; evaluators; meta; sequential; sample; test; prompt; evaluation; using self generated
1310-MAIN,Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models,"Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. Prior studies show that English-centric large language models map multilingual content into English-aligned representations at intermediate layers and then project them back into target-language token spaces in the final layer. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual data. Experiments on a multilingual generation control task reveal the interpretability of these dimensions, demonstrating that the interventions in these dimensions can switch the output language while preserving semantic content, and that it surpasses the performance of prior neuron-based approaches at a substantially lower cost.",Chengzhi Zhong; Fei Cheng; Qianying Liu; Yugo Murawaki; Chenhui Chu; Sadao Kurohashi,CHENGZHI ZHONG,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multilinguality & Low-Resource NLP,Interpretability & Model Analysis,dimensions; multilingual; english; intermediate; sparse; layers; final; control; language; prior
1312-MAIN,Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models,"Large language models are increasingly used for creative writing and engagement content, raising safety concerns about their outputs. Using humor generation as a testbed, this work evaluates how funniness optimization in modern LLM pipelines couples with harmful content by jointly measuring humor, stereotypicality, and toxicity. We further supplement this by analyzing incongruity signals through information-theoretic metrics. Across six models, we observe that harmful outputs receive higher humor scores, indicating a bias amplification loop between generators and evaluators. Information-theoretic analyses show that harmful cues widen predictive uncertainty and, surprisingly, can even make harmful punchlines more expected for some models, suggesting structural embedding in learned distributions. Experiments and human evaluation on an additional satire-generation task with human-perceived funniness judgments show that LLM funniness relies on increased stereotypicality and toxicity, including for closed models. Quantitatively, stereotypical/toxic jokes gain $10$%$-21$% in mean humor score, stereotypical jokes appear $11$% to $28$% more often among the jokes marked funny by an LLM-based metric, and up to $10$% more often in generations perceived as funny by humans.",Atharvan Dogra; Soumya Suvra Ghosal; Ameet Deshpande; Ashwin Kalyan; Dinesh Manocha,Atharvan Dogra,Oral,In-person,SALLE  LE RIAD,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",humor; harmful; toxicity; funny; stereotypical; engagement; information theoretic; theoretic; perceived; safety
1314-MAIN,Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in LLMs,"Prompt-based adversarial attacks are a key tool for assessing the robustness of large language models (LLMs). Yet, existing studies typically treat prompts as flat text, overlooking their internal structure, different components within a prompt contribute unequally to robustness. This work introduces PromptAnatomy, a framework that decomposes prompts into functional components and perturbs them selectively to expose component-wise vulnerabilities. We further propose ComPerturb, a controlled perturbation method that ensures linguistic plausibility through perplexity-based filtering. Using this framework, four instruction-tuning datasets are structurally annotated and validated by human reviewers. Experiments across five advanced LLMs show that ComPerturb achieves state-of-the-art attack success rates, while ablation analyses confirm the complementary effects of prompt dissection and perplexity filtering. These results highlight the importance of structural awareness in evaluating and improving the adversarial robustness of LLMs.",Yujia Zheng; Tianhao Li; Haotian Huang; Tianyu Zeng; Jingyu Lu; Chuangxin Chu; Yuekai Huang; Ziyou Jiang; Qian Xiong; Yuyao Ge; Mingyang Li,Yujia Zheng,Oral,In-person,SALLE  LE RIAD,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness","Linguistics, Syntax & Semantics",prompt; robustness; adversarial robustness; adversarial; components; perplexity; filtering; prompts; llms; tuning datasets
1315-MAIN,A Regex Minimization Benchmark: A PSPACE-Complete Challenge for Language Models,"Language models (LMs) have shown impressive reasoning capabilities across various domains. A fundamental question is the extent of their reasoning power. While recent studies show that LMs can solve NP-complete problems, their ability to handle PSPACE-complete problems remains underexplored. We investigate regex minimization as a PSPACE-complete challenge for LMs to address this issue. Regexes, formal expressions for regular languages, are widely used in NLP, SE or PL. There are several efficient tools to manipulate them with theoretical background. Inspired by this, we introduce the first benchmark for regex minimization containing over a million regexes paired with their minimal equivalents. Through extensive experiments with two LMs trained on our dataset and five open-source large language models (LLMs), we analyze how well LMs perform on PSPACE-complete problems, highlighting their capabilities of generalization and limitations in reasoning. To the best of our knowledge, this is the first study to systematically evaluate LM reasoning in regex minimization and establish a foundation for solving PSPACE-complete problems with LMs.",Hyundong Jin; Joonghyuk Hahn; Yo-Sub Han,Hyundong Jin,Oral,In-person,Pavillon  DE RABAT,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",complete; lms; regex; problems; regexes; reasoning; challenge; remains underexplored investigate; underexplored investigate; knowledge study
1322-MAIN,Teaching Small Language Models to Learn Logic through Meta-Learning,"Large language models (LLMs) are increasingly evaluated on reasoning tasks, yet their logical abilities remain contested. To address this, we study LLMsвЂ™ reasoning in a well-defined fragment of logic: syllogistic reasoning. We cast the problem as premise selection and construct controlled datasets to isolate logical competence. Beyond evaluation, an open challenge is enabling LLMs to acquire abstract inference patterns that generalize to novel structures. We propose to apply few-shot meta-learning to this domain, thereby encouraging models to extract rules across tasks rather than memorize patterns within tasks. Although meta-learning has been little explored in the context of logic learnability, our experiments show that it is effective: small models (1.5BвЂ“7B) fine-tuned with meta-learning demonstrate strong gains in generalization, with especially pronounced benefits in low-data regimes. These meta-learned models outperform GPT-4o and o3-mini on our syllogistic reasoning task.",Leonardo Bertolazzi; Manuel Vargas GuzmГЎn; Raffaella Bernardi; Maciej Malicki; Jakub Szymanik,Leonardo Bertolazzi,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",meta; meta learning; logic; syllogistic reasoning; syllogistic; learning; logical; reasoning; patterns; small
1324-MAIN,COMPACT: Building Compliance Paralegals via Clause Graph Reasoning over Contracts,"Contract compliance verification requires reasoning about cross-clause dependenciesвЂ”where obligations, exceptions, and conditions interact across multiple provisionsвЂ”yet existing legal NLP benchmarks like ContractNLI and CUAD focus exclusively on isolated single-clause tasks. We introduce COMPACT (COMpliance PAralegals via Clause graph reasoning over conTracts), a framework that models cross-clause dependencies through structured clause graphs. Our approach extracts deontic-temporal entities from clauses and constructs typed relationship graphs capturing definitional dependencies, exception hierarchies, and temporal sequences. From these graphs, we introduce ACE (Assessing Compliance in Enterprise) вЂ” a benchmark containing 4,700 carefully constructed compliance scenarios derived from 633 real-world contracts covering 26 types of agreements. Each scenario requires multi-hop reasoning across multiple clauses, and undergoes independent LLM-based validation to ensure quality. Evaluation reveals that multi-clause reasoning poses a fundamental challenge for state-of-the-art models (34-57% base accuracy), while training on ACE yields substantial improvements on compliance tasks (+22-43% points) without degrading general legal reasoning performance. ACE provides the first systematic benchmark for multi-clause contract compliance reasoning.",Ayush Singh; Dishank Aggarwal; PRANAV BHAGAT; Ainulla Khan; Sameer Malik; Amar Prakash Azad,Pranav Bhagat,Oral,In-person,SALLE  LA PALMERAIE,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",Domain NLP (Biomedical/Clinical/Legal/Scientific),clause; compliance; contracts; reasoning; contract; graphs; clauses; graph reasoning; compact; dependencies
1332-MAIN,Surprisal and Metaphor Novelty Judgments: Moderate Correlations and Divergent Scaling Effects Revealed by Corpus-Based and Synthetic Datasets,"Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying large language models (LLMs). This study investigates whether surprisal, a probabilistic measure of predictability in LLMs, correlates with judgments of metaphor novelty. We analyse surprisal from 16 model variants on corpus-based and synthetic metaphor novelty datasets. We also explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LLMs yield significant yet moderate correlations with judgments of novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (QualityвЂ“Power Hypothesis). We conclude that while surprisal can partially account for judgments of metaphor novelty, it remains a limited metric of linguistic creativity.",Omar Momen; Emilie Sitter; Berenike Herrmann; Sina ZarrieГџ,Omar Momen,Oral,In-person,SALLE  LA PALMERAIE,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Linguistics, Syntax & Semantics","LLM Evaluation, Benchmarks & Metrics",novelty; surprisal; corpus based; judgments; scaling; based synthetic; creativity; corpus; divergent; synthetic
1333-MAIN,Repairing Regex Vulnerabilities via Localization-Guided Instructions,"Regular expressions (regexes) are foundational to modern computing for critical tasks like input validation and data parsing, yet their ubiquity exposes systems to regular expression denial of service (ReDoS), a vulnerability requiring automated repair methods. Current approaches, however, are hampered by a trade-off. Symbolic, rule-based system are precise but fails to repair unseen or complex vulnerability patterns. Conversely, large language models (LLMs) possess the necessary generalizability but are unreliable for tasks demanding strict syntactic and semantic correctness. We resolve this impasse by introducing a hybrid framework, localized regex repair (LRR), designed to harness LLM generalization while enforcing reliability. Our core insight is to decouple problem identification from the repair process. First, a deterministic, symbolic module localizes the precise vulnerable subpattern, creating a constrained and tractable problem space. Then, the LLM invoked to generate a semantically equivalent fix for this isolated segment. This combined architecture successfully resolves complex repair cases intractable for rule-based repair while avoiding the semantic errors of LLM-only approaches. Our work provides a validated methodology for solving such problems in automated repair, improving the repair rate by 15.4%p over the state-of-the-art.",Sicheol Sung; Joonghyuk Hahn; Yo-Sub Han,Joonghyuk Hahn,Oral,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics","LLM Evaluation, Benchmarks & Metrics",repair; regular; regex; rule based; vulnerability; rule; symbolic; precise; automated; problem
1338-MAIN,Statistical Foundations of DIME: Risk Estimation for Practical Index Selection,"High-dimensional dense embeddings have become central to modern Information Retrieval, but many dimensions are noisy or redundant. Recently proposed DIME (Dimension IMportance Estimation), provides query-dependent scores to identify informative components of embeddings. DIME relies on a costly grid search to select a priori a dimensionality for all the query corpusвЂ™s embeddings. Our work provides a statistically grounded criterion that directly identifies the optimal set of dimensions for each query at inference time. Experiments confirm that this approach improves retrieval effectiveness and reduces embedding size by an average $~50$% of across different models and datasets at inference time.",Giulio D'Erasmo; Cesare Campagnano; Antonio Mallia; Pierpaolo Brutti; Nicola Tonellotto; Fabrizio Silvestri,Giulio D'Erasmo,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",embeddings; query; estimation; inference time; dimensions; provides; size average; identifies optimal; priori; average 50
1347-MAIN,"Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality","Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). However, it remains unclear whether these tests -- originally developed for humans -- yield meaningful results when applied to LLMs. In this study, we systematically evaluate the reliability and validity of human psychometric tests for three constructs: sexism, racism, and morality. We find moderate reliability across multiple item and prompt variations. Validity is evaluated through both convergent (i.e., testing theory-based inter-test correlations) and ecological approaches (i.e., testing the alignment between tests scores and behavior in real-world downstream tasks). Crucially, we find that psychometric test scores do not align, and in some cases even negatively correlate with, model behavior in downstream tasks, indicating low ecological validity. Our results highlight that systematic evaluations of psychometric tests are essential before interpreting their scores. They also suggest that psychometric tests designed for humans cannot be applied directly to LLMs without adaptation.",Jana Jung; Marlene Lutz; Indira Sen; Markus Strohmaier,Jana Jung,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",tests; psychometric; validity; ecological; sexism; scores; downstream tasks; testing; constructs; applied
1350-MAIN,ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations,"Large Language Models (LLMs) frequently confabulate scientific facts, but the mechanisms underlying these failures remain poorly understood. We introduce **Reddit False And Correct Texts** (ReFACT), a benchmark of 1,001 expert-annotated question-answer pairs with **span-level error annotations**, enabling fine-grained analysis of confabulation detection, localization, and correction. Evaluating 9 state-of-the-art LLMs reveals two fundamental limitations. First, models exhibit a dominant **salient distractor** failure mode: 61% of incorrect span predictions are semantically unrelated to actual errors, indicating models fixate on contextually prominent terms rather than true error locations. This pattern persists across model scales (1B to 70B), suggesting scaling alone is insufficient to address this limitation. Second, **comparative judgment** (selecting which of two answers contains factual confabulations) proves fundamentally harder than **independent judgment** (classifying a single answer as confabulated or not). For example, even GPT-4o's Fв‚Ѓ score drops from 0.67 to 0.53 when comparing factual versus confabulated answers side-by-side. This dramatic performance degradation challenges the reliability of LLM-as-Judge paradigms widely adopted in current benchmarks. Code and data are released on [anonymized](https://storage.googleapis.com/dataset-refact/refact-dataset.jsonl).",Yindong Wang; Martin PreiГџ; Margarita BugueГ±o; Jan Vincent Hoffbauer; Abdullatif Ghajar; Tolga Buz; Gerard de Melo,Yindong Wang,Oral,In-person,SALLE  LE RIAD,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics",Domain NLP (Biomedical/Clinical/Legal/Scientific),error annotations; error; judgment; span; scientific; answers; factual; annotations; answer; detection
1352-MAIN,Cosine Similarity as Logits?: A Scalable Knowledge Probe Using Embedding Vectors from Generative Language Models,"Recently, the use of pretrained language models (PLMs) as soft knowledge bases has gained growing interest, sparking the development of knowledge probes to evaluate their factual knowledge retrieval capabilities. However, existing knowledge probes for generative PLMs that support multi-token entities exhibit quadratic time complexity $\mathcal{O}(n^2)$, limiting the size of knowledge graphs used for probing. To address this, we propose DEcoder Embedding-based Relational (DEER) probe, utilizing embedding vectors extracted from generative PLMs. DEER probe achieves effective time complexity of linear order $\mathcal{O}(n)$, supports rank-based evaluation metrics including $\text{Hit@}k$, handles multi-token entity names and enables probing whilst disambiguation of homographic tail-enity names. We empirically show that DEER-probe correlates with existing knowledge probes, validating its probing capability, and we demonstrate the practical benefits of its improved scalability.",Tomoyuki Jinno; Kazuki Hayashi; Yusuke Sakai; Hidetaka Kamigaito; Taro Watanabe,Tomoyuki Jinno,Oral,In-person,SALLE  LE RIAD,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",Interpretability & Model Analysis,probe; knowledge; probes; probing; existing knowledge; multi token; generative; embedding; names; vectors
1359-MAIN,Generating Multi-Aspect Queries for Conversational Search,"Conversational information seeking (CIS) systems aim to model the user's information need within the conversational context and retrieve the relevant information. One major approach to modeling the conversational context aims to rewrite the user utterance in the conversation to represent the information need independently. In this work, we hypothesize that breaking down the information of an utterance into multiple queries covering different aspects of the information need can lead to more effective retrieval performance. This is more evident in more complex utterances that require gathering evidence from various information sources, where a single query rewrite or query representation cannot capture the complexity of the utterance. We propose MQ4CS, a multi-aspect query generation and retrieval framework, which uses Large Language Models (LLMs) to break the user utterance into multiple queries. This approach improves retrieval performance, as most utterances benefit from more than one rewritten query. We evaluate MQ4CS on six widely used CIS datasets, showing it outperforms state-of-the-art query rewriting methods. Using MQ4CS, we also construct MASQ, which includes multiple-aspect queries for the six datasets. Fine-tuning the \llama model on MASQ yields significant improvements. We make our code and dataset publicly available.",Zahra Abbasiantaeb; Simon Lupart; Mohammad Aliannejadi,Zahra Abbasiantaeb,Oral,In-person,Pavillon  DE RABAT,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Dialogue, Conversational & Interactive NLP","Retrieval, Grounding & External Knowledge (RAG)",utterance; information; query; conversational; aspect; queries; multi aspect; rewrite; conversational context; retrieval performance
1361-MAIN,Navigating the Infinite Dynamic Web Space: Effective In-Context Exploration via Cognitive Multi-Agent Collaboration,"Dynamic web navigation is challenging due to infinite decision space and the constantly changing nature of cyberspace. Existing methods rely on greedy strategies or value estimation, struggle to achieve effective backtracking and are heavily dependent on proprietary models. In this paper, we propose HintNavigator, a cognitive multi-agent collaboration framework that enhances cyberspace exploration capability through In-Context Exploration (ICE). Inspired by human cognitive planning process, we categorize the interaction history into Declarative History (environment observations) and Procedural History (action trajectories) to enhance historical reflection capability. These dual-history streams are dynamically integrated through specialized cognitive agents, enabling effective self-directed backtracking guided by working memory consolidation. Experiments show that HintNavigator achieves state-of-the-art performance among open-source LLM agents, surpassing proprietary model Claude-3.5 Sonnet on the WebArena benchmark.",Guozhao Mo; Yanjiang Liu; Yafei Shi; Jiawei Chen; Yang Li; Yaojie Lu; Hongyu Lin; Ben He; Le Sun; Bo Zheng; Xianpei Han,GuoZhao Mo,Oral,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",history; cognitive; exploration; dynamic web; multi agent collaboration; agent collaboration; infinite; collaboration; proprietary; effective
1366-MAIN,TimeMachine-bench: A Benchmark on Evaluating Model's Capability on Repository-level Migration Tasks,"With the advancement of automated software engineering, the main interest of research is shifting toward more real-world tasks that reflect the daily activities of software engineers. Despite its importance, software migration, a critical task of adapting code for newer environments, has been largely overlooked. In this work, we propose TimeMachine-bench, a benchmark centered on software migration tasks in real-world Python projects. Our benchmark consists of GitHub repositories whose tests begin to fail in response to dependency updates. The construction process is fully automated, enabling live updates of the benchmark. Furthermore, we curated a human-verified subset to guarantee the solvability of the problems. We evaluated the performance of agent-based baselines built on top of two state-of-the-art LLMs on the verified subset. Our results revealed that, while LLMs demonstrate a certain utility in migration tasks, they still face significant challenges, including spurious solutions in long-tail scenarios and unnecessary edits stemming from suboptimal tool-use strategies. We will release our dataset and implementation to accelerate further progress in this field.",Ryo Fujii; Makoto Morishita; Kazuki Yano; Jun Suzuki,Ryo Fujii,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents","Linguistics, Syntax & Semantics",software; bench benchmark; benchmark; updates; bench; verified; subset; tasks; automated; strategies release
1378-MAIN,Tandem Training for Language Models,"As language models continue to rapidly improve, we can expect their actions and reasoning to become difficult or impossible for weaker agents and humans to follow, undermining interpretability and oversight. With an eye on long-term futures, we pursue methods that encourage models to produce solutions that remain intelligible to weaker collaborators. We formalize intelligibility as handoff robustness: a strong model's solution is intelligible to a weaker model if randomly handing off control to the weaker model along the solution path does not cause failure. Building on this criterion, we introduce tandem training for language models, a reinforcement learning (RL) paradigm in which rollout tokens are intermittently and randomly sampled from a frozen weak model rather than the strong model being trained. Because rollouts succeed only when the strong model's actions and reasoning process can be continued by the weak model---when the two can co-construct a successful solution---optimizing standard RL objectives with tandem training implicitly incentivizes both correctness and intelligibility. In the GSM8K math reasoning task, tandem training reliably teaches models to abandon jargon and adapt their language to weaker partners while keeping task accuracy high. Our results demonstrate a promising route to building AI systems that remain auditable by weaker agents, with implications for human--AI collaboration and multi-agent communication.",Robert West; Ashton Anderson; Ece Kamar; Eric Horvitz,Robert West,Oral,In-person,SALLE  LA PALMERAIE,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Reasoning, Planning & Agents",Interpretability & Model Analysis,weaker; strong model; weak model; training language; intelligible; randomly; solution; actions; weak; training
1379-MAIN,Can MLLM Find Their Way in a City? Exploring Emergent Navigation from Web-Scale Knowledge,"Leveraging multimodal large language models (MLLMs) to construct embodied agents represents a promising approach for addressing real-world tasks. However, current benchmarks predominantly focus on language-centric tasks or heavily rely on simulated environments, thereby restricting their capacity to evaluate performance in realistic settings. To bridge this gap, we introduce CityNav, a comprehensive benchmark encompassing four diverse global cities, explicitly designed to evaluate the decision-making capabilities of raw MLLM-driven agents in real-world navigation tasks. Specifically, agents must rely solely on visual observations and internal multimodal reasoning to sequentially execute a significant number of decisions (50+) without additional environmental annotations or specialized architectural enhancements. Extensive evaluation reveals that popular linguistic reasoning techniques (e.g., Chain-of-Thought, Self-Consistency, Reflection) fail to deliver substantial improvements in performance. To address this, we propose Verbalization of Path (VoP), a novel method that explicitly grounds the agent's internal multimodal reasoning through verbalized navigational paths, substantially enhancing navigation success. Nonetheless, overall performance in particularly challenging cities remains insufficient, underscoring the critical necessity for advanced reasoning frameworks and robust methods capable of handling complex, long-range sequential decision-making tasks. The code and dataset will be released on acceptance.",Dwip Dalal; Utkarsh Mishra; Narendra Ahuja; Nebojsa Jojic,Dwip Dalal,Oral,In-person,Pavillon  DE RABAT,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",navigation; multimodal reasoning; mllm; agents; multimodal; decision making; reasoning; tasks; explicitly; decision
1381-MAIN,"Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models","Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.",Alla Chepurova; Aydar Bulatov; Mikhail Burtsev; Yuri Kuratov,Alla Chepurova,Oral,In-person,SALLE  LE LIXUS,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,kgs; triplets; kg construction; musique; ontology; knowledge graphs; construction; graphs; knowledge; pipeline
1385-MAIN,CAIRE: Cultural Attribution of Images with Retrieval,"As text-to-image models become increasingly prevalent, ensuring their equitable performance across diverse cultural contexts is critical. Efforts to mitigate cross-cultural biases have been hampered by trade-offs, including a loss in performance, factual inaccuracies, or offensive outputs. Despite widespread recognition of these challenges, an inability to reliably measure these biases has stalled progress. To address this gap, we introduce CAIRE, a novel evaluation metric that assesses the degree of cultural relevance of an image, given a user-defined set of labels. Our framework grounds entities and concepts in the image to a knowledge-base and uses factual information to give independent graded judgments for each culture label. On a manually curated dataset of culturally salient but rare items built using language models, CAIRE surpasses all baselines by F1 points. Additionally, we construct two datasets for culturally universal concepts, one comprising of T2I generated outputs and another retrieved from naturally-occurring data. CAIRE achieves PearsonвЂ™s correlations of and with human ratings on these sets, based on a 5-point Likert scale of cultural relevance. This demonstrates its strong alignment with human judgment across diverse image sources. Our code is here: https://anonymous.4open.science/r/CAIRE-Anon-E204, and our tool will be open sourced upon publication.",Arnav Yayavaram; Siddharth Yayavaram; Simran Khanuja; Michael Saxon; Graham Neubig,Siddharth Yayavaram,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",cultural; image; cultural relevance; culturally; concepts; relevance; biases; factual; outputs; code https anonymous
1395-MAIN,What Does Infect Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs,"In this paper, we introduce S-MedQA, an English medical question-answering (QA) dataset for benchmarking large language models (LLMs) in fine-grained clinical specialties. S-MedQA has over 20k examples, covers 15 medical specialties, and QA pairs can have multiple specialty annotations (e.g., when a question is cross-disciplinary), constructed with both machine and expert verification to maximize data availability. We use S-MedQA to investigate the role of clinical specialty data in the knowledge-intensive scenario of medical QA. Our results show that 1) training on data from a clinical specialty does not necessarily lead to best performance on that specialty, and 2) regardless of the specialty the LLM was fine-tuned on, token probabilities of clinically relevant terms increase consistently across all specialties. Thus, we hypothesize improvement gains are derived mostly from domain shifting (e.g., general to medical) rather than specialty-specific knowledge injection, and suggest rethinking the role of fine-tuning data in the medical domain. To motivate further advancements in the clinical NLP field, we release S-MedQA and all code needed to reproduce all our experiments to the research community.",Xinlan Yan; Di Wu; Yibin Lei; Christof Monz; Iacer Calixto,Xinlan Yan,Oral,In-person,SALLE  LE LIXUS,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),,specialty; medical; medqa; clinical; clinical specialty; specialties; data medical; role; does; fine
1405-MAIN,Redefining Retrieval Evaluation in the Era of LLMs,"Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR, assume that human users sequentially examine documents with diminishing attention to lower ranks. This assumption breaks down in Retrieval Augmented Generation (RAG) systems, where search results are consumed by Large Language Models (LLMs), which, unlike humans, process all retrieved documents as a whole rather than sequentially. Additionally, traditional IR metrics do not account for related but irrelevant documents that actively degrade generation quality, rather than merely being ignored. Due to these two major misalignments, namely human vs. machine position discount and human relevance vs. machine utility, classical IR metrics do not accurately predict RAG performance. We introduce a utility-based annotation schema that quantifies both the positive contribution of relevant passages and the negative impact of distracting ones. Building on this foundation, we propose UDCG (Utility and Distraction-aware Cumulative Gain), a metric using an LLM-oriented positional discount to directly optimize the correlation with the end-to-end answer accuracy. Experiments on five datasets and six LLMs demonstrate that UDCG improves correlation by up to 36\% compared to traditional metrics. Our work provides a critical step toward aligning IR evaluation with LLM consumers and enables more reliable assessment of RAG components.",Giovanni Trappolini; Florin Cuconasu; Simone Filice; Yoelle Maarek; Fabrizio Silvestri,Giovanni Trappolini,Oral,In-person,Pavillon  DE RABAT,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Retrieval, Grounding & External Knowledge (RAG)","LLM Evaluation, Benchmarks & Metrics",metrics; utility; documents; rag; sequentially; traditional; correlation; retrieval; machine; end
1411-MAIN,"Debate, Deliberate, Decide (D3): A Cost-Aware Adversarial Framework for Reliable and Interpretable LLM Evaluation","The evaluation of Large Language Models (LLMs) remains challenging due to inconsistency, bias, and the absence of transparent decision criteria in automated judging. We present Debate, Deliberate, Decide (D3), a cost-aware, adversarial multi-agent framework that orchestrates structured debate among role-specialized agents (advocates, a judge, and an optional jury) to produce reliable and interpretable evaluations. D3 instantiates two complementary protocols: (1) Multi-Advocate One-Round Evaluation (MORE), which elicits $k$ parallel defenses per answer to amplify signal via diverse advocacy, and (2) Single-Advocate Multi-Round Evaluation (SAMRE) with budgeted stopping, which iteratively refines arguments under an explicit token budget and convergence checks. We develop a probabilistic model of score gaps that (i) characterizes reliability and convergence under iterative debate and (ii) explains the separation gains from parallel advocacy. Under mild assumptions, the posterior distribution of the round-$r$ gap concentrates around the true difference and the probability of mis-ranking vanishes; moreover, aggregating across $k$ advocates provably increases expected score separation. We complement theory with a rigorous experimental suite across MT-Bench, AlignBench, and AUTO-J, showing state-of-the-art agreement with human judgments (accuracy and Cohen's $\kappa$), reduced positional and verbosity biases via anonymization and role diversification, and a favorable cost-accuracy frontier enabled by budgeted stopping. Ablations and qualitative analyses isolate the contributions of debate, aggregation, and anonymity. Together, these results establish D3 as a principled, practical recipe for reliable, interpretable, and cost-aware LLM evaluation.",Abir HARRASSE; Chaithanya Bandi; Hari Bandi,Abir Harrasse,Oral,In-person,SALLE  LE RIAD,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",debate; reliable interpretable; cost aware; round; cost; decide; advocate; llm evaluation; interpretable; stopping
12-FIND,Unveiling the Deficiencies of Pre-trained Text-and-Layout Models in Real-world Visually-rich Document Information Extraction,"Recently developed pre-trained text-and-layout models (PTLMs) have shown remarkable success in multiple information extraction tasks on visually-rich documents (VrDs). However, despite achieving extremely high performance on benchmarks, their real-world performance falls short of expectations. Owing to this issue, we investigate the prevailing evaluation pipeline to reveal that: (1) The inadequate annotations within benchmark datasets introduce spurious correlations between task inputs and labels, which would lead to overly-optimistic estimation of model performance. (2) The evaluation solely relies on the performance on benchmarks and is insufficient to comprehensively explore the capabilities of methods in real-world scenarios. These problems impede the prevailing evaluation pipeline from reflecting the real-world performance of methods, misleading the design choices of method optimization. In this work, we introduce EC-FUNSD, an entity-centric dataset crafted for benchmarking information extraction from visually-rich documents. This dataset contains diverse layouts and high-quality annotations. Additionally, this dataset disentangles the falsely-coupled segment and entity annotations that arises from the block-level annotation of FUNSD. Using the proposed dataset, we evaluate the real-world information extraction capabilities of PTLMs from multiple aspects, including their absolute performance, as well as generalization, robustness and fairness. The results indicate that prevalent PTLMs do not perform as well as anticipated in real-world information extraction scenarios. We hope that our study can inspire reflection on the directions of PTLM development.",Chong Zhang; Yixi Zhao; Yulu Xie; Chenshu Yuan; Yi Tu; Ya Guo; Mingxu Chai; Ziyu Shen; Yue Zhang; Qi Zhang,Chong Zhang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Information Extraction & Structured Prediction,"LLM Evaluation, Benchmarks & Metrics",information extraction; extraction; visually rich; real world; world; real; visually; information; visually rich documents; performance benchmarks
27-FIND,Entity-aware Cross-lingual Claim Detection for Automated Fact-checking,"Identifying claims requiring verification is a critical task in automated fact-checking, especially given the proliferation of misinformation on social media platforms. Despite notable progress, challenges remainвЂ”particularly in handling multilingual data prevalent in online discourse. Recent efforts have focused on fine-tuning pre-trained multilingual language models to address this. While these models can handle multiple languages, their ability to effectively transfer cross-lingual knowledge for detecting claims spreading on social media remains under-explored. In this paper, we introduce EX-Claim, an entity-aware cross-lingual claim detection model that generalizes well to handle multilingual claims. The model leverages entity information derived from named entity recognition and entity linking techniques to improve the language-level performance of both seen and unseen languages during training. Extensive experiments conducted on three datasets from different social media platforms demonstrate that our proposed model stands out as an effective solution, demonstrating consistent performance gains across 27 languages and robust knowledge transfer between languages seen and unseen during training.",Rrubaa Panchendrarajan; Arkaitz Zubiaga,Rrubaa Panchendrarajan,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multilinguality & Low-Resource NLP,Information Extraction & Structured Prediction,entity; claim; social media; claims; media; aware cross lingual; seen unseen; aware cross; social media platforms; cross lingual
31-FIND,WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning,"Large language model (LLM)-empowered web agents enable automating complex, real-time web navigation tasks in enterprise environments. However, existing web agents relying on supervised fine-tuning (SFT) often struggle with generalization and robustness due to insufficient reasoning capabilities when handling the inherently dynamic nature of web interactions. In this study, we introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based R1-style reinforcement learning framework explicitly designed to enhance single-step reasoning and planning for business-oriented web navigation tasks. We employ a structured reward function that evaluates both adherence to output formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly learn robust intermediate reasoning without explicit annotations or extensive expert demonstrations. Extensive experiments on the WorkArena benchmark demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by 10.26вЂ“16.59%, achieving competitive performance relative to proprietary LLM-based agents (GPT-4o) in workplace-oriented web navigation tasks.",Yuchen Zhuang; Di Jin; Jiaao Chen; Wenqi Shi; Hanrui Wang; Chao Zhang,Yuchen Zhuang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",,web; web agents; navigation tasks; web navigation; navigation; agents; llm based; sft; oriented; reasoning
36-FIND,Being Kind IsnвЂ™t Always Being Safe: Diagnosing Affective Hallucination in LLMs,"Large Language Models (LLMs) are increasingly engaged in emotionally vulnerable conversations that extend beyond information seeking to moments of personal distress. As they adopt affective tones and simulate empathy, they risk creating the illusion of genuine relational connection. We term this phenomenon \aha{}, referring to emotionally immersive responses that evoke false social presence despite the modelвЂ™s lack of affective capacity. To address this, we introduce \ahabench{}, a benchmark of 500 mental health-related prompts with expert-informed reference responses, evaluated along three dimensions: \emph{Emotional Enmeshment}, \emph{Illusion of Presence}, and \emph{Fostering Overdependence}. We further release \ahapairs{}, a 5K-instance preference dataset enabling Direct Preference Optimization (DPO) for alignment with emotionally responsible behavior. DPO fine-tuning substantially reduces affective hallucination without compromising reasoning performance, and human evaluations confirm AHaBench as an effective diagnostic tool. This work establishes affective hallucination as a distinct safety concern and provides resources for developing LLMs that are both factually reliable and psychologically safe. Warning: This paper contains examples of mental health-related language that may be emotionally distressing.",Sewon Kim; Jiwon Kim; SeungWoo Shin; Hyejin Chung; Daeun Moon; Yejin Kwon; Hyunsoo Yoon,Sewon Kim,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Reasoning, Planning & Agents",affective; emotionally; emph; hallucination; health related; illusion; mental health; presence; mental; dpo
47-FIND,Aligning Large Vision-Language Models via Joint Multimodal Preference Optimization,"Recent research has focused on addressing multimodal hallucination in Large Vision-Language Models (LVLMs) by extending Direct Preference Optimization (DPO) to incorporate visual preference supervision. However, these methods often lack fine-grained visual contrast mechanisms and rely on single-margin optimization, which limits their ability to capture precise visual semantics and results in weak multimodal alignment. To address these issues, we propose Joint Multimodal Preference Optimization (JoMPO), a novel optimization framework that symmetrically integrates a text-conditioned preference loss with a visual ranking-based objective. JoMPO leverages semantically contrastive imageвЂ“text pairs and listwise ranking over multiple visual contexts, enabling fine-grained visual grounding and more robust cross-modal alignment. To support this framework, we introduce the VisualвЂ“Textual Contrast (VTC) dataset, consisting of image pairs that are semantically similar but visually distinct, each paired with a contextually grounded textual response. Trained with only 5k contrastive pairs, JoMPO consistently demonstrates superior performance across diverse benchmarks, highlighting its effectiveness in mitigating hallucinations and improving image-text alignment in LVLMs.",Jiwon Kim; Hyunsoo Yoon,Jiwon Kim,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,,visual; preference; optimization; preference optimization; multimodal; fine grained visual; grained visual; pairs; lvlms; joint
51-FIND,Let's Put Ourselves in Sally's Shoes: Shoes-of-Others Prefixing Improves Theory of Mind in Large Language Models,"Recent studies have shown that Theory of Mind (ToM) in large language models (LLMs) has not reached human-level performance yet. Since fine-tuning LLMs on ToM datasets often degrades their generalization, several inference-time methods have been proposed to enhance ToM in LLMs. However, existing inference-time methods for ToM are specialized for inferring beliefs from contexts involving changes in the world state. In this study, we present a new inference-time method for ToM, Shoes-of-Others (SoO) prefixing, which makes fewer assumptions about contexts and is applicable to broader scenarios. SoO prefixing simply specifies the beginning of LLM outputs with ``Let's put ourselves in A's shoes.'', where A denotes the target character's name. We evaluate SoO prefixing on two benchmarks that assess ToM in conversational and narrative contexts without changes in the world state and find that it consistently improves ToM across five categories of mental states. Our analysis suggests that SoO prefixing elicits faithful thoughts, thereby improving the ToM performance.",Kazutoshi Shinoda; Nobukatsu Hojo; Kyosuke Nishida; Yoshihiro Yamazaki; Keita Suzuki; Hiroaki Sugiyama; Kuniko Saito,Kazutoshi Shinoda,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP","Efficiency, Scaling & NLP Systems",tom; inference time; let; theory mind; mind; contexts; time; inference; changes; theory
62-FIND,Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey,"Plane geometry problem solving (PGPS) has recently gained significant attention as a benchmark to assess the multi-modal reasoning capabilities of large vision-language models. Despite the growing interest in PGPS, the research community still lacks a comprehensive overview that systematically synthesizes recent work in PGPS. To fill this gap, we present a survey of existing PGPS studies. We first categorize PGPS methods into an encoder-decoder framework and summarize the corresponding output formats used by their encoders and decoders. Subsequently, we classify and analyze these encoders and decoders according to their architectural designs. Finally, we outline major challenges and promising directions for future research. In particular, we discuss the hallucination issues arising during the encoding phase within encoder-decoder architectures, as well as the problem of data leakage in current PGPS benchmarks.",Seunghyuk Cho; Zhenyue Qin; Yang Liu; Youngbin Choi; Seungbeom Lee; Dongwoo Kim,Seunghyuk Cho,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Multimodal & Speech/Audio,"Trustworthy, Safety, Privacy & Fairness",geometry problem solving; geometry problem; decoders; multi modal reasoning; encoder decoder; modal reasoning; geometry; problem; multi modal; problem solving
63-FIND,Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms,"Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance.",Kieran Henderson; Kian Omoomi; Vasudha Varadarajan; Allison Lahnala; Charles Welch,Kieran Henderson,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",,annotator; self; self disclosure; personal information; disclosure; norms; personal; information; predicting; sentences
64-FIND,Position Paper: How Should We Responsibly Adopt LLMs in the Peer Review Process?,"This position paper presents a novel perspective on the utilization of Large Language Models (LLMs) in the artificial intelligence paper review process. We first critique the current tendency for LLMs to be primarily used for simple review text generation, arguing instead that this approach overlooks more meaningful applications of LLMs that preserve human expertise at the core of evaluation. Instead, we advocate for leveraging LLMs to support key aspects of the review processвЂ”specifically, verifying the reproducibility of experimental results, checking the correctness and relevance of citations, and assisting with ethics review flagging. For example, integrating tools based on LLM Agents for code generation from research papers has recently enabled automated assessment of the reproducibility of the paper, thereby improving the transparency and reliability of research. By reorienting LLM usage toward these targeted and assistive roles, we outline a pathway for more effective and responsible integration of LLMs into peer review, ultimately supporting both reviewer efficiency and the integrity of the scientific process.",Juhwan Choi; JungMin Yun; Changhun Kim; YoungBin Kim,Juhwan Choi,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Summarization & Generation,"LLM Evaluation, Benchmarks & Metrics",review; position paper; review process; peer review; reproducibility; peer; llms; position; process; instead
65-FIND,Rad-Flamingo: A Multimodal Prompt driven Radiology Report Generation Framework with Patient-Centric Explanations,"In modern healthcare, radiology plays a pivotal role in diagnosing and managing diseases. However, the complexity of medical imaging data and the variability in interpretation can lead to inconsistencies and a lack of patient-centered insight in radiology reports. To address this challenge, a novel multimodal prompt-driven report generation framework Rad-Flamingo was developed, that integrates diverse data modalitiesвЂ”such as medical images, and clinical notesвЂ”to produce comprehensive and context-aware radiology reports. Our framework leverages innovative prompt engineering techniques to guide vision-language models in generating relevant information, ensuring these generated reports are not only accurate but also understandable to individual patients. A key feature of our framework is its ability to provide patient-centric explanations, offering clear and personalized insights into diagnostic findings and their implications. Additionally, we also demonstrate a synthetic data generation pipeline, to append any existing benchmark datasets' findings and impressions with patient-centric explanation. Experimental results demonstrate that this framework's effectiveness in enhancing report quality, improving understandability, and could foster better patient-doctor communication. This approach represents a significant step towards human-centered medical AI systems.",Md. Tousin Akhter; Devansh Lalwani; Kshitij Sharad Jadhav; Pushpak Bhattacharyya,MD TOUSIN AKHTER,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),Multimodal & Speech/Audio,patient; radiology; report; reports; centric; prompt driven; rad; medical; report generation; generation framework
69-FIND,I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search,"Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low diversity and suboptimal code generation. While recent work~\cite{chi2024sela} has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (\textbf{\textit{I-MCTS}}), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making process. Furthermore, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node's solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from estimated score to actual performance scores. Applied to the various ML tasks, our approach demonstrates a 6% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems. Resource available at \url{https://anonymous.4open.science/r/I-MCTS}",Zujie Liang; Feng Wei; Wujiang Xu; Yuxi qian; Lin Chen; Xinhui Wu,Zujie Liang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",tree; carlo tree search; monte carlo tree; introspective; carlo tree; tree search; monte carlo; monte; carlo; node
74-FIND,ThinkNote: Enhancing Knowledge Integration and Utilization of Large Language Models via Constructivist Cognition Modeling,"Large Language Models (LLMs) have demonstrated strong performance across a wide range of NLP tasks. However, they often exhibit suboptimal behaviors and inconsistencies when exposed to unfamiliar external information, underscoring their limitations in effectively leveraging such knowledge. Inspired by constructivist learning theory, we propose ThinkNote, a novel framework that enhances the external knowledge utilization of LLMs through a two-stage constructivist cognitive modeling process. Specifically, ThinkNote performs knowledge assimilation to align new information with the model's parametric memory, forming a coherent internal representation. It then applies thought accommodation to adapt internal reasoning, thereby promoting more consistent and reliable outputs. Extensive experimental results demonstrate that ThinkNote achieves a 10% improvement over strong baseline methods on various question-answering benchmarks. Further analysis indicates that ThinkNote effectively integrates and utilizes external knowledge to help LLMs generate accurate responses and improves their self-consistency. All data and code will be publicly available on GitHub.",Zhipeng Xu; Zhenghao Liu; Yukun Yan; Shuo Wang; Shi Yu; Zheni Zeng; Chaojun Xiao; Zhiyuan Liu; Ge Yu; Chenyan Xiong,Zhipeng Xu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Interpretability & Model Analysis,knowledge; external; utilization; external knowledge; internal; modeling; effectively; strong baseline; modeling large; cognitive modeling
84-FIND,Mitigating Copy Bias in In-Context Learning through Neuron Pruning,"Large language models (LLMs) have demonstrated impressive few-shot in-context learning (ICL) abilities. Still, we show that they are sometimes prone to a `copying bias', where they copy answers from provided examples instead of learning the underlying patterns. In this work, we propose a novel and simple method to mitigate such copying bias. First, we create a synthetic task and use the Integrated Gradients method to identify neurons that prioritize copying over generalization. We demonstrate that pruning these neurons consistently improves performance across a diverse set of ICL tasks, including both single-token and multi-token scenarios, while maintaining or even improving the model's general capabilities. We also show that our method is applicable across various LLM architectures, including Transformers and State-Space Models, without requiring modifications. In our analysis, we adopt a task-recognition perspective on ICL and examine task vectors (Hendel et al., 2023) induced by the model. We find that pruning enhances the quality of these vectors, suggesting that the pruned neurons previously hindered effective task recognition.",Ameen Ali Ali; Lior Wolf; Ivan Titov,ameen ali,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Interpretability & Model Analysis,"Trustworthy, Safety, Privacy & Fairness",copying; icl; neurons; pruning; bias; context learning; vectors; learning; recognition; token
95-FIND,How to Make LMs Strong Node Classifiers?,"Language Models (LMs) are increasingly challenging the dominance of domain-specific models, such as Graph Neural Networks (GNNs) and Graph Transformers (GTs), in graph learning tasks. Following this trend, we propose a novel approach that empowers off-the-shelf LMs to achieve performance comparable to state-of-the-art (SOTA) GNNs on node classification tasks, without requiring any architectural modifications. By preserving the LM's original architecture, our approach retains a key benefit of LM instruction tuning: the ability to jointly train on diverse datasets, fostering greater flexibility and efficiency. To achieve this, we introduce two key augmentation strategies: (1) Enriching LMs' input using topological and semantic retrieval methods, which provide richer contextual information, and (2) guiding the LMs' classification process through a lightweight GNN classifier that effectively prunes class candidates. Our experiments on real-world datasets show that backbone Flan-T5 LMs equipped with these augmentation strategies outperform SOTA text-output node classifiers and are comparable to top-performing vector-output node classifiers. By bridging the gap between specialized node classifiers and general LMs, this work paves the way for more versatile and widely applicable graph learning models. We will open-source the code upon publication.",Zhe Xu; Kaveh Hassani; Si Zhang; Hanqing Zeng; Michihiro Yasunaga; Limei Wang; Dongqi Fu; Ning Yao; Bo Long; Hanghang Tong,Zhe Xu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",lms; node; classifiers; graph; graph learning; gnns; augmentation strategies; sota; augmentation; comparable
100-FIND,Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives,"Training large language models with data collected from various domains can improve their performance on downstream tasks. However, given a fixed training budget, the sampling proportions of these different domains significantly impact the model's performance. How can we determine the domain weights across different data domains to train the best-performing model within constrained computational resources? In this paper, we provide a comprehensive overview of existing data mixture methods. First, we propose a fine-grained categorization of existing methods, extending beyond the previous offline and online classification. Offline methods are further grouped into heuristic-based, algorithm-based, and function fitting-based methods. For online methods, we categorize them into three groupsвЂ”online min-max optimization, online mixing law, and other approachesвЂ”by drawing connections with the optimization frameworks underlying offline methods. Second, we summarize the problem formulations, representative algorithms for each subtype of offline and online methods, and clarify the relationships and distinctions among them. Finally, we discuss the advantages and disadvantages of each method and highlight key challenges in the field of data mixture.",Yajiao LIU; Congliang Chen; Junchi YANG; Ruoyu Sun,Yajiao Liu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",,online; offline; data mixture; methods; mixture; domains; optimization; highlight key; models data; different data
107-FIND,The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable Schoolbooks,"The five idioms (i.e., varieties) of the Romansh language are largely standardized and are taught in the schools of the respective communities in Switzerland. In this paper, we present the first parallel corpus of Romansh idioms. The corpus is based on 291 schoolbook volumes, which are comparable in content for the five idioms. We use automatic alignment methods to extract 207k multi-parallel segments from the books, with more than 2M tokens in total. A small-scale human evaluation confirms that the segments are highly parallel, making the dataset suitable for NLP applications such as machine translation between Romansh idioms. We release the dataset under a CC-BY-NC-SA license and demonstrate its utility for machine translation by training and evaluating an LLM on a sample of the dataset.",Zachary William Hopton; Jannis Vamvas; Andrin BГјchler; Anna Rutkiewicz; Rico Cathomas; Rico Sennrich,Zachary William Hopton,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Machine Translation,"LLM Evaluation, Benchmarks & Metrics",idioms; parallel; corpus; segments; machine translation; comparable; translation; machine; dataset; small scale human
108-FIND,Unleashing the Unseen: Harnessing Benign Datasets for Jailbreaking Large Language Models,"Despite significant ongoing efforts in safety alignment, large language models (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks that can induce harmful behaviors, including through the use of adversarial suffixes. Building on prior research, we hypothesize that these adversarial suffixes are not mere bugs but may represent features that can dominate the LLM's behavior. To evaluate this hypothesis, we conduct several experiments. First, we demonstrate that benign features can be effectively made to function as adversarial suffixes, i.e., we develop a feature extraction method to extract sample-agnostic features from benign dataset in the form of suffixes and show that these suffixes may effectively compromise safety alignment. Second, we show that adversarial suffixes generated from jailbreak attacks may contain meaningful features, i.e., appending the same suffix to different prompts results in responses exhibiting specific characteristics. Third, we show that such benign-yet-safety-compromising features can be easily introduced through fine-tuning using only benign datasets. As a result, we are able to completely eliminate GPT's safety alignment in a blackbox setting through finetuning with only benign data. Our code and data is available at \url{anonymous.4open.science/r/suffix-maybe-features-D17C/}.",Wei Zhao; Zhe Li; Yige Li; Jun Sun,Wei Zhao,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",,benign; features; adversarial; safety alignment; safety; benign datasets; suffix; jailbreak attacks; jailbreak; alignment
109-FIND,JEEM: Vision-Language Understanding in Four Arabic Dialects,"We introduce JEEM, a benchmark designed to evaluate Vision-Language Models (VLMs) on visual understanding across four Arabic-speaking countries: Jordan, The Emirates, Egypt, and Morocco. JEEM includes the tasks of image captioning and visual question answering, and features culturally rich and regionally diverse content. This dataset aims to assess the ability of VLMs to generalize across dialects and accurately interpret cultural elements in visual contexts. In an evaluation of five prominent open-source Arabic VLMs and GPT-4o, we find that the Arabic VLMs consistently underperform, struggling with both visual understanding and dialect-specific generation. While GPT-4o ranks best in this comparison, the modelвЂ™s linguistic competence varies across dialects, and its visual understanding capabilities lag behind. This underscores the need for more inclusive models and the value of culturally-diverse evaluation paradigms.",Karima Kadaoui; Hanin atwany; Hamdan Al-Ali; Abdelrahman Mohamed; Ali Mekky; Sergei Tilga; Natalia Fedorova; Ekaterina Artemova; Hanan Aldarmaki; Yova Kementchedjhieva,Karima Kadaoui,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",dialects; arabic; visual understanding; visual; vlms; understanding arabic; understanding; culturally; gpt 4o; gpt
113-FIND,Detecting Primary Progressive Aphasia (PPA) from Text: A Benchmarking Study,"Classifying subtypes of primary progressive aphasia (PPA) from connected speech presents significant diagnostic challenges due to overlapping linguistic markers. This study benchmarks the performance of traditional machine learning models with various feature extraction techniques, transformer-based models, and large language models (LLMs) for PPA classification. Our results indicate that while transformer-based models and LLMs exceed chance-level performance in terms of balanced accuracy, traditional classifiers combined with contextual embeddings remain highly competitive. Notably, MLP using MentalBertвЂ™s embeddings achieves the highest accuracy. These findings underscore the potential of machine learning for enhancing the automatic classification of PPA subtypes.",Ghofrane Merhbene; Fabian Lecron; Philippe Fortemps; Bradford C. Dickerson; Mascha Kurpicz-Briki; Neguine Rezaii,Ghofrane Merhbene,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multimodal & Speech/Audio,"Linguistics, Syntax & Semantics",ppa; transformer based models; progressive; transformer based; based models; primary; machine learning; transformer; embeddings; machine
116-FIND,LayerNorm vs RMSNorm: Geometric Perspective and a Case Against Mean Subtraction,"This paper presents a novel geometric interpretation of LayerNorm and explores how LayerNorm influences the norm and orientation of hidden vectors in the representation space. We show that the definition of LayerNorm is innately linked to the uniform vector, defined as . We then show that the standardization step in LayerNorm can be understood in three simple steps: (i) remove the component of a vector along the uniform vector, (ii) normalize the remaining vector, and (iii) scale the resultant vector by, where is the dimensionality of the representation space. Finally, we compare the hidden representations of LayerNorm-based LLMs with models trained using RMSNorm and show that all LLMs naturally operate orthogonal to the uniform vector both during training and inference, that is, on average they do not have a component along the uniform vector during training or inference. This presents the first mechanistic evidence that removing the component along the uniform vector in LayerNorm is a redundant step. These results advocate for using RMSNorm over LayerNorm which is also more computationally efficient.",Akshat Gupta; Atahan Ozdemir; Caoqinwei Gong; Gopala Anumanchipalli,Akshat Gupta,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Efficiency, Scaling & NLP Systems",vector; uniform; rmsnorm; component; representation space; geometric; training inference; hidden; presents; representation
118-FIND,Continual Pretraining on Encrypted Synthetic Data for Privacy-Preserving LLMs,"Preserving privacy in sensitive data while pretraining large language models on small, domain-specific corpora presents a significant challenge. To address this, we propose an entity-based method that preserves personally identifiable information (PII) by synthesizing high-quality encrypted data using a weighted graph. This enables LLMs to encode new knowledge within their parameters by continual pretraining on encrypted synthetic data while granting authorized access to sensitive data through decryption keys. Our results show that they outperform base models and ensure PII security, while encrypted models perform slightly worse than those trained on unencrypted synthetic data. Incorporating a larger number of entities and leveraging the weighted graph in data synthesis improves LLM performance. Furthermore, encrypted models maintain instruction-following abilities with long retrieved contexts. This approach offers an effective solution for privacy-preserving pretraining in domain-specific applications, ensuring both security and utility.",Honghao Liu; Xuhui Jiang; Chengjin Xu; Cehao Yang; Yiran Cheng; Lionel Ni; Jian Guo,Honghao Liu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",,pretraining; synthetic data; privacy; sensitive data; continual pretraining; preserving; pii; privacy preserving; synthetic; security
126-FIND,Do Diacritics Matter? Evaluating the Impact of Arabic Diacritics on Tokenization and LLM Benchmarks,"Diacritics are orthographic marks added to letters to specify pronunciation, disambiguate lexical meanings, or indicate grammatical distinctions. Diacritics can significantly influence language processing tasks, especially in languages like Arabic, where diacritic usage varies widely across domains and contexts. While diacritics provide valuable linguistic information, their presence can increase subword fragmentation during tokenization, potentially degrading the performance of NLP models. In this paper, we systematically analyze the impact of diacritics on tokenization and benchmark task performance across major Large Language Models (LLMs). Our results demonstrate that while modern LLMs show robustness to the limited diacritics naturally found in texts, full diacritization leads to substantially increased token fragmentation and degraded performance, highlighting the need for careful handling of diacritics in the future development of Arabic LLMs.",Go Inoue; Bashar Alhafni; Nizar Habash; Timothy Baldwin,Go Inoue,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Linguistics, Syntax & Semantics",diacritics; tokenization; arabic; fragmentation; impact; degraded performance; tasks especially; performance nlp; benchmark task; need careful
127-FIND,Pelican Soup Framework: A Theoretical Framework for Language Model Capabilities,"In this work, we propose a simple theoretical framework, Pelican Soup, aiming to better understand how pretraining allows LLMs to (1) generalize to unseen instructions and (2) perform in-context learning, even when the verbalizers are irrelevant to the task. To this end, in our framework, we introduce the notion of ""knowledge base"" and ""reference-sense association"" and a simple formalism for natural language processing tasks. Our framework demonstrates how linguistic, psychology, and philosophy studies can inform our understanding of the language model and is connected to several other existing theoretical results. As an illustration of the usage of our framework, we derive a bound on in-context learning loss with our framework. Finally, we support our framework with empirical experiments and provide possible future research directions.",Ting-Rui Chiang; Dani Yogatama,Ting-Rui Chiang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)","Linguistics, Syntax & Semantics",framework; theoretical; theoretical framework; context learning; simple; language model; understanding language; empirical experiments; research directions; illustration
130-FIND,"I Know, but I Don't Know! How Persona Conflict Undermines Instruction Adherence in Large Language Models","Large Language Models (LLMs) are expected to generate appropriate responses while adhering to predefined prior constraints or knowledge, such as user personas, across various dialogue scenarios. However, real-world interactions frequently involve semantic conflicts between such prior information and actual user-provided inputs. Despite this, prior studies on persona-grounded dialogueвЂ”one of the representative tasks in personal preference modelingвЂ”have predominantly assumed idealized scenarios where persona and user utterances are fully aligned. To bridge this gap, we introduce and formalize the notion of persona conflict, wherein predefined personas contradict the personal information expressed by the user during interaction. We present a systematic verification framework to examine model behavior under such conflict scenarios. In detail, we propose a taxonomy that categorizes model behaviors into three distinct response types (adhering, sycophantic, and wavering) and develop a measurement schema grounded in this taxonomy. Our study provides a comprehensive analysis of the persona conflict phenomenon, identifying diverse key behavioral factors. Extensive experiments and in-depth analysis provide new insights into designing robust dialogue models capable of managing persona inconsistencies.",Seonmin Koo; Jinsung Kim; Heuiseok Lim,Seonmin Koo,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP",Interpretability & Model Analysis,persona; conflict; user; adhering; personal; know; predefined; personas; scenarios; prior
133-FIND,Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models,"Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at https://anonymous.4open.science/r/european_parliament_simulation-7285.",Maximilian Kreutner; Marlene Lutz; Markus Strohmaier,Maximilian Kreutner,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Summarization & Generation,"Linguistics, Syntax & Semantics",persona; european; voting; accurately predict; display; behavior; groups; predict; accurately; produce
144-FIND,Exploring Iterative Controllable Summarization with Large Language Models,"Large language models (LLMs) have demonstrated remarkable performance in abstractive summarization tasks. However, their ability to precisely control summary attributes (e.g., length or topic) remains underexplored, limiting their adaptability to specific user preferences. In this paper, we systematically explore the controllability of LLMs. To this end, we revisit summary attribute measurements and introduce iterative evaluation metrics, failure rate and average iteration count, to more precisely evaluate controllability beyond assessment of errors. Our findings show that LLMs struggle more with numerical attributes than with linguistic attributes. To address this challenge, we propose a guide-to-explain framework (GTE) for controllable summarization. GTE enables the model to identify misaligned attributes in the initial draft and guides it to self-explain errors in the previous output. By encouraging reflection on attribute misalignment, GTE generates well-adjusted summaries that satisfy the desired attributes with robust effectiveness while requiring surprisingly fewer iterations than other iterative approaches.",Sangwon Ryu; Heejin Do; Daehui Kim; Hwanjo Yu; Dongwoo Kim; Yunsu Kim; Gary Lee; Jungseul Ok,Sangwon Ryu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,"LLM Evaluation, Benchmarks & Metrics",attributes; iterative; summarization; controllability; precisely; explain; summary; attribute; controllable; errors
147-FIND,"The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models","Negotiation is a fundamental challenge for AI agents, as it requires an ability to reason strategically, model opponents, and balance cooperation with competition. We conduct the first comprehensive study systematically evaluating the effect of (LLM-)reasoning on the negotiation abilities of both commercial and open-weight LLMs, and do this across three languages. Using a self-play setup across three diverse dialogue games, we analyse trade-offs between performance and cost, the language consistency of reasoning processes, and the nature of strategic adaptation exhibited by models. Our findings show that enabling reasoning - that is, scaling test time compute -significantly improves negotiation outcomes by enhancing collaboration and helping models overcome task complexities, but comes at a substantial computational cost: reasoning improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400 %. Most critically, we uncover a significant multilingual reasoning distinction: open-weight models consistently switch to English for their internal reasoning steps, even when negotiating in German or Italian (and thus possibly impacting potential explainability gains through the disclosure of reasoning traces), while leading commercial models maintain language consistency between their reasoning and final output.",Sherzod Hakimov; Roland Bernard; Tim Leiber; Karl Osswald; Kristina Richert; Ruilin Yang; Raffaella Bernardi; David Schlangen,Sherzod Hakimov,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents",Multilinguality & Low-Resource NLP,reasoning; cost; language consistency; performance cost; open weight; commercial; weight; consistency; challenge ai; performance 31
148-FIND,ART: Adaptive Reasoning Trees for Explainable Claim Verification,"Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.",Sahil Wadhwa; Himanshu Kumar; Guanqun Yang; Abbaas Alif Mohamed Nishar; Pranab Mohanty; Swapnil Shinde; Yue Wu,Sahil Wadhwa,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents",,claim; claim verification; trees; explainable claim; explainable claim verification; verification; adaptive reasoning; explainable; art; argument
151-FIND,VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy,"Large language models (LLMs) have been widely deployed in Conversational AIs (CAIs), while exposing privacy and security threats. Recent research shows that LLM-based CAIs can be manipulated to extract private information from human users, posing serious security threats. However, the methods proposed in that study rely on a white-box setting that adversaries can directly modify the system prompt. This condition is unlikely to hold in real-world deployments. The limitation raises a critical question: can unprivileged attackers still induce such privacy risks in practical LLM-integrated applications? To address this question, we propose VortexPIA, a novel indirect prompt injection attack that induces privacy extraction in LLM-integrated applications under black-box settings. By injecting token-efficient data containing false memories, VortexPIA misleads LLMs to actively request private information in batches. Unlike prior methods, VortexPIA allows attackers to flexibly define multiple categories of sensitive data. We evaluate VortexPIA on six LLMs, covering both traditional and reasoning LLMs, across four benchmark datasets. The results show that VortexPIA significantly outperforms baselines and achieves state-of-the-art (SOTA) performance. It also demonstrates efficient privacy requests, reduced token consumption, and enhanced robustness against defense mechanisms. We further validate VortexPIA on multiple realistic open-source LLM-integrated applications, demonstrating its practical effectiveness.",Yu Cui; Sicheng Pan; Yifei Liu; Haibin Zhang; Cong Zuo,Yu Cui,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",,privacy; integrated; prompt injection; threats; private; indirect; injection; applications; prompt; security
157-FIND,VisDoT : Enhancing Visual Reasoning through Human-Like Interpretation Grounding and Decomposition of Thought,"Large vision-language models (LVLMs) struggle to reliably detect visual primitives in charts and align them with semantic representations, which severely limits their performance on complex visual reasoning. This lack of perceptual grounding constitutes a major bottleneck for chart-based reasoning. We propose VisDoT, a framework that enhances visual reasoning through human-like interpretation grounding. We formalize four perceptual tasks based on the theory of graphical perception such as position and length. Building on this foundation, we introduce decomposition-of-thought (DoT) prompting, which sequentially separates questions into visual perception sub-questions and logic sub-questions. Fine-tuning InternVL with VisDoT achieves a +11.2\% improvement on ChartQA and surpasses GPTвЂ‘4o on the more challenging ChartQAPro benchmark. On the newly introduced VisDoTQA benchmark, the model improves by +33.2\%. Furthermore, consistent zero-shot gains on diverse open-domain VQA benchmarks confirm the generalizability of the perception-logic separation strategy for visual question answering in general. VisDoT leverages human-like perception to enhance visual grounding, achieving state-of-the-art chart understanding and interpretable visual reasoning.",Eunsoo Lee; Jeongwoo Lee; Minki Hong; Jangho Choi; Jihie Kim,Eunsoo Lee,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Retrieval, Grounding & External Knowledge (RAG)",Multimodal & Speech/Audio,visual; visual reasoning; perception; grounding; human like; sub questions; reasoning human; chart; perceptual; reasoning
158-FIND,$\texttt{KNN-SSD}$: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization,"Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate the inference of large language models (LLMs) without compromising generation quality. It works by efficiently drafting multiple tokens using a compact model and then verifying them in parallel using the target LLM. Notably, Self-Speculative Decoding proposes skipping certain layers to construct the draft model, which eliminates the need for additional parameters or training. Despite its strengths, we observe in this work that drafting with layer skipping exhibits significant sensitivity to domain shifts, leading to a substantial drop in acceleration performance. To enhance the domain generalizability of this paradigm, we introduce $\texttt{KNN-SSD}$, an algorithm that leverages K-Nearest Neighbor (KNN) search to match different skipped layers with various domain inputs. We evaluated our algorithm in various models and multiple tasks, observing that its application leads to $1.3\times$$\sim$$1.6\times$ speedup in LLM inference.",Mingbo Song; Heming Xia; Jun Zhang; Chak Tou Leong; Qiancheng Xu; Wenjie Li; Sujian Li,Mingbo Song,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Summarization & Generation,knn; speculative; speculative decoding; decoding; skipping; nearest neighbor; nearest; drafting; neighbor; texttt
159-FIND,Seeing Between the Verbs: Resolving Ambiguities with Multimodal Sense Clustering,"Evaluating visual activity recognition systems is challenging due to inherent ambiguities in verb semantics and image interpretation. When describing actions in images, synonymous verbs can refer to the same event (e.g., *brushing* vs. *grooming*), while different perspectives can lead to equally valid but distinct verb choices (e.g., *piloting* vs. *operating*). Standard exact-match evaluation, which relies on a single gold answer, fails to capture these ambiguities, resulting in an incomplete assessment of model performance. To address this, we propose a vision-language clustering framework that constructs **verb sense clusters**, providing a more robust evaluation. Our analysis of the imSitu dataset shows that each image maps to an average of 2.8 sense clusters, with each cluster representing a distinct perspective of the image. We evaluate multiple activity recognition models and compare our cluster-based evaluation with standard evaluation methods. Additionally, our human alignment analysis suggests that the cluster-based evaluation better aligns with human judgements, offering a more nuanced assessment of model performance.",Louie Hong Yao; Nicholas Jarvis; Tianyu Jiang,Tianyu Jiang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",ambiguities; verb; cluster; sense; activity recognition; verbs; cluster based; activity; clusters; image
162-FIND,HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition,"Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a model's ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that while most multilingual ASR models initially struggle with CS-ASR, this capability can be enabled through fine-tuning with both natural and synthetic CS data.",Gio Paik; Yongbeom Kim; Soungmin Lee; Sangmin Ahn; Chan Woo Kim,Gio Paik,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",asr; code switching; switching; multilingual asr; asr models; korean; english code switching; hierarchical; multilingual; english code
165-FIND,Complexity-aware fine-tuning,"General-purpose Large Language Models (LLMs) are frequently fine-tuned through supervised fine-tuning (SFT) to enhance performance in specific domains. Better results can be achieved by distilling the chain-of-thought of a larger model at the cost of numerous expensive calls and a much greater amount of data. We propose a novel blueprint for efficient fine-tuning that uses reasoning only for complex data identified by entropy. Specifically, across two small open models ($\approx 3B$) we split the training data into complexity categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large language models (LLMs) via SFT and distillation, and show that our pipeline significantly outperforms the standard SFT approach ($0.58$ vs $0.45$ average accuracy) and outperforms the distillation approach ($0.58$ vs $0.56$ average accuracy) while using $81\%$ less data. We publish our code and data to facilitate further research in this direction.",Andrey Goncharov; Daniil Vyazhev; Petr Sychev; Edvard Khalafyan; Alexey Zaytsev,Andrey Goncharov,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",sft; fine; average accuracy; distillation; entropy; fine tuning; tuning; average; complexity; standard sft
168-FIND,Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Question Answering Task,"Retrieval-augmented generation (RAG) has become a cornerstone of contemporary NLP, enhancing large language models (LLMs) by allowing them to access richer factual contexts through in-context retrieval. While effective in monolingual settings, especially in English, its use in multilingual tasks remains unexplored. This paper investigates the effectiveness of RAG across multiple languages by proposing novel approaches for multilingual open-domain question-answering. We evaluate the performance of various multilingual RAG strategies, including question-translation (tRAG), which translates questions into English before retrieval, and Multilingual RAG (MultiRAG), where retrieval occurs directly across multiple languages. Our findings reveal that tRAG, while useful, suffers from limited coverage. In contrast, MultiRAG improves efficiency by enabling multilingual retrieval but introduces inconsistencies due to cross-lingual variations in the retrieved content. To address these issues, we propose Crosslingual RAG (CrossRAG), a method that translates retrieved documents into a common language (e.g., English) before generating the response. Our experiments show that CrossRAG significantly enhances performance on knowledge-intensive tasks, benefiting both high-resource and low-resource languages",Leonardo Ranaldi,Leonardo Ranaldi,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multilinguality & Low-Resource NLP,"Retrieval, Grounding & External Knowledge (RAG)",rag; multilingual; retrieval; translates; multiple languages; knowledge intensive; english; retrieved; question; languages
175-FIND,SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving,"Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.",Ashutosh Bajpai; Akshat Bhandari; Akshay Nambi; Tanmoy Chakraborty,Ashutosh Bajpai,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Multimodal & Speech/Audio,spatial; visual; symbolic reasoning; reasoning; comprehension; infused; symbolic; structured; perception; representations
176-FIND,Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs,"Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillsetвЂ”a collection of verified scriptsвЂ”by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.",Paiheng Xu; Gang Wu; Xiang Chen; Tong Yu; Chang Xiao; Franck Dernoncourt; Tianyi Zhou; Wei Ai; Viswanathan Swaminathan,Paiheng Xu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,"Dialogue, Conversational & Interactive NLP",software; runtime; api; synergy; execution feedback; code; skill; interfaces; apis; code generation
178-FIND,How Important is вЂ�PerfectвЂ™ English for Machine Translation Prompts?,"Large language models (LLMs) achieve state-of-the-art performance in machine translation, but are also known to be sensitive to errors in user prompts. Given these models are largely trained on and respond best to prompts in standard English, this may affect the quality of LLM outputs for second language English speakers as well as real-world lay users, with potentially disproportionate effects on the former. We explore this effect by modeling a range of error types exhibited by such users, motivated by studies of L2 English, and quantifying their impact on LLM performance. We work with two related tasks: machine translation and machine translation evaluation. We find that LLMs-as-MT are brittle to natural spelling errors but not to errors at the phrasal level. However, the variance in quality caused by these errors is lower than the variance over the initial prompt choice, suggesting that ``perfect English'' for a given prompt is less important than choosing a good prompt. Since lay users and L2 speakers may use non-optimal prompts as well as display imperfect language skills, our work calls for increasing the resilience of model performance to both these phenomena to best serve a diverse user base, both from a robustness and fairness perspective.",PatrГ­cia SchmidtovГЎ; Niyati Bafna; Seth Aycock; Gianluca Vico; Wiktor Kamzela; Kathy HГ¤mmerl; VilГ©m Zouhar,Patricia Schmidtova,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Machine Translation,,machine translation; errors; english; translation; machine; prompts; users; lay; speakers; prompt
183-FIND,$K$ETCHUP: $K$-Step Return Estimation for Sequential Knowledge Distillation,"We propose a novel $K$-step return estimation method (called $K$ETCHUP) for Reinforcement Learning (RL)-based knowledge distillation (KD) in text generation tasks. Our idea is to induce a $K$-step return by using the Bellman Optimality Equation for multiple steps. Theoretical analysis shows that this $K$-step formulation reduces the variance of the gradient estimates, thus leading to improved RL optimization, especially when the student model size is large. Empirical evaluation on three text generation tasks demonstrates that our approach yields superior performance in both standard task metrics and large language model (LLM)-based evaluation. These results suggest that our $K$-step return induction offers a promising direction for enhancing RL-based KD in LLM research.",Jiabin Fan; Guoqing Luo; Michael Bowling; Lili Mou,Jiabin Fan,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Summarization & Generation,"LLM Evaluation, Benchmarks & Metrics",return; step; rl based; text generation tasks; generation tasks; knowledge distillation; text generation; distillation; estimation; estimation method
185-FIND,Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering,"Linear concept vectors effectively steer LLMs, but existing methods suffer from noisy features in diverse datasets that undermine steering robustness. We propose Sparse Autoencoder-Denoised Concept Vectors (SDCV), which selectively keep the most discriminative SAE latents while reconstructing hidden representations. Our key insight is that concept-relevant signals can be explicitly separated from dataset noise by scaling up activations of top-k latents that best differentiate positive and negative samples. Applied to linear probing and difference-in-mean, SDCV consistently improves steering success rates by 4-16% across six challenging concepts, while maintaining topic relevance.",Haiyan Zhao; Xuansheng Wu; Fan Yang; Bo Shen; Ninghao Liu; Mengnan Du,Haiyan Zhao,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,,concept; vectors; steering; latents; linear; sparse; propose sparse; sae latents; methods suffer; positive negative samples
191-FIND,Shifting Perspectives: Steering Vectors for Robust Bias Mitigation in LLMs,"We present a novel approach to bias mitigation in large language models (LLMs) by applying steering vectors to modify model activations in forward passes. We compute 8 steering vectors, each corresponding to a different social bias axis, such as age, gender, or race, on a training subset of the BBQ dataset and compare the effectiveness of these to 3 additional bias mitigation methods across 4 datasets. When optimized on the BBQ dataset, our individually tuned steering vectors achieve average improvements of 12.8% on BBQ, 8.3% on CLEAR-Bias, and 1% on StereoSet, and show improvements over prompting and Self-Debias in all cases, and improvements over fine-tuning in 12 out of 17 evaluations. In addition, steering vectors showed the lowest impact on MMLU scores of the four bias mitigation methods tested. The work presents the first systematic investigation of steering vectors for bias mitigation, and we demonstrate that they are a powerful and computationally efficient strategy for reducing bias in LLMs, with broader implications for enhancing AI safety.",Zara Siddique; Irtaza Khalid; Liam Turner; Luis Espinosa-Anke,Zara Siddique,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,steering vectors; bias mitigation; bias; vectors; steering; mitigation; bbq; mitigation methods; improvements; ai safety
198-FIND,MAPS: A Multilingual Benchmark for Agent Performance and Security,"Agentic AI systems, which build on Large Language Models (LLMs) and interact with tools and memory, have rapidly advanced in capability and scope. Yet, since LLMs have been shown to struggle in multilingual settings, typically resulting in lower performance and reduced safety, agentic systems risk inheriting these limitations. This raises concerns about the accessibility of such systems, as users interacting in languages other than English may encounter unreliable or security-critical agent behavior. Despite growing interest in evaluating agentic AI, existing benchmarks focus exclusively on English, leaving multilingual settings unexplored. To address this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate agentic AI systems across diverse languages and tasks. MAPS builds on four widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code generation), MATH (mathematical reasoning), and the Agent Security Benchmark (security). We translate each dataset into eleven diverse languages, resulting in 805 unique tasks and 9,660 total language-specific instances - enabling a systematic analysis of the multilingual effect on AI agents' performance and robustness. Empirically, we observe a degradation in both performance and security when transitioning from English to other languages, with severity varying by task and correlating with the amount of translated input. Building on these findings, we provide actionable recommendations to guide agentic AI systems development and assessment under multilingual settings. This work establishes the first standardized evaluation framework for multilingual agentic AI, encouraging future research towards equitable, reliable, and accessible agentic AI. The MAPS benchmark suite will become publicly available upon publication.",Omer Hofman; Jonathan Brokman; Oren Rachmil; Shamik Bose; Vikas Pahuja; Toshiya Shimizu; Trisha Starostina; Kelly Marchisio; Seraphina Goldfarb-Tarrant; Roman Vainshtein,Omer Hofman,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Efficiency, Scaling & NLP Systems",agentic ai; agentic; security; multilingual; maps; agentic ai systems; multilingual settings; ai systems; systems; benchmark suite
200-FIND,Linking Knowledge to Care: Knowledge Graph-Augmented Medical Follow-Up Question Generation,"Clinical diagnosis is time-consuming, requiring intensive interactions between patients and medical professionals. While large language models (LLMs) could ease the pre-diagnostic workload, their limited domain knowledge hinders effective medical question generation. We introduce a Knowledge Graph-augmented LLM with active in-context learning to generate relevant and important follow-up questions, KG-Followup, serving as a critical module for the pre-diagnostic assessment. The structured medical domain knowledge graph serves as a seamless patch-up to provide professional domain expertise upon which the LLM can reason. Experiments demonstrate that KG-Followup outperforms state-of-the-art methods by 8%+ on curated ClinicalInquiryBench and 5%+ on FollowupBench in recall.",Liwen Sun; Xiang Yu; Ming Tan; Zhuohao Chen; Anqi Cheng; Ashutosh Joshi; Chenyan Xiong,Liwen Sun,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),Summarization & Generation,medical; knowledge graph; knowledge; question generation; graph augmented; domain knowledge; graph; follow; diagnostic; domain
203-FIND,DebateQA: Evaluating Question Answering on Debatable Knowledge,"The rise of large language models (LLMs) has enabled us to seek answers to inherently debatable questions on LLM chatbots, necessitating a reliable way to evaluate their ability. However, traditional QA benchmarks assume fixed answers are inadequate for this purpose. To address this, we introduce DebateQA, a dataset of 2,941 debatable questions, each accompanied by multiple human-annotated partial answers that capture a variety of perspectives. We develop two metrics: Perspective Diversity, which evaluates the comprehensiveness of perspectives, and Dispute Awareness, which assesses if the LLM acknowledges the question's debatable nature. Experiments demonstrate that both metrics are aligned with human preferences and stable across different underlying models. Using DebateQA with two metrics, we assess 12 prevalent LLMs and retrieval-augmented generation methods. Our findings reveal that while LLMs generally excel at recognizing debatable issues, their ability to provide comprehensive answers encompassing diverse perspectives varies considerably.",Rongwu Xu; Xuan Qi; Zehan Qi; Wei Xu; Zhijiang Guo,Xuan Qi,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,answers; perspectives; metrics; questions; way evaluate; dispute; llms retrieval; llms retrieval augmented; diverse perspectives; encompassing diverse
214-FIND,Personal Information Parroting in Language Models,"Modern language models are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which language models (LMs) memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization and parroting: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e. when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and timesteps of pretraining (70k-143k iterations) on the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.",Nishant Subramani; Kshitish Ghate; Mona T. Diab,Nishant Subramani,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,instances; pretraining; personal information; personal; memorization; suite; addresses; models varying; 70k; positively
220-FIND,Harmful Factuality: LLMs Correcting What They Shouldn't,"Large Language Models (LLMs) often automatically revise facts in provided content to align with their internal knowledge, a behavior that, while aiming for factual accuracy, can detrimentally override source material. This paper systematically investigates and formally defines this critical issue as Harmful Factuality Hallucination, where LLMs unexpectedly correct perceived inaccuracies in the input, prioritizing global factual correctness over essential source fidelity. Moving beyond anecdotal evidence, we introduce a robust framework to induce and quantify Harmful Factuality by applying controlled soft (Gaussian Embedding Perturbation) and hard (LLM-Instructed Entity Replacement) entity perturbations. We evaluate a diverse set of open-source (e.g., Llama series) and commercial (e.g., GPT-4o) LLMs of varying scales across abstractive summarization, rephrasing, and context-grounded question-answering tasks. Our experiments reveal that Harmful Factuality is prevalent, with its incidence significantly influenced by model scale (larger models often exhibit higher rates), perturbation type, entity position within the source, and task characteristics. Furthermore, through analysis of Dual Presence outputs, we identify and categorize three core behavioral mechanisms that underlie this phenomenon. Importantly, we also demonstrate that a simple instructional defense prompt can substantially mitigate Harmful Factuality, reducing it by approximately 50% in several leading models. This research provides a foundational methodology and crucial insights for evaluating and alleviating source-conflicting behaviors, thereby supporting the development of more reliable and source-faithful LLM systems.",Mingchen Li; Hanzhi Zhang; Heng Fan; Junhua Ding; Yunhe Feng,Junhua Ding,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Summarization & Generation,"Retrieval, Grounding & External Knowledge (RAG)",factuality; harmful; source; entity; perturbation; factual; llms; leading models; mitigate harmful; varying scales
228-FIND,Toward Beginner-Friendly LLMs for Language Learning: Controlling Difficulty in Conversation,"Practicing conversations with large language models (LLMs) presents a promising alternative to traditional in-person language learning. However, most LLMs generate text at a near-native level of complexity, making them ill-suited for beginner learners (CEFR: A1вЂ“A2). In this paper, we investigate whether controllable generation techniques can adapt LLM outputs to better support absolute beginners. We evaluate these methods through both automatic metrics and a user study with university-level learners of Japanese. Our findings show that while prompting alone fails, controllable generation techniques can successfully improve output comprehensibility for beginner speakers (from 40.4% to 84.3%). We further introduce a new token-level evaluation metric, Token Miss Rate (TMR), that quantifies the proportion of incomprehensible tokens per utterance and correlates strongly with human judgments. To support future research in AI-assisted language learning, we release our code, models, annotation tools, and dataset.",Meiqing Jin; Liam Dugan; Chris Callison-Burch,Liam Dugan,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Dialogue, Conversational & Interactive NLP","LLM Evaluation, Benchmarks & Metrics",language learning; learners; controllable; level; learning; techniques; support; token; incomprehensible tokens; strongly human
237-FIND,CodeGuard: Improving LLM Guardrails in CS Education,"Large language models (LLMs) are increasingly embedded in Computer Science (CS) classrooms to automate code generation, feedback, and assessment. However, their susceptibility to adversarial or ill-intentioned prompts threatens student learning and academic integrity. To cope with this important issue, we evaluate existing off-the-shelf LLMs in handling unsafe and irrelevant prompts within the domain of CS education. We identify important shortcomings in existing LLM guardrails which motivates us to propose CodeGuard, a comprehensive guardrail framework for educational AI systems. CodeGuard includes (i) a first-of-its-kind taxonomy for classifying prompts; (ii) the CodeGuard dataset, a collection of 8,000 prompts spanning the taxonomy; and (iii) PromptShield, a lightweight sentence-encoder model fine-tuned to detect unsafe prompts in real time. Experiments show that PromptShield achieves 0.93 F1 score, surpassing existing guardrail methods. Additionally, further experimentation reveals that CodeGuard reduces potentially harmful or policy-violating code completions by 30-65% without degrading performance on legitimate educational tasks. The code, datasets, and evaluation scripts are made freely available to the community.",Nishat Raihan; Noah Erdachew; FNU Jayoti Devi; Joanna C. S. Santos; Marcos Zampieri,Nishat Raihan,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Summarization & Generation,prompts; guardrail; guardrails; unsafe; educational; education; taxonomy; code; important; existing
240-FIND,ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs,"In todayвЂ™s rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained вЂњatomicвЂќ facts, improving extraction exhaustivity and stability. From these atomic facts, atomic KGs are derived and then merged in parallel. Empirical evaluations demonstrate that ATOM maintains high exhaustivity, achieves stability across multiple runs, and outperforms baseline methods while ensuring low latency and high scalability for large-scale deployments.",Yassir Lairgi; Ludovic Moncla; Khalid Benabdeslem; RГ©my Cazabet; Pierre ClГ©au,Yassir Lairgi,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Efficiency, Scaling & NLP Systems",,facts; multiple runs; temporal; runs; atomic; temporal knowledge; dynamic; stability; knowledge; unstructured
244-FIND,On the Interplay between Human Label Variation and Model Fairness,"The impact of human label variation (HLV) on model fairness is an unexplored topic. This paper examines the interplay by comparing training on majority-vote labels with a range of HLV methods. Our experiments show that without explicit debiasing, HLV training methods have a positive impact on fairness.",Kemal Kurniawan; Meladel Mistica; Timothy Baldwin; Jey Han Lau,Kemal Kurniawan,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness",,hlv; fairness; human label; label variation; human label variation; interplay; label; variation; impact; majority vote
246-FIND,Where do LLMs currently stand on biomedical NER in both clean and noisy settings ?,"Biomedical Named Entity Recognition (NER) consists of identifying and classifying important biomedical entities mentioned in text. Traditionally, biomedical NER has heavily relied on domain-specific pre-trained language models; particularly variant of BERT models. With the emergence of large language models (LLMs), some studies have evaluated their performance on biomedical NLP tasks. These studies consistently show that, despite their general capabilities, LLMs still fall short compared to specialized BERT-based models for biomedical NER. However, as LLMs continue to advance at a remarkable pace, natural questions arise: Are they still far behind, or are they starting to be competitive? In this study, we investigate the performance of recent LLMs across multiple biomedical NER datasets under both clean and noisy dataset conditions. Our findings reveal that LLMs are progressively closing the performance gap with BERT-based models and demonstrate particular strengths in low-data settings. Moreover, our results suggest that in-context learning with LLMs exhibits a notable degree of robustness to noise, making them a promising alternative in settings where labeled data is scarce or noisy.",Christophe Ye; Cassie S. Mitchell,Christophe Ye,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Information Extraction & Structured Prediction,Domain NLP (Biomedical/Clinical/Legal/Scientific),biomedical; ner; bert; noisy; bert based; llms; clean; based models; settings; studies
249-FIND,Scaling Data-Constrained Language Models with Synthetic Data,"Large language models (LLMs) improve with more training data, but practical limits on data collection increasingly constrain further scaling. Advances in instruction-following LLMs have enabled controlled, high-quality text generation, making synthetic data a promising remedy. However, its effectiveness for pre-training non-English LLMs remains underexplored. We study this question for Japanese in a fixed token budget setting in which organic Japanese Web text constitutes only a small share, while far more organic English Web text and instruction-following LLMs capable of generating fluent Japanese are available. We compare three strategies to fill the data shortfall: generating synthetic Japanese text, repeating the limited Japanese Web text, and using English Web text. Experiments show that synthetic Japanese corpora outperform both baselines and approach the performance achieved when the entire token budget is filled with additional organic Japanese Web text.",Hirokazu Kiyomaru; Yusuke Oda; Takashi Kodama; Chaoran Liu; Daisuke Kawahara,Hirokazu Kiyomaru,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Summarization & Generation,"Efficiency, Scaling & NLP Systems",japanese; web; organic; text; synthetic; following llms; instruction following llms; token budget; budget; english
251-FIND,The Unintended Trade-off of AI Alignment: Balancing Hallucination Mitigation and Safety in LLMs,"Hallucination in large language models (LLMs) has been widely studied in recent years, with progress in both detection and mitigation aimed at improving truthfulness. Yet, a critical side effect remains largely overlooked: enhancing truthfulness can negatively impact safety alignment. In this paper, we investigate this trade-off and show that increasing factual accuracy often comes at the cost of weakened refusal behavior. Our analysis reveals that this arises from overlapping components in the model that simultaneously encode hallucination and refusal information, leading alignment methods to suppress factual knowledge unintentionally. We further examine how fine-tuning on benign datasets, even when curated for safety, can degrade alignment for the same reason. To address this, we propose a method that disentangles refusal-related features from hallucination features using sparse autoencoders, and preserves refusal behavior during fine-tuning through subspace orthogonalization. This approach prevents hallucinations from increasing while maintaining safety alignment.We evaluate our method on commonsense reasoning tasks and harmful benchmarks (AdvBench and StrongReject). Results demonstrate that our approach preserves refusal behavior and task utility, mitigating the trade-off between truthfulness and safety.",Omar Mahmoud; Ali Khalil; Thommen George Karimpanal; Buddhika Laknath Semage; Santu Rana,Omar Mohamed Ahmed Mahmoud,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,refusal; refusal behavior; hallucination; safety; truthfulness; alignment; preserves refusal; trade; behavior; safety alignment
252-FIND,The ModelвЂ™s Language Matters: A Comparative Privacy Analysis of LLMs,"Large Language Models (LLMs) are increasingly deployed across multilingual applications that handle sensitive data, yet their scale and linguistic variability introduce major privacy risks. Mostly evaluated for English, this paper investigates how language structure affects privacy leakage in LLMs trained on English, Spanish, French, and Italian medical corpora. We quantify six linguistic indicators and evaluate three attack vectors: extraction, counterfactual memorization, and membership inference. Results show that privacy vulnerability scales with linguistic redundancy and tokenization granularity: Italian exhibits the strongest leakage, while English shows higher membership separability. In contrast, French and Spanish display greater resilience due to higher morphological complexity. Overall, our findings provide the first quantitative evidence that language matters in privacy leakage, underscoring the need for language-aware privacy-preserving mechanisms in LLM deployments.",Abhishek Kumar Mishra; Antoine Boutet; Lucas Magnana,Abhishek Kumar Mishra,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness",,privacy; leakage; privacy leakage; spanish; membership; matters; italian; french; linguistic; english
253-FIND,Towards the First NLP Benchmark for Ladin - an Extremely Low-Resource Language,"The performance of large language models (LLMs) tends to degrade for extremely low-resource languages, primarily due to the lack of labeled training data. Despite growing interest, the availability of high-quality natural language processing (NLP) datasets for these languages remains limited. This paper addresses such gap by focusing on Ladin, an endangered Romance language, specifically the Val Badia variant. Leveraging a small set of parallel LadinвЂ“Italian sentence pairs, we create synthetic datasets for sentiment analysis and question answering by translating monolingual Italian data. To ensure linguistic quality, we apply rigorous filtering and back-translation procedures in our method. We further demonstrate that incorporating these synthetic datasets into machine translation training leads to substantial improvements over existing ItalianвЂ“Ladin translation baselines. Our contributions include sentiment analysis and question answering datasets for Ladin, establishing foundational resources that support broader NLP research and downstream applications for underrepresented languages.",Ulin Nuha; Adam Jatowt,Ulin Nuha,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Machine Translation,Multilinguality & Low-Resource NLP,extremely low; extremely low resource; sentiment analysis; translation; extremely; synthetic datasets; nlp; italian; sentiment; datasets
255-FIND,DRAGON: Domain-specific Robust Automatic Data Generation for RAG Optimization,"Retrieval-augmented generation (RAG) can substantially enhance the performance of LLMs on knowledge-intensive tasks. Various RAG paradigmsвЂ”including vanilla, planning-based, and iterative RAGвЂ”all depend on a robust retriever, yet existing retrievers rely heavily on public knowledge and often falter when faced with domain-specific queries. To address these limitations, we introduce DRAGON, a framework that combines a data-construction modeling approach with a scalable synthetic data-generation pipeline, specifically designed to optimize domain-specific retrieval performance and bolster retriever robustness. To evaluate RAG performance on domain-specific RAGs, we propose DRAGONBench, a benchmark spanning 8 domain-specific document collections across 4 distinct fields and featuring a wide spectrum of query complexities, answerability, and hops. Leveraging DRAGON, we generate a large-scale synthetic datasetвЂ”encompassing both single-hop and multi-hop queriesвЂ”to enrich retriever training. Extensive experiments demonstrate that retrievers trained on this data yield significant performance gains and exhibit strong cross-domain generalization. Moreover, when our optimized retrievers are integrated into vanilla, planning-based, and iterative RAG paradigms, we observe consistent end-to-end improvements in system accuracy.",Haiyang Shen; Hang Yan; zhongshi Xing; Mugeng Liu; Yue Li; Zhiyang Chen; Yuxiang Wang; Jiuzheng Wang; Yun Ma,Haiyang Shen,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,rag; dragon; domain specific; domain; retrievers; retriever; based iterative; specific; vanilla; data generation
264-FIND,Causal Activation Steering via Sparse Mediation,"Activation steering or editing hidden states to control language-model behavior can be framed as a causal mediation problem: inputs induce internal activations, a subset of which act as mediators transmitting targeted behaviors to outputs. We formalize a structural graph over transformer layers and derive front-doorвЂ”style identification conditions that justify steering through mediating subspaces while preserving non-mediating features, thereby reducing confounding and off-target effects. Within this mediation-first view, we present CAS-BiPO, a sparse mediation steering approach that learns targeted behavioral interventions via regularized training. Empirically, our method achieves 97-100% of dense baseline effectiveness across four behavioral control tasks while using only 10-30% of activation dimensions. Learned masks concentrate 94.3% of steering effects in 26.7% of dimensions, with neurons exhibiting 2.2$\times$ higher activation changes, validating the sparse mediation hypothesis. Our causal framework provides theoretical grounding while CAS-BiPO demonstrates that end-to-end learning of interpretable, reliable interventions is both feasible and advantageous.",Toan Doan; Uyen Le; Thin Nguyen,Toan Doan,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Retrieval, Grounding & External Knowledge (RAG)",mediation; steering; activation; cas; activation steering; causal; sparse; interventions; behavioral; targeted
265-FIND,Causal Direct Preference Optimization for Language Model Alignment,"Direct Preference Optimization (DPO) is a powerful approach for aligning large language models (LLMs) with human preferences by formulating preference learning as a supervised classification problem over pairwise human-labeled outputs, thereby enabling stable and efficient training. We show that DPO inherits bias from confounders (e.g., topic, style, user objectives) that shape data generation and carry through to training, hindering recovery of true human preferences. We address this from a causal perspective, proposing Causal Direct Preference Optimization (CDPO), a general framework that incorporates causal inference principles to mitigate the influence of confounders and sharpen the signal of genuine human preferences. Our approach preserves the tractability of direct optimization while enhancing robustness to spurious correlations and annotation biases. Empirical evaluations on benchmark datasets show that CDPO surpasses DPO-based baselines by achieving unbiased fine-tuning through causal reasoning, confirming the effectiveness of confounder-aware preference optimization.",Uyen Le; Thin Nguyen; Toan Nguyen; Toan Doan; Trung Le; Bac Le,Thuc Uyen Le,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Summarization & Generation,causal; preference optimization; preference; optimization; direct; direct preference; direct preference optimization; dpo; human preferences; preferences
267-FIND,LLMs Faithfully and Iteratively Compute Answers During CoT: A Systematic Analysis With Multi-hop Arithmetics,"This study investigates the internal information flow of large language models (LLMs) while performing chain-of-thought (CoT) style reasoning. Specifically, with a particular interest in the faithfulness of the CoT explanation to LLMs' final answer, we explore (i) when the LLMs' answer is (pre)determined, especially before the CoT begins or after, and (ii) how strongly the information from CoT specifically has a causal effect on the final answer. Our experiments with controlled arithmetic tasks reveal a systematic internal reasoning mechanism of LLMs. They have not derived an answer at the moment when input was fed into the model. Instead, they compute (sub-)answers while generating the reasoning chain on the fly. Therefore, the generated reasoning chains can be regarded as faithful reflections of the model's internal computation.",Keito Kudo; Yoichi Aoki; Tatsuki Kuribayashi; Shusaku Sone; Masaya Taniguchi; Ana Brassard; Keisuke Sakaguchi; Kentaro Inui,Keito Kudo,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents",Interpretability & Model Analysis,cot; answer; internal; final answer; compute; reasoning; final; llms; chain; answers
268-FIND,VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery,"Cultural heritage artifacts preserve invaluable records of human history, yet their computational analysis requires expert-level reasoning that combines fine-grained visual perception with domain-specific knowledge. General-purpose Multimodal Large Language Models (MLLMs) achieve strong results on common benchmarks but fall short on specialized tasks such as authentication, provenance, and historical attribution. We introduce VaseVL, a reinforcement learningвЂ“enhanced framework tailored to cultural heritage analysis, and release VaseVQA, a comprehensive benchmark containing 31,773 images and 67,614 questionвЂ“answer pairs with clearly defined training and test splits across seven question types. Our method employs Group Relative Policy Optimization (GRPO) with taxonomy-aware reward shaping to align optimization with task-specific metrics, combining keyword overlap and semantic similarity with type-conditioned weights to target challenging categories. Experiments demonstrate that VaseVL outperforms both zero-shot MLLMs and baseline fine-tuned models, achieving the best performance on style classification and historical attribution while significantly improving compositional robustness. Our code and dataset will be released.",Jinchao Ge; Tengfei Cheng; Biao Wu; Zeyu Zhang; SHIYA HUANG; Judith Bishop; Gillian Shepherd; Meng Fang; Ling Chen; Yang Zhao,Jinchao Ge,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Reasoning, Planning & Agents",cultural heritage; heritage; historical; attribution; mllms; cultural; optimization; multimodal; knowledge general; compositional robustness
272-FIND,PromptPrism: A Linguistically-Inspired Taxonomy for Prompts,"Prompts are the interface for eliciting the capabilities of large language models (LLMs). Understanding their structure and components is critical for analyzing LLM behavior and optimizing performance. However, the field lacks a comprehensive framework for systematic prompt analysis and understanding. We introduce PromptPrism, a linguistically-inspired taxonomy that enables prompt analysis across three hierarchical levels: functional structure, semantic component, and syntactic pattern. By applying linguistic concepts to prompt analysis, PromptPrism bridges traditional language understanding and modern LLM research, offering insights that purely empirical approaches might miss. We show the practical utility of PromptPrism by applying it to three applications: (1) a taxonomy-guided prompt refinement approach that automatically improves prompt quality and enhances model performance across a range of tasks; (2) a multi-dimensional dataset profiling method that extracts and aggregates structural, semantic, and syntactic characteristics from prompt datasets, enabling comprehensive analysis of prompt distributions and patterns; (3) a controlled experimental framework for prompt sensitivity analysis by quantifying the impact of semantic reordering and delimiter modifications on LLM performance. Our experimental results validate the effectiveness of our taxonomy across these applications, demonstrating that PromptPrism provides a foundation for refining, profiling, and analyzing prompts.",Sullam Jeoung; Yueyan Chen; Yi Zhang; Shuai Wang; Haibo Ding; Lin Lee Cheong,Sullam Jeoung,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Linguistics, Syntax & Semantics",prompt; taxonomy; analysis; profiling; linguistically; prompts; syntactic; inspired; semantic; analyzing
279-FIND,HiGraAgent: Dual-Agent Adaptive Reasoning over Hierarchical Knowledge Graph for Open Domain Multi-hop Question Answering,"Open Domain Multi-hop Question Answering faces a dual compositionality challenge: reasoning over complex query structures and integrating evidence scattered across contexts. Despite recent advancements in Graph-based Retrieval-Augmented Generation (GraphRAG), persistent limitations in complex reasoning and retrieval inaccuracies continue to constrain the efficacy of multi-hop QA systems. We introduce HiGraAgent, a framework that unifies graph-based retrieval with adaptive reasoning. It constructs a Hierarchical Knowledge Graph (HiGra) with entity alignment, reducing redundancy by 34.5% while preserving expressiveness; employs HiGraRetriever, a hybrid graph-semantic retriever that consistently outperforms the strongest graph-based method across benchmarks; and integrates a dual-agent adaptive reasoning protocol where a Seeker and a Librarian dynamically coordinate retrieval and reasoning. Together, these innovations enable HiGraAgent to achieve 85.3% average accuracy on HotpotQA, 2WikiMultihopQA, and MuSiQue, surpassing the strongest prior system by 11.7%. Our results highlight the importance of reframing multi-hop QA as a problem of adaptive reasoning, offering a more robust and flexible paradigm for complex information seeking.",Hung Luu; Long Nguyen; Trung Pham; Hieu Pham; Tho Quan,Tho Quan,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)","Reasoning, Planning & Agents",adaptive reasoning; graph; multi hop; hop; adaptive; reasoning; graph based; dual; graph based retrieval; domain multi
280-FIND,Hire Your Anthropologist! Rethinking Culture Benchmarks Through an Anthropological Lens,"Cultural evaluation of large language models has become increasingly important, yet current benchmarks often reduce culture to static facts or homogeneous values. This view conflicts with anthropological accounts that emphasize culture as dynamic, historically situated, and enacted in practice. To analyze this gap, we introduce a four-part framework that categorizes how benchmarks frame culture, such as knowledge, preference, performance, or bias. Using this lens, we qualitatively examine 20 cultural benchmarks and identify six recurring methodological issues, including treating countries as cultures, overlooking within-culture diversity, and relying on oversimplified survey formats. Drawing on established anthropological methods, we propose concrete improvements: incorporating real-world narratives and scenarios, involving cultural communities in design and validation, and evaluating models in context rather than isolation. Our aim is to guide the development of cultural benchmarks that go beyond static recall tasks and more accurately capture the responses of the models to complex cultural situations.",Mai Alkhamissi; Yunze Xiao; Badr AlKhamissi; Mona T. Diab,Mai Alkhamissi,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",,culture; cultural; benchmarks; lens; static; benchmarks static; scenarios involving; introduce framework; gap introduce framework; static recall
286-FIND,Suppressing Final Layer Hidden State Jumps in Transformer Pretraining,"This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for jump strength around the final layer, and then demonstrate its prevalence across many public pre-trained models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose a regularizer that penalizes the jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with our jump-suppressing regularizer (JREG), reveal improved task performance compared to the baseline without altering the model architecture.",Keigo Shibata; Kazuki Yano; Ryosuke Takahashi; Jaesung Lee; Wataru Ikeda; Jun Suzuki,Keigo Shibata,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",,transformer; pre; angular; suppressing; final layer; final; layer; pre trained models; regularizer; trained models
288-FIND,Intention-Adaptive LLM Fine-Tuning for Text Revision Generation,"Large Language Models (LLMs) have achieved impressive capabilities in various context-based text generation tasks, such as summarization and reasoning; however, their applications in intention-based generation tasks remain underexplored. One such example is revision generation, which requires the generated text to explicitly reflect the writer's actual intentions. Identifying intentions and generating desirable revisions are challenging due to their complex and diverse nature. Although prior work has employed LLMs to generate revisions with few-shot learning, they struggle with handling entangled multi-intent scenarios. While fine-tuning LLMs using intention-based instructions appears promising, it demands large amounts of annotated data, which is expensive and scarce in the revision community. To address these challenges, we propose Intention-Tuning, an intention-adaptive layer-wise LLM fine-tuning framework that dynamically selects a subset of LLM layers to learn the intentions and subsequently transfers their representations to revision generation. Experimental results suggest that Intention-Tuning is effective and efficient on small revision corpora, outperforming several PEFT baselines.",Zhexiong Liu; Diane Litman,Zhexiong Liu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,,intention; revision; intentions; tuning; revisions; generation; llm fine tuning; llm fine; generation tasks; fine tuning
291-FIND,ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting,"The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.",Yuxing Tian; Fengran Mo; Weixu Zhang; Yiyan Qi; Jian-Yun Nie,Yuxing Tian,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,Summarization & Generation,attention; based ranking; ranking; ranking methods; attention based; weighting; concentrated; documents; tokens; based
293-FIND,MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration,"Agentic AI has significantly extended the capabilities of large language models (LLMs) by enabling complex reasoning and tool use. However, most existing frameworks are tailored to domains such as mathematics, coding, or web automation, and fall short on geospatial tasks that require spatial reasoning, multi-hop planning, and real-time map interaction. To address these challenges, we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with customized toolsets and agentic scaffolds for map-integrated geospatial reasoning. Unlike existing flat agent-based approaches that treat tools uniformlyвЂ”often overwhelming the LLM when handling similar but subtly different geospatial APIsвЂ”MapAgent decouples planning from execution. A high-level planner decomposes complex queries into subgoals, which are routed to specialized modules. For tool-heavy modulesвЂ”such as map-based servicesвЂ”we then design a dedicated map-tool agent that efficiently orchestrates related APIs adaptively in parallel to effectively fetch geospatial data relevant for the query, while simpler modules (e.g., solution generation or answer extraction) operate without additional agent overhead. This hierarchical design reduces cognitive load, improves tool selection accuracy, and enables precise coordination across similar APIs. We evaluate MapAgent on four diverse geospatial benchmarksвЂ”MapEval-Textual, MapEval-API, MapEval-Visual, and MapQAвЂ”and demonstrate substantial gains over state-of-the-art tool-augmented and agentic baselines.",Md Hasebul Hasan; Mahir Labib Dihan; Tanzima Hashem; Mohammed Eunus Ali; Md Rizwan Parvez,Md Rizwan Parvez,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents",,geospatial; map; tool; agent; agentic; geospatial reasoning; hierarchical; apis; modules; planning
294-FIND,Comprehensive Study of Bilingual and Multi-category Instruction Pre-training,"Instruction pre-training (IPT) has recently emerged as an effective intermediate stage between vanilla pre-training and post-training for large language models (LLMs). However, the optimal design of IPT corpora---such as the balance between raw and instruction-response data, languages, and task categories---remains unclear. We systematically study IPT corpus composition using a bilingual (English and Japanese) and multi-category (coding, general, math, and reasoning) instruction-response dataset. Through extensive IPT experiments across four base models, including both English-centric and bilingual LLMs, we find that: (1) more instruction-response data generally enhances model performance, particularly for models with large VPT budgets; (2) Japanese instruction data can improve English performance through cross-lingual transfer; and (3) the effectiveness of post-training varies across categories: coding performance is largely determined during IPT, while math and reasoning continue to improve during post-training.",Takashi Kodama; Yusuke Oda,Takashi Kodama,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multilinguality & Low-Resource NLP,"Reasoning, Planning & Agents",instruction; instruction response; post training; bilingual; training; pre training; post; response; math reasoning; pre
297-FIND,"Reflect, Rewrite, Repeat: How Simple Arithmetic Enables Advanced Reasoning in Small Language Models","Contemporary advancements in language model reasoning typically require computationally intensive reinforcement learning (RL) and massive datasets, creating barriers for resource-constrained teams. In this work, we demonstrate that high-quality, iterative training on minimal data can rival modern RL approaches. We introduce a resource-efficient framework that combines Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT) with selective guidance from larger models, iteratively refining solutions through a ""reflect, rewrite, repeat"" cycle (R). Using Qwen-2.5-7B and Qwen 2.5-Math-7B as base models, our method shows meaningful performance improvements across arithmetic, symbolic and cognitive reasoning benchmarksвЂ”including GSM8K (83.1% в†’ 88.6%), AIME'25@10 (20.0% в†’ 30.0%) and LastLetterConcat (40.7% в†’ 53.3%) problems. The model-agnostic nature of our R framework is further demonstrated through substantial improvements when applied to Mistral and LLaMA-based models. Remarkably, these gains are achieved using mere 700 basic arithmetic training samples, in stark contrast to the hundreds of thousands of examples typically required by RL-based systems. Our results suggest that reasoning improvements need not strictly depend on large-scale data. By emphasizing strategically curated training grounded in foundational principles, we achieve competitive generalization with minimal resource overhead. Our R pipeline also generates high-quality SFT data with high-fidelity reasoning traces as byproduct, further enabling scalable and annotation-free fine-tuning.",Mengdie Flora Wang; Haochen Xie; Mun Young Kim; Baishali Chaudhury; Meghana Ashok; Suren Gunturu; Sungmin Hong; Jae Oh Woo,Mengdie Flora Wang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",arithmetic; repeat; rewrite; reasoning; improvements; sft; qwen; resource; reflect; minimal
298-FIND,Don’t Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation,"With the growing use of large language models (LLMs) as evaluators, their application has expanded to code evaluation tasks, where they assess the correctness of generated code without relying on reference implementations. While this offers scalability and flexibility, it also raises a critical, unresolved question: Can LLM judges fairly and robustly evaluate semantically equivalent code with superficial variations? Functionally correct code often exhibits variationsвЂ”such as differences in variable names, comments, or formattingвЂ”that should not influence its correctness. Yet, whether LLM judges can reliably handle these variations remains unclear. We present the first comprehensive study of this issue, defining six types of potential bias in code evaluation and revealing their systematic impact on LLM judges. Across five programming languages and multiple LLMs, we empirically demonstrate that all tested LLM judges are susceptible to both positive and negative biases, resulting in inflated or unfairly low scores. Moreover, we observe that LLM judges remain vulnerable to these biases even when prompted to generate test cases before scoring, highlighting the need for more robust code evaluation method",Jiwon Moon; Yerin Hwang; Dongryeol Lee; taegwan kang; Yongil Kim; Kyomin Jung,Jiwon Moon,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",llm judges; judges; code evaluation; code; biases; llm; variations; evaluation; correctness; raises critical
302-FIND,COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations,"Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting from expert-driven verification to a community-based setup, where users contribute explanatory notes to clarify why a post might be misleading. An important challenge here is determining whether an explanation is helpful for understanding real-world claims and the reasons why, which remains largely underexplored in prior research. In practice, most community notes remain unpublished due to slow community annotation, and the reasons for helpfulness lack clear definitions. To bridge these gaps, we introduce the task of predicting both the helpfulness of explanatory notes and the reason for this. We present COMMUNITYNOTES, a large-scale multilingual dataset of 105k posts with user-provided notes and helpfulness labels. We further propose a framework that automatically generates and improves reason definitions via automatic prompt optimization, and integrate them into prediction. Our experiments show that the optimized definitions can improve both helpfulness and reason prediction. Finally, we show that the helpfulness information are beneficial for existing fact-checking systems.",Rui Xing; Preslav Nakov; Timothy Baldwin; Jey Han Lau,Rui Xing,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Efficiency, Scaling & NLP Systems",Multilinguality & Low-Resource NLP,helpfulness; notes; definitions; fact checking; checking; community; reason; fact; reasons; explanatory
306-FIND,Deterministic Personality Editing of Large Language Models Using Adversarial Conversational History,"Large Language Models (LLMs) are increasingly deployed in domains such as education, mental health and customer support, where stable and consistent personas are critical for reliability. Yet, existing studies focus on narrative or role-playing tasks and overlook how adversarial conversational history alone can reshape induced personas. Black-box persona manipulation remains unexplored, raising concerns for robustness in realistic interactions. In response, we introduce the task of $\textit{persona editing}$, which adversarially steers LLM traits through user-side inputs under a black-box, inference-only setting. To this end, we propose $\texttt{PHISH}$ (Persona Hijacking via Implicit Steering in History), the first framework to expose a new vulnerability in LLM safety that embeds semantically loaded cues into user queries to gradually induce reverse personas. We also define a metric to quantify attack success. Across 3 benchmarks and 8 LLMs, $\texttt{PHISH}$ predictably shifts personas, triggers collateral changes in correlated traits, and exhibits stronger effects in multi-turn settings. In high-risk domains mental health, tutoring, and customer support, $\texttt{PHISH}$ reliably manipulates personas, validated by both human and LLM-as-Judge evaluations. Importantly, $\texttt{PHISH}$ causes only a small reduction in reasoning benchmark performance, leaving overall utility largely intact while still enabling significant persona manipulation. While current guardrails offer partial protection, they remain brittle under sustained attack. Our findings expose new vulnerabilities in personas and highlight the need for context-resilient persona in LLMs. We will share the codebase and dataset publicly.",Jivnesh Sandhan; Fei Cheng; Tushar Sandhan; Yugo Murawaki,Jivnesh Sandhan,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",personas; persona; texttt; history; customer support; traits; manipulation; mental health; customer; editing
308-FIND,ParsTranslit: Truly Versatile Tajik-Farsi Transliteration,"As a digraphic language, the Persian language utilizes two written standards: Perso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite the significant similarity between the dialects of each country, script differences prevent simple one-to-one mapping, hindering written communication and interaction between Tajikistan and its Persian-speaking ``siblings''. To overcome this, previously-published efforts have investigated machine transliteration models to convert between the two scripts. Unfortunately, most efforts did not use datasets other than those they created, limiting these models to certain domains of text such as archaic poetry or word lists. A truly usable transliteration system must be capable of handling varied domains, meaning that suck models lack the versatility required for real-world usage. The contrast in domain between data also obscures the task's true difficulty. We present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi transliteration trained across all available datasets, and present two datasets of our own. Our results across domains provide clearer understanding of the task, and set comprehensive comparable leading benchmarks. Overall, our model achieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik and 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available at https://anonymous.4open.science/r/ParsTranslit-FB30/.",Rayyan Merchant; Kevin Tang,Rayyan Merchant,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Information Extraction & Structured Prediction,,farsi; transliteration; persian; truly; efforts; written; domains; sequence; datasets; available
309-FIND,"One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations","Sentence embedding methods have made remarkable progress, yet they still struggle to capture the implicit semantics within sentences. This can be attributed to the inherent limitations of conventional sentence embedding methods that assign only a single vector per sentence. To overcome this limitation, we propose DualCSE, a sentence embedding method that assigns two embeddings to each sentence: one representing the explicit semantics and the other representing the implicit semantics. These embeddings coexist in the shared space, enabling the selection of the desired semantics for specific purposes such as information retrieval and text classification. Experimental results demonstrate that DualCSE can effectively encode both explicit and implicit meanings and improve the performance of the downstream task.",Kohei Oda; Po-Min Chuang; Kiyoaki Shirai; Natthawut Kertkeidkachorn,Kohei Oda,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Linguistics, Syntax & Semantics","Retrieval, Grounding & External Knowledge (RAG)",sentence; implicit; sentence embedding; semantics; explicit implicit; embedding methods; sentence embedding methods; explicit; embedding; embeddings
312-FIND,MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration,"The paradigm of augmenting Large Language Models (LLMs) with external tools is rapidly converging on the Model-Context Protocol (MCP), a federated architecture of tool ""servers'' that mirrors the modern internet. Existing benchmarks, however, remain misaligned with this architecture, typically evaluating agents with APIs in an unstructured, non-hierarchical manner. They often test components like retrieval and reasoning in isolation or sidestep the critical real-world challenge of functional overlap among tools. This leads to an incomplete and overly optimistic assessment of agent capabilities. To address these gaps, we introduceMSC-Bench, a large-scale benchmark for evaluating end-to-end, multi-hop tool use within an explicit, hierarchical MCP ecosystem. A core contribution is our methodology for constructing ground truth by identifying ""equal function sets''---groups of functionally equivalent tools. This approach enables the use of objective and reproducible metrics (e.g., F1-score) and mitigates the reliance on fallible LLM-as-a-judge evaluations. MSC-Bench is structured as a five-level curriculum designed to systematically test a full spectrum of agent capabilities, from single-tool execution to complex cross-server orchestration and robustness to out-of-scope requests. Our experiments reveal that rigid hierarchies can hinder performance without co-designed strategies and that even state-of-the-art agents exhibit systemic weaknesses in robustness. MSC-Bench provides a much-needed diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using systems.",Jia-Kai Dong; I-Wei Huang; Chun-Tin Wu; YI-TIEN TSAI,Jia-Kai Dong,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",,tool; bench; orchestration; tools; hierarchical; architecture; end; agent; test; agents
317-FIND,SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation,"Large Language Models (LLMs) often struggle with complex mathematical reasoning, where prose-based generation leads to unverified and arithmetically unsound solutions. Current prompting strategies like Chain of Thought still operate within this unreliable medium, lacking a mechanism for deterministic verification. To address these limitations, we introduce SymCode, a neurosymbolic framework that reframes mathematical problem-solving as a task of verifiable code generation using the SymPy library. We evaluate SymCode on challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating significant accuracy improvements of up to 13.6 percentage points over baselines. Our analysis shows that SymCode is not only more token-efficient but also fundamentally shifts model failures from opaque logical fallacies towards transparent, programmatic errors. By grounding LLM reasoning in a deterministic symbolic engine, SymCode represents a key step towards more accurate and trustworthy AI in formal domains.",Sina Bagheri Nezhad; Yao Li; Ameeta Agrawal,Sina Bagheri Nezhad,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Summarization & Generation,mathematical; deterministic; code generation; verifiable; mathematical reasoning; generation; reasoning; math 500; model failures; strategies like
319-FIND,Unsupervised Detection of LLM-Generated Text in Korean Using Syntactic and Semantic Cues,"As Large Language Models (LLMs) are increasingly used for content creation, detecting AI-generated text has become a critical challenge. Prior work has largely focused on English, leaving low-resource languages such as Korean underexplored. We propose an unsupervised detection framework that integrates two complementary signals: syntactic token cohesiveness (TOCSIN) and semantic regeneration similarity (SimLLM). To support evaluation, we construct a Korean pairwise dataset of 1,000 anchors with continuation- and regeneration-style generations and further assess performance across domains (news, research paper abstracts, essays) and model families (GPT-3.5, GPT-4o, HyperCLOVA X, LLaMA-3-8B). Without any training, our ensemble achieves up to 0.963 F1 and 0.985 ROC-AUC, outperforming baselines. These results demonstrate that the combination of syntactic and semantic cues enables robust unsupervised detection in low-resource settings. Code available at https://anonymous.4open.science/r/llm-detection-main-687D/.",Heejeong Jeon; MinSu Park; YunSeok Choi; Eunil Park,Heejeong Jeon,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multilinguality & Low-Resource NLP,"LLM Evaluation, Benchmarks & Metrics",unsupervised; korean; syntactic; detection; regeneration; semantic cues; syntactic semantic; generated text; semantic; cues
334-FIND,NLP Privacy Risk Identification in Social Media (NLP-PRISM): A Survey,"Natural Language Processing (NLP) is integral to social media analytics but often processes content containing Personally Identifiable Information (PII), behavioral cues, and metadata raising privacy risks such as surveillance, profiling, and targeted advertising. To systematically assess these risks, we review $203$ peer-reviewed papers and propose the~\textit{NLP Privacy Risk Identification in Social Media (NLP-PRISM)} framework, which evaluates vulnerabilities across six dimensions: data collection, preprocessing, visibility, fairness, computational risk, and regulatory compliance. Our analysis shows that transformer models achieve F1-scores ranging from $0.58$вЂ“$0.84$, but incur a $1\% - 23\%$ drop under privacy-preserving fine-tuning. Using NLP-PRISM, we examine privacy coverage in six NLP tasks: sentiment analysis ($16$), emotion detection ($14$), offensive language identification ($19$), code-mixed processing ($39$), native language identification ($29$), and dialect detection ($24$) revealing substantial gaps in privacy research. We further found a ($\downarrow 2\%-9\%$) trade-off in model utility, MIA AUC (membership inference attacks) $0.81$, AIA accuracy $0.75$ (attribute inference attacks). Finally, we advocate for stronger anonymization, privacy-aware learning, and fairness-driven training to enable ethical NLP in social media contexts.",Dhiman Goswami; Jai Kruthunz Naveen Kumar; Sanchari Das,Sanchari Das,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness","Efficiency, Scaling & NLP Systems",privacy; nlp; social media; prism; media; identification; social; privacy risk; risk; language identification
336-FIND,CROWDSELECT: SyntheticInstruction Data Selection with Multi-LLM Wisdom,"Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely mainly on single-dimensional signals (i.e., reward scores, model perplexity), they fail to capture the complexity of instruction-following across diverse fields. Therefore, we investigate more diverse signals to capture comprehensive instruction-response pair characteristics and propose three foundational metrics that leverage Multi-LLMs wisdom, informed by (1) diverse LLM responses and (2) reward model assessment. Building upon base metrics, we propose CROWDSELECT, an integrated metric incorporating a clustering-based approach to maintain response diversity. Our comprehensive experiments demonstrate that our foundation metrics consistently improve performance across 4 base models on MT-bench and Arena-Hard. CROWDSELECT, efficiently incorporating all metrics, achieves state-of-the-art performance in both Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and 11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring valuable insights for future research in this direction.",Yisen Li; Lingfeng Yang; Wenxuan Shen; Pan Zhou; Yao Wan; Weiwei Lin; Dongping Chen,Dongping Chen,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Machine Translation,instruction; metrics; arena; mt bench; data selection; hard; bench; instruction following; incorporating; reward
339-FIND,"Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations","Recent multimodal large language models (MLLMs) extend language understanding beyond text to speech, enabling unified reasoning across modalities. While biases in text-based LLMs have been widely examined, their persistence and manifestation in spoken inputs remain underexplored. This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar Dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and FleissвЂ™ $\kappa$), we evaluate nine representative models under linguistic language and accent, demographic gender, and structural option order perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation.",Sheng-Lun Wei; Yu-Ling Liao; Yen-Hua Chang; Hen-Hsen Huang; Hsin-Hsi Chen,"YU-LING, LIAO",Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Multimodal & Speech/Audio,"Trustworthy, Safety, Privacy & Fairness",speech; demographic; mllms; option; accent; text speech; gender; order; unified; assessing
343-FIND,Pushing the Frontiers of Scientific Fact-Checking: The SCINLP Dataset,"Large Language Models (LLMs) are increasingly being used to understand how scientific research evolves, drawing growing interest from the research community. However, limited work has explored the scientific fact-checking of research questions and claims from manuscripts, particularly within the NLP domain, an emerging direction for advancing scientific integrity and knowledge validation. In this work, we propose a novel scientific fact-checking dataset, SCINLP, tailored to the NLP domain. Our proposed framework on SCINLP systematically verifies the veracity of complex scientific research questions across varying rationale contexts, while also assessing their temporal positioning. SCINLP includes supporting and refuting research questions from a curated collection of influential and reputed NLP papers published between 2000 and 2024. In our framework, we use multiple LLMs and diverse rationale contexts from our dataset to examine scientific claims and research focus, complemented by feasibility judgments for deeper insight into scientific reasoning in NLP.",Iffat Maab; Junichi Yamagishi,Iffat Maab,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Reasoning, Planning & Agents",scientific; research; fact checking; checking; nlp; fact; scientific research; rationale; questions; claims
344-FIND,SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation,"With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs yields better RAG performance, but processing rich documents remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (SemantiC Document Layout ANalysis), a novel approach that enhances both textual and visual Retrieval-Augmented Generation (RAG) systems that work with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering contiguous components. We trained the SCAN model by fine-tuning object detection models on an annotated dataset. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.4 points and visual RAG performance by up to 10.4 points, outperforming conventional approaches and even commercial document processing solutions.",Nobuhiro Ueda; Yuyang Dong; KrisztiГЎn Boros; Daiki Ito; Takuya Sera; Masafumi Oyamada,Nobuhiro Ueda,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)",Multimodal & Speech/Audio,scan; rag; document; rag performance; visual; rich documents; rich; documents; semantic; retrieval augmented generation
347-FIND,Unified Multimodal Interleaved Document Representation for Retrieval,"Information Retrieval (IR) methods aim to identify documents relevant to a query, which have been widely applied in various natural language tasks. However, existing approaches typically consider only the textual content within documents, overlooking the fact that documents can contain multiple modalities, including images and tables. Also, they often segment each long document into multiple discrete passages for embedding, which prevents them from capturing the overall document context and interactions between paragraphs. To address these two challenges, we propose a method that holistically embeds documents interleaved with multiple modalities by leveraging the capability of recent vision-language models that enable the processing and integration of text, images, and tables into a unified format and representation. Moreover, to mitigate the information loss from segmenting documents into passages, instead of representing and retrieving passages individually, we further merge the representations of segmented passages into one single document representation, while we additionally introduce a reranking strategy to decouple and identify the relevant passage within the document if necessary. Then, through extensive experiments on diverse IR scenarios considering both the textual and multimodal queries, we show that our approach substantially outperforms relevant baselines, thanks to the consideration of the multimodal information within documents.",Jaewoo Lee; Joonho Ko; Jinheon Baek; Soyeong Jeong; Sung Ju Hwang,Jaewoo Lee,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",Multimodal & Speech/Audio,documents; passages; document; multiple modalities; document representation; interleaved; representation; relevant; tables; multimodal
348-FIND,TELLME: Test-Enhanced Learning for Language Model Enrichment,"Continual pre-training (CPT) has been widely adopted as a method for domain expansion in large language models. However, CPT has consistently been accompanied by challenges, such as the difficulty of acquiring large-scale domain-specific datasets and high computational costs. In this study, we propose a novel method called Test-Enhanced Learning for Language Model Enrichment (TELLME) to alleviate these issues. TELLME leverages the Test-Enhanced Learning (TEL) principle, whereby the modelвЂ™s learning efficiency is improved using quizzes during training. It integrates this principle with CPT, thereby promoting efficient domain-specific knowledge acquisition and long-term memory retention. Experimental results demonstrate that TELLME outperforms existing methods by up to 23.6% in the financial domain and achieves a 9.8% improvement in long-term memory retention.",Minjun Kim; Inho Won; HyeonSeok Lim; MinKyu Kim; Junghun Yuk; Wooyoung Go; Jongyoul Park; Jungyeul Park; KyungTae Lim,Minjun Kim,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Efficiency, Scaling & NLP Systems",,enhanced; enrichment; long term memory; retention; term memory; principle; domain; learning; long term; test
354-FIND,Beyond Accuracy: Alignment and Error Detection across Languages in the Bi-GSM8K Math-Teaching Benchmark,"Recent advancements in LLMs have significantly improved mathematical problem-solving, with models like GPT-4 achieving human-level performance. However, proficiently solving mathematical problems differs fundamentally from effectively teaching mathematics. To bridge this gap, we introduce the Bi-GSM8K benchmark, a bilingual English-Korean dataset enriched with teacher solutions, student solutions, and annotations marking students' initial errors. This dataset is designed to evaluate two core capabilities of LLMs: (1) measuring similarity between student and teacher solutions, and (2) identifying the initial error point in student solutions. Our method achieves high agreement with human judgments, with Pearson 0.89 and Spearman 0.88 on English, and Pearson 0.89 and Spearman 0.87 on Korean. It also offers significantly lower latency and resource usage than commercial APIs, demonstrating strong computational efficiency. In the error detection task, open-source models achieved approximately 86% accuracy, with performance within 10% points of commercial LLMs API, suggesting strong practical potential. Our key contributions include the open-source release of Bi-GSM8K, novel evaluation metrics, and comparative analyses of LLM performance across languages.",Jieun Park; KyungTae Lim; JOON-HO LIM,Jieun Park,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",solutions; gsm8k; student; spearman; error detection; error; teaching; teacher; korean; commercial
355-FIND,VN-MTEB: Vietnamese Massive Text Embedding Benchmark,"Vietnam ranks among the top countries in terms of both internet traffic and online toxicity. As a result, implementing embedding models for recommendation and content control duties in applications is crucial. However, a lack of large-scale test datasets, both in volume and task diversity, makes it tricky for scientists to effectively evaluate AI models before deploying them in real-world, large-scale projects. To solve this important problem, we introduce a Vietnamese benchmark, VN-MTEB for embedding models, which we created by translating a large number of English samples from the Massive Text Embedding Benchmark using our new automated framework, thereby contributing an extension of the Massive Multilingual Text Embedding Benchmark with our additional Vietnamese tasks and datasets. We leverage the strengths of large language models (LLMs) and cutting-edge embedding models to conduct translation and filtering processes to retain high-quality samples, guaranteeing a natural flow of language and semantic fidelity while preserving named entity recognition (NER) and code snippets. Our comprehensive benchmark consists of 41 datasets from six tasks specifically designed for Vietnamese text embeddings. In our analysis, we find that bigger and more complex models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks.",Loc Pham; Tung Luu; Thu Vo; Minh Nguyen; Viet Hoang,Bao Loc Pham,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Information Extraction & Structured Prediction,Machine Translation,embedding; vietnamese; text embedding benchmark; embedding benchmark; text embedding; massive; embedding models; massive text; massive text embedding; mteb
364-FIND,"See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval","Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks. The code will be made publicly available.",Mingyu Jeon; Sungjin Han; Jinkwon Hwang; Minchol Kwon; Jonghee Kim; Junyeong Kim,Mingyu Jeon,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,"Efficiency, Scaling & NLP Systems",video; memory; moment; store; frame; captions; resolution; query; risking; relevant segments
366-FIND,RB-LoRA: Rank-Balanced Aggregation for Low-Rank Adaptation with Federated Fine-Tuning,"Federated fineвЂ‘tuning of foundation models is impeded by the need to communicate billions of parameters. LowвЂ‘rank adaptation (LoRA) alleviates this by updating only compact adapter matrices. However, varying client device capabilities lead to different adapter ranks, causing rank heterogeneity that undermines aggregation, and existing reconciliation methods still incur bias or inefficiency. To address this challenge, we propose RB-LoRA, a principled rankвЂ‘balanced aggregation framework that decomposes each update into rankвЂ‘wise components and aligns them using analytically derived weights. Experiments on both language models and vision transformers demonstrate consistent improvements across both one-shot and three-shot settings in language and vision tasks.",Sihyeon ha; Yongjeong Oh; Yo-Seb Jeon,Sihyeon Ha,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness",,rank; lora; aggregation; federated; rank adaptation; adapter; balanced; adaptation; shot; vision
371-FIND,"Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It","We conduct a systematic audit of three widely used reasoning benchmarks, SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark items and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic issues in benchmark design (e.g., duplicated items, ambiguous wording, and implausible answers), as well as scoring procedures that prioritize output form over reasoning process. Through systematic human annotation and re-evaluation on cleaned benchmark subsets, we find that model scores often improve not due to due to erratic surface wording variations and not to improved reasoning. Infact, further analyses show that model performance is highly sensitive to minor input variations such as context availability and phrasing, revealing that high scores may reflect alignment with format-specific cues rather than consistent inference based on the input. These findings challenge the validity of current benchmark-based claims about reasoning in LLMs, and highlight the need for evaluation protocols that assess reasoning as a process of drawing inference from available information, rather than as static output selection. We release audited data and evaluation tools to support more interpretable and diagnostic assessments of model reasoning.",Seyed Mahed Mousavi; Edoardo Cecchinato; Lucia HornГ­kovГЎ; giuseppe riccardi,Seyed Mahed Mousavi,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",,reasoning; benchmark; wording; scores; items; reasoning process; variations; diagnostic; evaluation; output
374-FIND,Confidence-Driven Multi-Scale Model Selection for Cost-Effective NLU,"Large Language Models (LLMs) exhibit varying levels of Natural Language Understanding (NLU), with larger models performing better but at higher computational costs. To optimize cost-effectiveness, simpler queries should be handled by smaller, more efficient models, while complex tasks are escalated to larger ones. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks are delegated to a larger model when necessary, ensuring reliability while minimizing computation. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy comparable to the largest model while reducing computational costs by 20% to 40%. When applied to GPT-4o API calls, it reduces token usage by approximately 60%, further improving cost efficiency. These findings underscore the potential of confidence-based model selection to enhance real-world LLM deployment, particularly in resource-constrained settings such as edge devices and commercial API applications.",Bo-Wei Chen; Chung-Chi Chen; An-Zi Yen,An-Zi Yen,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",confidence; nlu; model selection; larger; cost; api; computational costs; language understanding; costs; selection
375-FIND,Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference,"Structured output from large language models (LLMs) has enhanced efficiency in processing generated information and is increasingly adopted in industrial applications. Prior studies have investigated the impact of structured output on LLMs' generation quality, often presenting one-way findings. Some suggest that structured format enhances completeness and factual accuracy, while others argue that it restricts the reasoning capacity of LLMs and leads to reductions in standard evaluation metrics. Potential limitations of these assessments include restricted testing scenarios, weakly controlled comparative settings, and reliance on coarse metrics. In this work, we present a refined analysis using causal inference. Based on one assumed and two guaranteed constraints, we derive five potential causal structures characterizing the influence of structured output on LLMs' generation: (1) collider without m-bias, (2) collider with m-bias, (3) single cause from instruction, (4) single cause from output format, and (5) independence. Across seven public and one developed reasoning tasks, we find that coarse metrics report positive, negative, or neutral effects of structured output on GPT-4o's generation. However, causal inference reveals no causal impact in 43 out of 48 scenarios. In the remaining 5, 3 involve multifaceted causal structures influenced by concrete instructions. Further experiments show that OpenAI-o3 are more resilient to output formats than general-purpose GPT-4o and GPT-4.1, highlighting an unaware advantage of reasoning models.",Han Yuan; Yue Zhao; Li Zhang; Wuqiong Luo; Zheng Ma,Zheng Ma,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",Summarization & Generation,output; causal; structured; causal inference; format; output format; coarse; cause; gpt; impact
381-FIND,Breaking the Illusion of Reasoning in Polish LLMs: Quality over Quantity of Thought,"Recent advances in large language models (LLMs) have introduced explicit reasoning capabilities, yet the factors that truly drive their improved performance remain unclear. In this work, we disentangle the effects of reasoning quality and sequence length by fine-tuning 8B models on several Polish variants of the Mixture-of-Thoughts (MoT-PL) dataset, each representing a distinct reasoning style: Detailed, Summarized, BabyThink, Lengthy. We found that the model trained on high-quality reasoning traces achieved better average performance than all other models; neither longer reasoning with similar quality nor low-quality reasoning with similar length achieved similar gains. Qualitative and quantitative analyses further reveal that reasoning clarity, rather than verbosity, determines model performance. These findings underscore the importance of reasoning content quality in LLM training and provide new insights into designing more effective reasoning-oriented datasets and models.",Dzmitry Pihulski; MikoЕ‚aj Langner; Jan Eliasz; Przemyslaw Kazienko; Jan Kocon; Teddy Ferdinan,Teddy Ferdinan,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents",,reasoning; quality; similar; polish; quality reasoning; length; achieved; llms quality; reasoning style; illusion
383-FIND,RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library,"The advancement of reasoning capabilities in Large Language Models (LLMs) requires substantial amounts of high-quality reasoning data, particularly in mathematics. Existing data synthesis methods, such as data augmentation from annotated training sets or direct question generation based on relevant knowledge points and documents, have expanded datasets but face challenges in mastering the internal logic of the problem during generation and ensuring the verifiability of the solutions. To address these issues, we propose \textbf{RV-Syn}, a novel \underline{\textbf{R}}ational and \underline{\textbf{V}}erifiable mathematical \underline{\textbf{Syn}}thesis approach. RV-Syn first constructs a structured library of mathematical operations and then composes them into executable computational graphs, which serve as verifiable solution blueprints. These graphs are subsequently back-translated into complex problems, enabling solution-guided, logic-aware problem generation while inherently ensuring the verifiability of the solving process. Experimental results show RV-Syn surpasses existing synthesis methods, including those involving human-crafted problems. Our method achieves a 6.3% performance gain over the previous state-of-the-art synthetic data on LLaMA-3-8B and demonstrates superior data efficiency, outperforming others with only half the training data (50k vs. 100k), enabling a more scalable and robust reasoning dataset generation framework.",jiapeng wang; Jinhao Jiang; Zhiqiang Zhang; JUN ZHOU; Xin Zhao,jiapeng wang,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Summarization & Generation,"Reasoning, Planning & Agents",underline; textbf; synthesis; mathematical; synthesis methods; data synthesis; library; verifiable; generation; logic
387-FIND,WebNovelBench: Placing LLM Novelists on the Web Novel Distribution,"Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation.",Leon Lin; Jun Zheng; Haidong Wang,LIN LIANGTAO,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Summarization & Generation,web; novels; storytelling; long form; narrative; llm; novel; form; driven; generation
388-FIND,From Semantics to Style: A Cross-Dataset Comparative Framework for Sentence Similarity Predictions,"Semantic Textual Similarity (STS) is a central embedding task in natural language processing, yet its definition is inherently ambiguous and varies across datasets. While previous studies have contributed to improving the interpretability of embedding models, most of these advancements are method-based and research on the relationships among datasets remains limited. In this paper, we introduce a comparative framework that attaches lightweight poolers to a frozen encoder, enabling consistent analysis across STS, Paraphrase identification (PI), and Triplet datasets. Our experiments on 21 datasets show that semantic notions are highly correlated across STS/PI settings, whereas style is a distinct concept that requires explicit separation from semantics. Layer-wise and hierarchical clustering analyses further reveal the differences in the properties among concepts and the structural organization of the embedding representation space. These findings suggest that semantic and style should be treated as distinct components in embedding models, with implications for both interpretability and practical applications.",Yusuke Yamauchi; Akiko Aizawa,Yusuke Yamauchi,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Interpretability & Model Analysis,"Linguistics, Syntax & Semantics",sts; embedding; style; comparative; embedding models; datasets; semantic; semantics; interpretability; similarity
390-FIND,Feature Drift: How Fine-Tuning Repurposes Representations in LLMs,"Fine-tuning LLMs introduces many important behaviors, such as instruction following, safety alignment. This makes it crucial to study how fine-tuning changes models' internal mechanisms. Sparse autoencoders (SAEs) offer a powerful tool for interpreting neural networks by extracting concepts (features) represented in their activations. Previous work observed that SAEs trained on base models transfer effectively to instruction-tuned (chat) models, attributed to activation similarity. In this work, we propose *feature drift* as an alternative explanation: the feature space remains relevant, but the distribution of feature activations changes. In other words, fine-tuning recombines existing concepts rather than learning new ones. We validate this by showing base SAEs reconstruct both base and chat activations comparably despite systematic differences, with individual features exhibiting clear drift patterns. In a refusal behavior case study, we identify base SAE features that drift to activate on harmful instructions in chat models. Causal interventions using these features confirm that they mediate refusal. Our findings suggest that monitoring how existing features drift, rather than searching for entirely new features, may provide a more complete explanation of how fine-tuning changes model capabilities.",Andrey V. Galichin; Anton Korznikov; Alexey Dontsov; Oleg Rogov; Elena Tutubalina; Ivan Oseledets,Alexey Dontsov,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Dialogue, Conversational & Interactive NLP","Trustworthy, Safety, Privacy & Fairness",drift; features; feature; chat; saes; base; fine tuning; tuning; activations; changes
391-FIND,Detecting Winning Arguments with Large Language Models and Persuasion Strategies,"Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a chain-of-thought framework that guides reasoning over six persuasion strategies. Results show that strategy-guided reasoning improves the prediction of persuasiveness. To better understand the influence of content, we organize the Winning Argument dataset into broad discussion topics and analyze performance across them. We publicly release this topic-annotated version of the dataset to facilitate future research. Overall, our methodology demonstrates the value of structured, strategy-aware prompting for enhancing interpretability and robustness in argument quality assessment.",Tiziano Labruna; Arkadiusz Modzelewski; Giorgio Satta; Giovanni Da San Martino,Tiziano Labruna,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents",Interpretability & Model Analysis,persuasion; winning; argument; persuasiveness; strategies; arguments; detecting; annotated; strategy; version dataset
395-FIND,The Devil is in the Distributions: Explicit Modeling of Scene Content is Key in Zero-Shot Video Captioning,"Zero-shot video captioning requires that a model generate high-quality captions without human-annotated video-text pairs for training. State-of-the-art approaches to the problem leverage CLIP to extract video-informed text prompts to guide language models in generating captions. However, by using representations at a single granularity (e.g., noun phrases or full sentences), these methods tend to focus on one key aspect of the scene and build a caption that ignores the rest of the visual input. To address this issue, and generate more accurate and complete captions, we propose a novel progressive multi-granularity textual prompting strategy for zero-shot video captioning. Our approach constructs three distinct memory banks, encompassing noun phrases, scene graphs of noun phrases, and entire sentences. Moreover, we introduce a category-aware retrieval mechanism that models the distribution of natural language surrounding the specific topics, to promote prompt diversity while ensuring visual relevance. Extensive experiments on both in-domain and cross-domain settings demonstrate that the proposed method consistently outperforms state-of-the-art approaches.",Mingkai Tian; Guorong Li; Yuankai Qi; Anton van den Hengel; Qingming Huang,Mingkai Tian,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Retrieval, Grounding & External Knowledge (RAG)",video; scene; noun; phrases; captioning; captions; zero shot; zero; granularity; shot
396-FIND,Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning,"While the reasoning abilities of large language models (LLMs) continue to advance, it remains underexplored how such abilities vary across languages in multilingual LLMs and whether different languages generate distinct reasoning paths. In this work, we explore how reasoning knowledge in different languages complements one another. We propose a novel cross-lingual reward modeling framework that aggregates reasoning signals across languages to enhance mathematical reasoning. Our results show a substantial improvement (about 10 scores) in multilingual mathematical reasoning performance compared to using reward modeling within a single language, benefiting even high-resource languages. While English often exhibits the highest performance in multilingual models, we find that cross-lingual sampling benefits English, particularly under low sampling budgets. Our findings reveal new opportunities to improve multilingual reasoning by leveraging the complementary strengths of diverse languages.",Sara Rajaee; Rochelle Choenni; Ekaterina Shutova; Christof Monz,Sara Rajaee,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multilinguality & Low-Resource NLP,"Reasoning, Planning & Agents",reward modeling; reasoning; languages; mathematical reasoning; mathematical; reward; multilingual; cross lingual; lingual; different languages
397-FIND,Nuanced Toxicity Detection in Spanish: A New Corpus and Benchmark Study,"The rise of toxic content on digital platforms has intensified the demand for automatic moderation tools. While English has benefited from large-scale annotated corpora, Spanish remains under-resourced, particularly for nuanced cases of toxicity such as irony, sarcasm, or indirect aggression. We present an extended version of the NECOS-TOX corpus, comprising 4,011 Spanish comments collected from 16 major news outlets. Each comment is annotated across three levels of toxicity (Non-Toxic, Slightly Toxic, and Toxic), following an iterative annotation protocol that achieved substantial inter-annotator agreement (k = 0.74). To reduce annotation costs while maintaining quality, we employed a human-in-the-loop active learning strategy, with manual correction of model pre-labels. We benchmarked the dataset with traditional machine learning (ML) methods, domain-specific transformers, and instruction-tuned large language models (LLMs). Results show that compact encoder models (e.g., RoBERTa-base-bne, 125M parameters) perform on par with much larger models (e.g., LLaMA-3.1-8B), underscoring the value of in-domain adaptation over raw scale. Our error analysis highlights persistent challenges in distinguishing subtle forms of toxicity, especially sarcasm and implicit insults, and reveals entity-related biases that motivate anonymization strategies. The dataset and trained models are released publicly.",Alba MarГ­a MГЎrmol-Romero; Robiert SepГєlveda-Torres; Estela Saquete; MarГ­a-Teresa MartГ­n-Valdivia; L. Alfonso UreГ±a,Alba María Mármol Romero,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",toxic; toxicity; spanish; sarcasm; nuanced; corpus; annotation; annotated; news outlets; moderation tools
398-FIND,Persona Switch: Mixing Distinct Perspectives in Decoding Time,"Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose **Persona Switch**, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13\% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.",Junseok Kim; Nakyeong Yang; Kyomin Jung,Junseok Kim,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents",,persona; role play; switch; output; play; prompting; zero shot; zero; role; shot
399-FIND,Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes,"The rapid rise of deepfake technology poses a severe threat to social and political stability by enabling hyper-realistic synthetic media capable of manipulating public perception. However, existing detection methods struggle with two core limitations: (1) modality fragmentation, which leads to poor generalization across diverse and adversarial deepfake modalities; and (2) shallow inter-modal reasoning, resulting in limited detection of fine-grained semantic inconsistencies. To address these, we propose ConLLM (Contrastive Learning with Large Language Models), a hybrid framework for robust multimodal deepfake detection. ConLLM employs a two-stage architecture: stage 1 uses Pre-Trained Models (PTMs) to extract modality-specific embeddings; stage 2 aligns these embeddings via contrastive learning to mitigate modality fragmentation, and refines them using LLM-based reasoning to address shallow inter-modal reasoning by capturing semantic inconsistencies. ConLLM demonstrates strong performance across audio, video, and audio-visual modalities. It reduces audio deepfake EER by up to 50%, improves video accuracy by up to 8%, and achieves approximately 9% accuracy gains in audio-visual tasks. Ablation studies confirm that PTM-based embeddings contribute 9%вЂ“10% consistent improvements across modalities. Our code and data is available at: https://anonymous.4open.science/r/ConLLM-0D8A/README.md",Gautam Siddharth Kashyap; Harsh Joshi; Niharika Jain; Ebad Shabbir; Jiechao Gao; Nipun Joshi; Usman Naseem,Gautam Siddharth Kashyap,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,,deepfake; audio; modality; modalities; modal; audio visual; fragmentation; semantic inconsistencies; stage; shallow
402-FIND,Detection of Adversarial Prompts with Model Predictive Entropy,"Large Language Models (LLMs) are increasingly deployed in high-impact scenarios, raising concerns about their safety and security. Despite existing defense mechanisms, LLMs remain vulnerable to adversarial attacks. This paper introduces the novel attack-agnostic pipeline SENTRY (semantic entropy-based attack recognition system) for detecting such attacks by leveraging the predictive entropy of model outputs, quantified through the Token-Level Shifting Attention to Relevance (TokenSAR) score, a weighted token entropy measurement. Our approach dynamically identifies adversarial inputs without relying on prior knowledge of attack specifications. It requires only ten newly generated tokens, making it a computationally efficient and adaptable solution. We evaluate the pipeline on multiple state-of-the-art models, including Llama, Vicuna, Falcon, Deep Seek, and Mistral, using a diverse set of adversarial prompts generated via the h4rm31 framework. Experimental results demonstrate a clear separation in TokenSAR scores between benign, malicious, and adversarial prompts. This distinction enables effective threshold-based classification, achieving robust detection performance across various model architectures. Our method outperforms traditional defenses in terms of adaptability and resource efficiency.",Franziska Rubenbauer; Sebastian Steindl; Patrick Levi; Daniel Loebenberger; Ulrich SchГ¤fer,Sebastian Steindl,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Trustworthy, Safety, Privacy & Fairness",,adversarial; entropy; adversarial prompts; attack; prompts; predictive; attacks; pipeline; token; detection
403-FIND,"Actors, Frames and Arguments: A Multi-Decade Computational Analysis of Climate Discourse in Financial News using Large Language Models","We curate a 980,061-article corpus of climate-related financial news from the Dow Jones Newswire (2000вЂ“2023) and introduce a three-stage ActorвЂ“FrameвЂ“Argument (AFA) pipeline that uses large language models to extract actors, stances, frames, and argumentative structures. We conduct AFA extraction on a stratified, uncertainty-enriched sample of 4,143 articles that preserves the temporal and thematic distributions of the full corpus. Reliability is established with a 2,000-article human-annotated gold standard and a Decompositional Verification Framework (DVF) that decomposes evaluation into completeness, faithfulness, coherence, and relevance, with multi-judge scoring calibrated against human ratings. Our longitudinal analysis uncovers a structural shift after 2015: coverage transitions from risk and regulatory-burden frames toward economic opportunity and technological innovation; financial institutions and companies increasingly deploy opportunity-centered arguments, while NGOs emphasize environmental urgency and governments stress compliance. Methodologically, we provide a replicable paradigm for longitudinal media analysis with LLMs. For high-stake domain insights, we map how the financial sector has internalized and reframed the climate crisis across two decades.",Ruiran Su; Markus Leippold; Janet B. Pierrehumbert,Ruiran Su,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",,financial; frames; opportunity; article; news; arguments; analysis; corpus; stratified; internalized
405-FIND,RECAP: REwriting Conversations for Intent Understanding in Agentic Planning,"Understanding user intent is essential for effective planning in conversational assistants, particularly those powered by large language models (LLMs) coordinating multiple agents. However, real-world dialogues are often ambiguous, underspecified, or dynamic, making intent understanding a persistent challenge. Traditional classification-based approaches struggle to generalize in open-ended settings, leading to brittle interpretations and poor downstream planning. We propose RECAP (REwriting Conversations for Agent Planning), a new benchmark designed to evaluate and advance intent rewriting, reframing user-agent dialogues into concise representations of user goals. RECAP captures diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. Alongside the dataset, we introduce an LLM-based evaluator that compares planning utility given a user-agent dialogue. Using RECAP, we develop a prompt-based rewriting approach that outperforms baselines, in terms of plan preference. We further demonstrate that fine-tuning two DPO-based rewriters yields additional utility gains. Our results highlight intent rewriting as a critical and tractable component for improving agentic planning in open-domain dialogue systems.",Kushan Mitra; Dan Zhang; Hannah Kim; Estevam Hruschka,Kushan Mitra,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents","Dialogue, Conversational & Interactive NLP",rewriting; planning; intent; conversations; user; intent understanding; user agent; dialogues; agent; agentic
412-FIND,Modeling Turn-Taking with Semantically Informed Gestures,"In conversation, humans use multimodal cues, such as speech, gestures, and gaze, to manage turn-taking. While linguistic and acoustic features are informative, gestures provide complementary cues for modeling these transitions. To study this, we introduce DnD Gesture++, an extension of the multi-party DnD Gesture corpus enriched with 2,663 semantic gesture annotations spanning iconic, metaphoric, deictic, and discourse types. Using this dataset, we model turn-taking prediction through a Mixture-of-Experts framework integrating text, audio, and gestures. Experiments show that incorporating semantically guided gestures yields consistent performance gains over baselines, demonstrating their complementary role in multimodal turn-taking.",Varsha Suresh; M. Hamza Mughal; Christian Theobalt; Vera Demberg,M. Hamza Mughal,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,"Linguistics, Syntax & Semantics",turn taking; taking; turn; semantically; complementary; cues; modeling; multimodal; yields consistent; acoustic features
418-FIND,Do Large Language Models Reflect Demographic Pluralism in Safety?,"Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as Anthropic-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BeaverTails) using Mistral-7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage~II, pluralistic sensitivity is evaluated using LLMs-as-RatersвЂ”Gemma-7B, GPT-4o, and LLaMA-2-7BвЂ”under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust. Code and data available at: https://anonymous.4open.science/r/Demo-SafetyBench-6B91/",Usman Naseem; Gautam Siddharth Kashyap; Sushant Kumar Ray; Rafiq Ali; Ebad Shabbir; Abdullah Mohammad,Gautam Siddharth Kashyap,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",demographic; pluralistic; safety; demo; instruct; sensitivity; stage; llama; safety large language; safety large
427-FIND,Adversarial Decoding: Generating Readable Documents for Adversarial Objectives,"We design, implement, and evaluate adversarial decoding, a new, generic text generation technique that produces readable documents for adversarial objectives such as RAG poisoning, jailbreaking, and evasion of defensive filters. Prior generation methods either produce easily detectable gibberish (even methods that optimize for low perplexity), or cannot handle objectives that include embedding similarity. In particular, they cannot produce readable adversarial documents that (1) are retrieved by RAG systems in response to broad classes of queries, and (2) adversarially influence subsequent generation. We measure the effectiveness of adversarial decoding for different objectives and demonstrate that it outperforms existing methods while producing adversarial documents that cannot be automatically distinguished from natural documents by fluency and readability.",Collin Zhang; Tingwei Zhang; Vitaly Shmatikov,Collin Zhang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,"Trustworthy, Safety, Privacy & Fairness",adversarial; documents; readable; objectives; decoding; produce; rag; generation; methods; defensive
434-FIND,MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators,"Evaluating the quality of open-domain chatbots has become increasingly reliant on LLMs acting as automatic judges. However, existing meta-evaluation benchmarks are static, outdated, and lacking in multilingual coverage, limiting their ability to fully capture subtle weaknesses in evaluation. We introduce MEDAL, an automated multi-agent framework for curating more representative and diverse open-domain dialogue evaluation benchmarks. Our approach leverages several LLMs to generate user-chatbot multilingual dialogues, conditioned on varied seed contexts. Then, a state-of-the-art LLM (GPT-4.1) is used for a multidimensional analysis of the performance of the chatbots, uncovering noticeable cross-lingual performance differences. Guided by this large-scale evaluation, we curate a new meta-evaluation multilingual benchmark and human-annotate samples with nuanced quality judgments. This benchmark is then used to assess the ability of several reasoning and non-reasoning LLMs to act as evaluators of open-domain dialogues. Using MEDAL, we uncover that state-of-the-art judges fail to reliably detect nuanced issues such as lack of empathy, common sense, or relevance.",John MendonГ§a; Alon Lavie; Isabel Trancoso,John Mendonça,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",Multilinguality & Low-Resource NLP,open domain; medal; open domain dialogue; meta evaluation; domain dialogue; multilingual; open; chatbots; evaluation benchmarks; domain
437-FIND,Which Works Best for Vietnamese? A Practical Study of Information Retrieval Methods across Domains,"Large Language Models (LLMs) have achieved remarkable progress, yet their reliance on parametric knowledge often leads to hallucinations. Retrieval-Augmented Generation (RAG) mitigates this issue by grounding outputs in external documents, where the quality of retrieval is critical. While retrieval methods have been widely benchmarked in English, it remains unclear which approaches are most effective for Vietnamese, a language characterized by informal queries, noisy documents, and limited resources. Prior studies are restricted to clean datasets or narrow domains, leaving fragmented insights. To the best of our knowledge, we present the first comprehensive benchmark of retrieval methods for Vietnamese across multiple real-world domains. We systematically compare lexical, dense, and hybrid methods on datasets spanning education, legal, healthcare, customer support, lifestyle, and Wikipedia, and introduce two new datasets capturing authentic educational counseling and customer service interactions. Beyond reporting benchmark numbers, we distill a set of empirical insights that clarify trade-offs, highlight domain-specific challenges, and provide practical guidance for building robust Vietnamese QA systems. Together, these contributions offer the first large-scale, practice-oriented perspective on Vietnamese retrieval and inform both academic research and real-world deployment in low-resource languages. All datasets and evaluation scripts are available atВ https://anonymous.4open.science/r/ViRE.",Long Nguyen; Tho Quan,Nguyen Song Thien Long,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)",Domain NLP (Biomedical/Clinical/Legal/Scientific),vietnamese; retrieval; retrieval methods; customer; datasets; domains; methods; documents; best; insights
438-FIND,MemeWeaver: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection,"Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon, where perpetrators reinforce prejudices and group identity within like-minded communities. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. In this work, we present MemeWeaver, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. We systematically evaluate multiple visual-textual fusion strategies and show that our approach consistently outperforms state-of-the-art baselines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.",Paolo Italiani; David Gimeno-GГіmez; Luca Ragazzi; Gianluca Moro; Paolo Rosso,Paolo Italiani,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multimodal & Speech/Audio,"Reasoning, Planning & Agents",graph; sexism; graph reasoning; fusion; inter; online; end; multimodal; reasoning; harassment
442-FIND,SEAM: Bridging the Temporal-Semantic Granularity Gap for LLM-based Speech Recognition,"Speech-LLM integration faces a temporal-semantic granularity gap: speech representations scale with temporal duration while text tokens scale with semantic content. Existing duration-based methods generate embeddings at fixed rates, creating distributional mismatch with LLM pre-training. We propose SEAM (Speech Encoder-Decoder Alignment Module), an encoder-decoder architecture employing variable-rate generation through cross-attention between speech features and text embeddings. SEAM produces embeddings at adaptive rates that closely match natural text distributions while preserving pre-trained knowledge by freezing both speech encoder and LLM. We introduce a multi-stage training strategy and First Token Guidance to improve initial token prediction. SEAM achieves competitive performance on LibriSpeech (2.6\%/5.2\% WER). More significantly, trained only on LibriSpeech (960h), SEAM achieves 4.7\% WER on cross-domain TED-LIUM-v2, demonstrating that integrating LLM's linguistic knowledge enables effective generalization beyond limited speech training data.",Junseok Oh; Ji-Hwan Kim,Junseok Oh,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multimodal & Speech/Audio,Summarization & Generation,speech; semantic granularity; temporal semantic; duration; speech encoder; temporal; encoder; wer; encoder decoder; embeddings
450-FIND,"Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness","Large Language Models (LLMs) encode substantial factual knowledge, yet measuring and systematizing this knowledge remains challenging. Converting it into structured formatвЂ”for example through recursive extraction approaches such as the GPTKB methodology (Hu et al., 2025b)вЂ”is still underexplored. Key open questions include whether such extraction can terminate, whether its outputs are reproducible, and how robust they are to variations. We systematically study LLM knowledge materialization using $miniGPTKBs$ (domain-specific, tractable subcrawls), analyzing termination, reproducibility, and robustness across three categories of metrics: yield, lexical similarity, and semantic similarity. We experiment with four variations (seed, language, randomness, model) and three illustrative domains (from history, entertainment, and finance). Our findings show (i) high termination rates, though model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies by perturbation typeвЂ”high for seeds and temperature, lower for languages and models. These results suggest that LLM knowledge materialization can reliably surface core knowledge, while also revealing important limitations.",Luca Giordano; Simon Razniewski,Luca Giordano,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Information Extraction & Structured Prediction,,llm knowledge; reproducibility; knowledge; robustness; variations; similarity; extraction; suggest llm; languages models; study llm
459-FIND,Investigating Gender Stereotypes in Large Language Models via Social Determinants of Health,"Large Language Models (LLMs) excel in Natural Language Processing (NLP) tasks, but they often propagate biases embedded in their training data, which is potentially impactful in sensitive domains like healthcare. While existing benchmarks evaluate biases related to individual social determinants of health (SDoH) such as gender or ethnicity, they often overlook interactions between these factors and lack context-specific assessments. This study investigates bias in LLMs by probing the relationships between gender and other SDoH in French patient records. Through a series of experiments, we found that embedded stereotypes can be probed using SDoH input and that LLMs rely on embedded stereotypes to make gendered decisions, thus positioning the assessment of relationships between SDoH as a potential complement to the evaluation of LLM performance.",Trung Hieu Ngo; Adrien Bazoge; Solen Quiniou; Pierre-Antoine GOURRAUD; Emmanuel Morin,Trung Hieu NGO,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"LLM Evaluation, Benchmarks & Metrics",stereotypes; embedded; gender; social determinants; determinants health; social determinants health; determinants; relationships; health; biases
463-FIND,FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale,"Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks---masked operation prediction and step completion---that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.",Isabelle Lee; Sarah Liaw; Dani Yogatama,Isabelle Lee,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",traces; fol; structured logical; logical inference; operation; masked; reasoning traces; completion; logical; verified
474-FIND,Uncertainty Quantification for Evaluating Gender Bias in Machine Translation,"The predictive uncertainty of machine translation (MT) models is typically used as a quality estimation proxy. In this work, we posit that apart from confidently translating when a single correct translation exists, models should also maintain uncertainty when the input is ambiguous. We use uncertainty to measure gender bias in MT systems. When the source sentence includes a lexeme whose gender is not overtly marked, but whose target-language equivalent requires gender specification, the model must infer the appropriate gender from the context and can be susceptible to biases. Prior work measured bias via gender accuracy, however it cannot be applied to ambiguous cases. Using semantic uncertainty, we are able to assess bias when translating both ambiguous and unambiguous source sentences, and find that high translation accuracy does not correlate with exhibiting uncertainty appropriately, and that debiasing affects the two cases differently.",Ieva Staliunaite; Julius Cheng; Andreas Vlachos,Ieva Raminta Staliunaite,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Machine Translation,,gender; uncertainty; ambiguous; translation; bias; gender bias; translating; machine translation; cases; machine
478-FIND,PIRA: Preference-Oriented Instruction-Tuned Reward Models with Dual Aggregation,"Reward models are crucial for aligning Large Language Models (LLMs) with human preferences but face two representative challenges. First, traditional discriminative reward models usually concatenate questions and responses directly as input, resulting in low data efficiency. Second, reward models are vulnerable to reward overoptimization. We propose \textbf{PIRA}, a training paradigm addressing these issues through three strategies: (1) Reformulating questionвЂ“answer pairs into preference-based instructions for clearer and more explicit task specification, (2) aggregating rewards from diverse preference tasks to reduce bias and improve robustness, and (3) averaging value-head outputs under varying dropout rates to stabilize rewards. Extensive experiments have demonstrated the effectiveness of PIRA.",Yongfu Xue,Yongfu Xue,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","Trustworthy, Safety, Privacy & Fairness",reward; reward models; preference; rewards; dropout; rewards extensive; rewards extensive experiments; reduce bias; bias improve; reformulating
480-FIND,The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models,"Large Language Models (LLMs) are increasingly integral to information dissemination and decision-making processes. Given their growing societal influence, understanding potential biases, particularly within the political domain, is crucial to prevent undue influence on public opinion and democratic processes. This work investigates political bias and stereotype propagation across eight prominent LLMs using the two-dimensional Political Compass Test (PCT). Initially, the PCT is employed to assess the inherent political leanings of these models. Subsequently, persona prompting with the PCT is used to explore explicit stereotypes across various social dimensions. In a final step, implicit stereotypes are uncovered by evaluating models with multilingual versions of the PCT. Key findings reveal a consistent left-leaning political alignment across all investigated models. Furthermore, while the nature and extent of stereotypes vary considerably between models, implicit stereotypes elicited through language variation are more pronounced than those identified via explicit persona prompting. Interestingly, for most models, implicit and explicit stereotypes show a notable alignment, suggesting a degree of transparency or ""awareness"" regarding their inherent biases. This study underscores the complex interplay of political bias and stereotypes in LLMs.",Konrad LГ¶hr; Shuzhou Yuan; Michael FГ¤rber,Konrad Löhr,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Trustworthy, Safety, Privacy & Fairness",Multilinguality & Low-Resource NLP,stereotypes; political; implicit; explicit; political bias; persona prompting; bias; persona; processes; inherent
482-FIND,TIPA: Typologically Informed Parameter Aggregation,"Massively multilingual language models enable cross-lingual generalization but underperform on low-resource and unseen languages. While adapter-based fine-tuning offers a parameter-efficient solution, training language-specific adapters at scale remains costly. We introduce Typologically Informed Parameter Aggregation (TIPA), a training-free framework that constructs proxy language adapters by aggregating existing ones, weighted by typological similarity. Integrated into the MAD-X architecture, these proxies enable zero-shot cross-lingual transfer without additional training. We evaluate TIPA on five NLP tasks and over 230 languages. TIPA consistently outperforms baselines such as English-only fine-tuning and selecting the typologically closest-language adapter, with the largest gains for languages lacking dedicated adapters. Our results demonstrate that typologically informed aggregation provides a viable alternative to language-specific modules without any training needed.",Stef Accou; Wessel Poelman,Stef Accou,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multilinguality & Low-Resource NLP,,typologically; adapters; aggregation; informed; parameter; adapter; language specific; training; languages; cross lingual
483-FIND,Can Calibration of Positional Encodings Enhance RAG Performance?,"Large language models suffer from positional biases like the ""Lost in the Middle'"" (LiM) phenomenon and recency bias, which undermine the performance of long-context retrieval augmented generation. In this work, we investigate the role of rotary position embeddings (RoPE) in this context. Our empirical study confirms the persistence of these biases in modern language models. Drawing on these findings, we introduce Caliope, a training-free calibration framework for modifying RoPE inputs at inference time. Our calibrators yield substantial improvements in specific model-task configurations, revealing complex performance trade-offs between simple needle-in-a-haystack experiments and more demanding cross-chunk reasoning tasks. This work offers a practical mitigation framework and key insights into the nature of positional biases.",Tom Zehle; Matthias AГџenmacher,Tom Zehle,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)","LLM Evaluation, Benchmarks & Metrics",positional; positional biases; rope; biases; calibration; inputs inference; inputs inference time; yield substantial; recency; tasks work
487-FIND,CrisiText: A dataset of warning messages for LLM training in emergency communication,"Effectively identifying threats and mitigating their potential damage during crisis situations, such as natural disasters or violent attacks, is paramount for safeguarding endangered individuals. To tackle these challenges, AI has been used in assisting humans in emergency situations. Still, the use of NLP techniques remains limited, and mostly focuses on classification tasks. The significant potential of timely warning message generation using NLG architectures, however, has been largely overlooked. In this paper we present CrisiText, the first large-scale dataset for the generation of warning messages across 13 different types of crisis scenarios. The dataset contains more than 400,000 warning messages (spanning almost 18,000 crisis situations) aimed at assisting civilians during and after such events. To generate the dataset, we started from existing crisis descriptions, and created chains of events related to the scenarios. Each event was then paired with a warning message. The generations follow expertвЂ™s written guidelines to ensure correct terminology and factuality of their suggestions. Additionally, each message is accompanied by three suboptimal warning types to allow for the study of different NLG approaches. To this end, we conducted a series of experiments comparing supervised fine-tuning setups with preference alignment, zero-shot, and few-shot approaches. We further assessed model performance in out-of-distribution scenarios and evaluated the effectiveness of an automatic post-editor.",Giacomo Gonella; Gian Maria Campedelli; Stefano Menini; Marco Guerini,Giacomo Gonella,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Summarization & Generation,Machine Translation,warning; crisis; messages; situations; nlg; crisis situations; assisting; scenarios; events; dataset
488-FIND,FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition,"Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacherвЂ“student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. We train a student model on this dataset to enable the research community to efficiently expand annotations in their desired languages. The regression model for filtering useful NER passages achieves more than 84 F1. We assess annotation quality using LLM-based ratings, which show high faithfulness (3.99/5) and completeness (4.05/5). We further translate the label set into the respective target languages and observe a performance decrease of 0.02-0.09 F1. This result shows that many multilingual models rely heavily on English label prompts and that improved cross-lingual alignment is essential for future work.",Jonas Golde; Patrick Haller; Alan Akbik,Jonas Golde,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Information Extraction & Structured Prediction,Multilinguality & Low-Resource NLP,passages; ner; multilingual; entity; regression; named entity recognition; label; entity recognition; named entity; student
489-FIND,"Bias in the East, Bias in the West: A Bilingual Analysis of LLM Political Bias on U.S.- and China-Related Issues","Large language models (LLMs) can exhibit political biases, which creates a risk of undue influence on LLM users and public opinion. Yet despite LLMs being used across the world, there is little evidence on how political biases vary across languages. And despite a growing number of frontier LLMs (e.g., DeepSeek) released by non-U.S. organizations, there is limited understanding of how political biases vary across LLMs developed in different political contexts. To address these gaps, we measure LLM bias on U.S.- and China-related issues, and how bias varies by 1) prompt language (English vs. Chinese) and 2) model origin (U.S. vs. Chinese). For this purpose, we create a new parallel dataset of 36k realistic test prompts asking models to write about a balanced set of 60 political issues sourced from national U.S. and Chinese news outlets. Using this dataset, we show that both model origin and prompt language systematically influence bias. Language effects dominate on China-related issues, particularly those involving sovereignty and human rights, while model origin effects are more pronounced on U.S.-related governance and foreign policy topics. Overall, our results highlight a need for language and context-specific measurement of LLM political bias.",Ying Ying Lim; Paul Rottger,Paul Rottger,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,political; bias; political biases; related issues; origin; issues; related; chinese; llm political; political bias
491-FIND,"Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone","As Vision Language Models (VLMs) become integral to real-world applications, understanding their demographic biases is critical. We introduce GRAS, a benchmark for uncovering demographic biases in VLMs across gender, race, age, and skin tone, offering the most diverse coverage to date. We further propose the GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark five state-of-the-art VLMs and reveal concerning bias levels, with the least biased model attaining a GRAS Bias Score of 98, far from the unbiased ideal of 0. Our findings also reveal a methodological insight: evaluating bias in VLMs with visual question answering (VQA) requires considering multiple formulations of a question. Our code, data, and evaluation results are publicly available at https://anonymous.4open.science/r/vqa-skin-bias-5DA3/",Shaivi Malik; Hasnat Md Abdullah; Sriparna Saha; Amit Sheth,Shaivi Malik,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",bias; vlms; demographic biases; gender race age; race age; gender race; race; tone; age; vqa
492-FIND,A Simple and Efficient Learning-Style Prompting for LLM Jailbreaking,"Safety alignment aims to prevent Large Language Models (LLMs) from responding to harmful queries. To strengthen safety protections, jailbreak methods are developed to simulate malicious attacks and uncover vulnerabilities. In this paper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel jailbreak approach that systematically transforms imperative harmful requests into learning-style questions with only straightforward hypotheticality indicators. Further, we introduce new metrics to thoroughly evaluate the efficiency and harmfulness of jailbreak methods. Experiments on the AdvBench dataset across a wide range of models demonstrate HILLвЂ™s strong generalizability. It achieves top attack success rates on the majority of models and across malicious categories while maintaining high efficiency with concise prompts. Results of various defense methods show the robustness of HILL, with most defenses having mediocre effects or even increasing the attack success rates. In addition, the assessment of defenses on our constructed safe prompts reveals inherent limitations of LLMs' safety mechanisms and flaws in the defense methods. This work exposes significant vulnerabilities of safety measures against learning-style elicitation, highlighting a critical challenge of fulfilling both helpfulness and safety alignments.",Xuan Luo; YUE WANG; Zefeng He; Geng Tu; Jing Li; Ruifeng Xu,Xuan LUO,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Trustworthy, Safety, Privacy & Fairness",,safety; jailbreak; defense methods; attack success rates; style; success rates; learning; attack success; defense; defenses
494-FIND,Aggregating Crowd of LLMs for Cost-Effective Data Annotation,"Recent advancements in Large Language Models (LLMs) have shown promise for automated data annotation, yet reliance on expensive commercial models like GPT-4 limits accessibility. This paper rigorously evaluates the potential of open-source smaller LLMs (sLLMs) as a cost-effective alternative. We introduce a new benchmark dataset, Multidisciplinary Open Research Data (MORD), comprising 12,277 annotated sentence segments from 1,500 schoolarly articles across five research domains, to systematically assess sLLM performance. Our experiments demonstrate that sLLMs achieve annotation quality surpassing Amazon MTurk workers and approach GPT-4вЂ™s accuracy at significantly lower costs. We further propose to build the Crowd of LLMs, which aggregates annotations from multiple sLLMs using label aggregation algorithms. This approach not only outperforms individual sLLMs but also reveals that combining sLLM annotations with human crowd labels yields superior results compared to either method alone. Our findings highlight the viability of sLLMs for democratizing high-quality data annotation while underscoring the need for tailored aggregation methods to fully realize their potential.",Jiacheng Liu; Xiaofeng Hou,Xiaofeng Hou,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",,crowd; data annotation; annotation; cost effective; aggregation; annotations; cost; gpt; potential; llms
497-FIND,Representation Collapse in Machine Translation Through the Lens of Angular Dispersion,"Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.",Evgeniia Tokarchuk; Maya K. Nachesa; Sergey Troshin; Vlad Niculae,Evgeniia Tokarchuk,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Machine Translation,Interpretability & Model Analysis,collapse; representation; translation; angular; regularization; continuous; machine translation; transformer; neural; machine
499-FIND,Training-Free Text Emotion Tagging via LLM-Based Best-Worst Scaling,"Large Language Models (LLMs) have been frequently used as automatic annotators for tasks such as Text Emotion Recognition (TER). We consider a scenario in which annotators assign at least one emotion label from a large set of options to a text snippet. For this emotion tagging task, we propose a novel zero-shot algorithm that leverages Best-Worst Scaling (BWS), prompting the LLM to choose the least and most suitable emotions for a given text from several label subsets. The LLM's choices can be represented by a graph linking labels via worse-than relations. Random walks on this graph yield the final score for each label. We compare our algorithm with naive prompting approaches as well as an established BWS-based method. Extensive experiments demonstrate the suitability of the method. It proves to compare favorably to the benchmarks in terms of both accuracy and calibration with respect to human annotations. Moreover, our algorithm's automatic annotations are shown to be suitable for finetuning lightweight emotion classification models. The proposed method consumes considerably fewer computational resources than the established BWS approach.",Lukas Christ; Shahin Amiriparian,Lukas Christ,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",,emotion; label; algorithm; tagging; suitable; text; annotators; established; scaling; compare
512-FIND,Can LLMs Reason Like Doctors? Exploring the Limits of Large Language Models in Complex Medical Reasoning,"Large language models (LLMs) have shown remarkable progress in reasoning across multiple domains. However, it remains unclear whether their abilities reflect genuine reasoning or sophisticated pattern matching, a distinction critical in medical decision-making, which requires reliable multistep problem-solving. To this end, we conducted one of the largest evaluations of its kind, assessing 77 LLMs with diverse fine-tuning approaches, ranging from 1 billion parameters to frontier models. Guided by medical problem-solving theory, we selected three medical question-answering benchmarks targeting key reasoning skills: reasoning processes, susceptibility to cognitive biases, and metacognitive abilities. Additionally, we manually annotated a subset of questions to assess the abduction, deduction, and induction capabilities of LLMs, offering fine-grained insights into the reasoning processes employed by physicians, an aspect that has received relatively limited attention in this domain. Most models, particularly smaller ones, struggled even with specialized fine-tuning or advanced prompting. Larger models performed better but still showed clear limitations in complex medical reasoning, highlighting the need to improve specific reasoning strategies to better reflect physician decision-making. The datasets and code used in this study are publicly available at: https://anonymous.4open.science/r/Can-LLMs-Reason-Like-Doctors-6B20/README.md",Flavio Merenda; Jose Manuel Gomez-Perez; German Rigau,Flavio Merenda,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Reasoning, Planning & Agents",Domain NLP (Biomedical/Clinical/Legal/Scientific),medical; reasoning; complex medical; doctors; medical reasoning; llms reason; reasoning processes; problem solving; decision making; processes
515-FIND,Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish,"Large Language Models (LLMs) have become an increasingly important tool in research and society at large. While LLMs are regularly used all over the world by experts and lay-people alike, they are predominantly developed with English-speaking users in mind, performing well in English and other wide-spread languages while less-resourced languages such as Luxembourgish are seen as a lower priority. This lack of attention is also reflected in the sparsity of available evaluation tools and datasets. In this study, we investigate the viability of language proficiency exams as such evaluation tools for the Luxembourgish language. We find that large models such as Claude and DeepSeek-R1 typically achieve high scores, while smaller models show weak performances. We also find that the performances in such language exams can be used to predict performances in other NLP tasks in Luxembourgish.",Cedric Lothritz; Jordi Cabot; Laura Bernardy,Cedric Lothritz,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multilinguality & Low-Resource NLP,"LLM Evaluation, Benchmarks & Metrics",performances; exams; evaluation tools; language proficiency; proficiency; tools; language; large models; developed english; reflected
516-FIND,Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders,"Sparse Autoencoders (SAEs) have been successfully used to probe Large Language Models (LLMs) and extract interpretable concepts from their internal representations. These concepts are linear combinations of neuron activations that correspond to human-interpretable features. In this paper, we investigate the effectiveness of SAE-based explainability approaches for sentence classification, a domain where such methods have not been extensively explored. We present a novel SAE-based model \verb|ClassifSAE| tailored for text classification, leveraging a specialized classifier head and incorporating an activation rate sparsity loss. We benchmark this architecture against established methods such as ConceptShap, Independent Component Analysis, HI-Concept and a standard TopK-SAE baseline. Our evaluation covers several classification benchmarks and backbone LLMs. We further enrich our analysis with two novel metrics for measuring the precision of concept-based explanations, using an external sentence encoder. Our empirical results show that \verb|ClassifSAE| improves both the causality and interpretability of the extracted features.",Mathis Le Bail; JГ©rГ©mie Dentan; Davide Buscaldi; Vanier Sonia,Mathis Le Bail,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Interpretability & Model Analysis,"LLM Evaluation, Benchmarks & Metrics",sae; classification; concepts; sae based; verb; text classification; interpretable; autoencoders; sparse autoencoders; concept
517-FIND,"TextMine: Data, Evaluation Framework and Ontology-guided LLM Pipeline for Humanitarian Mine Action","Humanitarian Mine Action (HMA) addresses the challenge of detecting and removing landmines from conflict regions. Much of the life-saving operational knowledge produced by HMA agencies is buried in unstructured reports, limiting the transferability of information between agencies. To address this issue, we propose TextMine: the first dataset, evaluation framework and ontology-guided large language model (LLM) pipeline for knowledge extraction in the HMA domain. TextMine structures HMA reports into (subject, relation, object)-triples, thus creating domain-specific knowledge. To ensure real-world relevance, we created the dataset in collaboration with Cambodian Mine Action Center (CMAC). We further introduce a bias-aware evaluation framework that combines human-annotated triples with an LLM-as-Judge protocol to mitigate position bias in reference-free scoring. Our experiments show that ontology-aligned prompts improve extraction accuracy by up to 44.2%, reduce hallucinations by 22.5%, and enhance format adherence by 20.9% compared to baseline models. We publicly release the dataset and code.",Chenyue Zhou; GГјrkan Solmaz; Flavio Cirillo; Kiril Gashteovski; Jonathan FГјrst,Chenyue Zhou,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",ontology; action; evaluation framework; llm pipeline; triples; reports; knowledge; extraction; guided; bias
523-FIND,MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning,"Mathematical reasoning remains one of the most challenging domains for large language models (LLMs), requiring not only linguistic understanding but also structured logical deduction and numerical precision. While recent LLMs demonstrate strong general-purpose reasoning abilities, their mathematical competence across diverse languages remains underexplored. Existing benchmarks primarily focus on English or a narrow subset of high-resource languages, leaving significant gaps in assessing multilingual and cross-lingual mathematical reasoning. To address this, we introduce MathMist, a parallel multilingual benchmark for mathematical problem solving and reasoning. MathMist encompasses over 21K aligned questionвЂ“answer pairs across seven languages, representing a balanced coverage of high-, medium-, and low-resource linguistic settings. The dataset captures linguistic variety, multiple types of problem settings, and solution synthesizing capabilities. We systematically evaluate a diverse suite of models, including open-source small and medium LLMs, proprietary systems, and multilingual-reasoning-focused models, under zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our results reveal persistent deficiencies in LLMsвЂ™ ability to perform consistent and interpretable mathematical reasoning across languages, with pronounced degradation in low-resource settings. All the codes and data are available at GitHub: https://anonymous.4open.science/r/MathMist-3103",Mahbub E Sobhani; Md. Faiyaz Abdullah Sayeedi; Tasnim Mohiuddin; Md Mofijul Islam; Swakkhar Shatabda,Mahbub E Sobhani,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multilinguality & Low-Resource NLP,"Reasoning, Planning & Agents",mathematical; reasoning; mathematical reasoning; solving reasoning; problem solving reasoning; multilingual; languages; mathematical problem; mathematical problem solving; medium
532-FIND,Enhancing Reliability in Community Question Answering with an Expert-Oriented RAG System,"In recent years, pre-trained large language models (LLMs) have become a cornerstone for automatically generating answers in question-and-answer (Q&A) communities, significantly reducing user wait times and improving response quality. However, these models require substantial computational resources and are prone to generating hallucinated or unreliable content. To overcome these limitations, we propose an advanced expert-oriented Retrieval-Augmented Generation (RAG) framework as a cost-effective and reliable alternative. Central to our approach is a user-aware question entailment recognition module, which leverages user modeling to identify archived questions with answers that fully or partially address the user's new query. This user modeling significantly improves retrieval relevance, resulting in reduced hallucination and enhanced answer quality. The framework synthesizes expert-written answers from similar questions to generate unified responses. Experimental results on the CQADupStack and SE-PQA datasets show the superiority of our user-aware approach over its user-agnostic counterpart, with ROUGE-1 gains of 3.6% and 0.9%. Both human and AI evaluations confirm the effectiveness of incorporating user modeling in minimizing hallucination and delivering contextually appropriate answers, demonstrating its potential for real-world Q&A systems. The code and data are available on a GitHub repository at https://anonymous.4open.science/r/User-Oriented-RAG-CQA.",Seyyede Zahra Aftabi; Saeed Farzi,Saeed Farzi,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,user; answers; oriented; user aware; rag; expert; modeling; question; hallucination; generating
535-FIND,Unsupervised Text Style Transfer for Controllable Intensity,"Unsupervised Text Style Transfer (UTST) aims to build a system to transfer the stylistic properties of a given text without parallel text pairs. Compared with text transfer between style polarities, UTST for controllable intensity is more challenging due to the subtle differences in stylistic features across different intensity levels. Faced with the challenges posed by the lack of parallel data and the indistinguishability between adjacent intensity levels, we propose a SFT-then-PPO paradigm to fine-tune an LLM. We first fine-tune the LLM with synthesized parallel data. Then, we further train the LLM with PPO, where the rewards are elaborately designed for distinguishing the stylistic intensity in hierarchical levels. Both the global and local stylistic features are considered to formulate the reward functions. The experiments on two UTST benchmarks showcase that both rewards have their advantages and applying them to LLM fine-tuning can effectively improve the performance of an LLM backbone based on various evaluation metrics. Even for close levels of intensity, we can still observe the noticeable stylistic difference between the generated text.",Shuhuan Gu; Wenbiao Tao; Xinchen Ma; Kangkang He; Ye Guo; Xiang Li; Yunshi Lan,Shuhuan Gu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,Multilinguality & Low-Resource NLP,intensity; stylistic; levels; transfer; text; intensity levels; text style transfer; text style; parallel; style
536-FIND,SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases,"Text-to-SQL systems translate natural language questions into executable SQL queries, and recent progress with large language models (LLMs) has driven substantial improvements in this task. Schema linking remains a critical component in Text-to-SQL systems, reducing prompt size for models with narrow context windows and sharpening model focus even when the entire schema fits. We present a zero-shot, training-free schema linking approach that first constructs a schema graph based on foreign key relations, then uses a single prompt to a lightweight LLM to extract source and destination tables from the user query, followed by applying classical path-finding algorithms and post-processing to identify the optimal sequence of tables and columns that should be joined, enabling the LLM to generate more accurate SQL queries. To handle real-world databases where foreign keys may be missing or inconsistent, we further propose an LLM-guided joinability discovery step that infers table connections before graph construction, ensuring robustness across diverse schemas. Despite being simple, cost-effective, and highly scalable, our method achieves state-of-the-art results on both the BIRD and Spider 2.0 benchmarks, outperforming previous specialized, fine-tuned, and complex multi-step LLM-based approaches.",AmirHossein Safdarian; Milad Mohammadi; Ehsan Jahanbakhsh Bashirloo; Mona Shahamat Naderi; Heshaam Faili,AmirHossein Safdarian,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",sql; schema; schema linking; text sql; linking; sql systems; foreign; text sql systems; databases; sql queries
539-FIND,Binary Token-Level Classification with DeBERTa for All-Type MWE Identification: A Lightweight Approach with Linguistic Enhancement,"We present a comprehensive approach for multiword expression (MWE) identification that combines binary token-level classification, linguistic feature integration, and data augmentation. Our DeBERTa-v3-large model achieves 69.8% F1 on the CoAM dataset, surpassing the best results (Qwen-72B, 57.8% F1) on this dataset by 12 points while using 165 times fewer parameters. We achieve this performance by (1) reformulating detection as binary token-level START/END/INSIDE classification rather than span-based prediction, (2) incorporating NP chunking and dependency features that help discontinuous and NOUN-type MWEs identification, and (3) applying oversampling that addresses severe class imbalance in the training data. These results demonstrate that carefully designed smaller models can substantially outperform LLMs on structured NLP tasks, with important implications for resource-constrained deployments.",Diego Rossini; Lonneke van der Plas,Diego Rossini,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Linguistics, Syntax & Semantics",,binary; token level; identification; classification; token; type; level; data results demonstrate; data results; discontinuous
543-FIND,Emotion Alignment Between Text and Speech is Limited: A Cross-Modal Study,"Although expressive TTS systems aim to capture human-like emotion, little is known about how well emotional signals in text correspond to those in speech. In this short paper, we investigate the emotion alignment (Valence, Arousal, Dominance) between text and speech models. We use 8 large language models for identifying text emotions and two audio models for speech analysis, across three genres: Podcasts, Audiobooks and TED talks. Findings show that while language models perform well on emotion recognition from situational text, and the audio models performs well on speech, they show alignment on the Valence dimension only. Further, the genre of the content significantly impacts the alignment performance: audiobooks exhibit better text-audio correlation than TED talks. Our results highlight the limitations of text-based emotion expression in TTS systems: performance depends on genre, is mostly limited to Valence, and more context fails to improve emotional alignment between text and speech.",David Lindevelt; Suzan Verberne; Joost Broekens,David Lindevelt,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,,emotion; speech; text; text speech; alignment; audio models; ted; audio; tts systems; genre
548-FIND,Seeing All Sides: Multi-Perspective In-Context Learning for Subjective NLP,"Modern language models excel at factual reasoning but struggle with value diversity: the multiplicity of plausible human perspectives. Tasks such as hate speech or sexism detection expose this limitation, where human disagreement captures the diversity of perspectives that models need to account for, rather than dataset noise. In this paper, we explore whether multi-perspective in-context learning (ICL) can align large language models (LLMs) with this diversity without parameter updates. We evaluate four LLMs on five datasets across three languages (English, Arabic, Italian), considering three label-space representations (aggregated hard, disaggregated hard, and disaggregated soft) and five demonstration selection and ordering strategies. Our multi-perspective approach outperforms standard prompting on aggregated English labels, while disaggregated soft predictions better align with human judgments in Arabic and Italian datasets.These findings highlight the importance of perspective-aware LLMs for reducing bias and polarization, while also revealing the challenges of applying ICL to socially sensitive tasks. We further probe the model faithfulness using XAI, offering insights into how LLMs handle human disagreement.",Benedetta Muscato; Yue Li; Gizem Gezici; Zhixue Zhao; Fosca Giannotti,Benedetta Muscato,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,"Reasoning, Planning & Agents",multi perspective; disaggregated; perspective; human disagreement; diversity; disagreement; icl; aggregated; soft; italian
556-FIND,Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models,"Large Language Models (LLMs) are increasingly used in creative tasks. However, a formal and systematic assessment of creativity capabilities in LLMs is still missing, which limits our understanding of how capable LLMs are in generating content that is diverse and contextually relevant. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their results on the task are lower than those of two baselines that do not possess any creative abilities, highlighting its critical shortcomings. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce the Conditional Divergent Association Task (CDAT). CDAT conditions novelty on cue appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show most creativity, whereas advanced families favor appropriateness at lower novelty; We hypothesize training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We will release the dataset and code to ensure reproducibility.",Kumiko Nakajima; Jan Zuiderveld; Sandro Pezzelle,Kumiko Nakajima,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",creativity; appropriateness; novelty; divergent; creative; association task; association; families; lower; llms
559-FIND,Are Multimodal LLMs Movie Buffs?,"No. Whilst Multimodal Large Language Models (MLLMs) have been shown to perform very well on general video data, we systematically show that their performance on movies lags behind. This is surprising as MLLMs are increasingly used for movie understanding. To measure the performance of MLLMs on movies, we explore three pillars of movie mastery: movie knowledge, cinematographic knowledge, and critical analysis. Through a combination of quantitative and in-depth qualitative evaluations we identify where MLLMs show promise and, in particular, where they fail. Our findings show that in small-scale settings involving factual knowledge, MLLMs are able to outperform existing methods. However, once cinematographic and critical analysis is required, MLLMs are insufficiently able to extract meaningful information from the visual modality to be able to provide useful insights.",Carlo Bretti; Pascal Mettes; Nanne Van Noord,Carlo Bretti,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,Interpretability & Model Analysis,mllms; able; movies; critical analysis; knowledge; multimodal; able outperform; mastery; outperform existing; qualitative evaluations
564-FIND,Process Evaluation for Agentic Systems,"The significance of tasks entrusted to LLM-based research assistants (agents) and the associated societal risks are increasing each year. Agents are being explored in critical domains such as medicine, finance, law, infrastructure, and other sensitive applications that require system transparency and high user trust. The quality of these agents is typically evaluated by accuracy, sometimes extended to partial correctness. In this position paper, we argue that this focus on outcomes is insufficient as it can obscure risky agent behaviours such as skipping important steps, hallucinating tool use, relying on outdated parametric knowledge and other means of bypassing recommended processes. Our core position is that a holistic agent evaluation must include process evaluation, especially for critical applications. We conduct a small-scale study to assess the feasibility of automatic process evaluation, present a proof-of-concept compliance score, analyse use cases of bad and good behaviours, and offer recommendations for a more holistic evaluation of agents.",Milan Gritta; Debjit Paul; Gerasimos Lampouras; Jun Wang; Xiaoguang Li; Lifeng Shang,Gerasimos Lampouras,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",agents; behaviours; holistic; evaluation; position; process; agent; applications; conduct small scale; position paper argue
569-FIND,MIMIC: Multi-party Dialogue Augmentation via Speaker Stylistic Transfer,"Annotated data scarcity has long hindered progress in dialogue discourse parsing. To fill this gap, we introduce MIMIC, a framework for augmenting discourse-annotated corpora via speaker stylistic transfer using Large Language Models (LLMs). MIMIC rephrases utterances while preserving discourse coherence, using the MASK metric to identify speakers for replacement that enrich structural diversity and the MIRROR method to select substitute speakers who have experienced similar discourse interactions. Experimental results on STAC and Molweni corpora show that parsers trained with MIMIC-augmented data improve both link prediction and relation classification, with consistent gains for underrepresented discourse patterns and in low-resource scenarios.",Gaetano Cimino; Giuseppe Carenini; Vincenzo Deufemia,Gaetano Cimino,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Linguistics, Syntax & Semantics",Multilinguality & Low-Resource NLP,discourse; mimic; stylistic; speaker; speakers; corpora; dialogue; transfer; annotated; discourse coherence
571-FIND,TechING: Towards Real World Technical Image Understanding via VLMs,"Professionals working in technical domain typically hand-draw (on whitebpoard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions, however, later if they want to edit these, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but these struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this resource paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves best all round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve average F1 score of Llama 3.2 11B-instruct by 6.97x.",Tafazzul Nadeem; Bhavik Shangari; Manish Rai; Gagan Raj Gupta; Ashutosh Modi,Tafazzul Nadeem,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,images; technical; 11b; drawn; diagrams; hand; vlms; llama; instruct; hand drawn
572-FIND,Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text,"Multimodal models excel in English, supported by abundant imageвЂ“text and audioвЂ“text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely on machine translation, while advances in multilingual text modeling remain underutilized. We introduce M2M, a lightweight alignment method that learns only a few linear layersвЂ”using English text aloneвЂ”to map multilingual text embeddings into multimodal space. Despite its simplicity, M2M matches baseline performance in English (94.9% Recall@10) and achieves strong zero-shot transfer (89.5% Recall@10 averaged across 11 languages, 10 unseen) on XTD Text-to-Image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond imageвЂ“text retrieval, M2M generalizes to AudioвЂ“Text retrieval and cross-lingual Text-to-Image generation. We release code, checkpoints, and multilingual evaluation datasets (https://github.com/m2m-codebase/M2M) to facilitate further research.",Piyush Singh Pasi,Piyush Singh Pasi,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Multimodal & Speech/Audio,Multilinguality & Low-Resource NLP,text; multilingual; multimodal; recall 10; multilingual multimodal; audioвђ; imageвђ text; text retrieval; imageвђ; multilingual text
576-FIND,Do GUI Grounders Truly Understand UI Elements?,"Graphical User Interface (GUI) grounding is critical for effective GUI agents. Despite recent progress, key challenges remain: 1) existing grounding models and benchmarks are skewed toward web and mobile environments, neglecting desktop interfaces (especially windows); and 2) grounding capability is assessed using accuracy on a single ""best"" instruction per UI element. However, users can refer to a UI element in diverse valid ways -- via visual attributes, spatial relations, etc, and a capable grounding model should produce consistent outputs across such variations. Focusing on desktop environments, we introduce GUI Grounding Sensitivity Benchmark, which investigates the model sensitivity to multiple descriptions of the same UI element. We design an automatic pipeline to generate multiple valid instructions per UI element, and develop nuanced data validation methods, as frontier models even hallucinate to produce a single instruction. Evaluation of 12 models reveals they are reasonably sensitive and their performance on existing benchmarks does not reflect their true ability. Building on the insight that a given grounding model struggles more with certain instructions or relations, we introduce the GUI Grounding Diagnosis Agent, which generates challenging instructions using model feedback and iterative refinement. Our agent reports high success rate (upto 84%) in generating instructions that fail the state-of-the-art GUI grounding models.",Surgan Jandial; Yinheng Li; Justin Wagle; Kazuhito Koishida,Surgan Jandial,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Retrieval, Grounding & External Knowledge (RAG)",gui; grounding; element; instructions; valid; relations; environments; sensitivity; produce; instruction
577-FIND,Scaling Cultural Resources for Improving Generative Models,"Generative models are known to have reduced performance in different global cultural contexts and languages. While continual data updates have been known to be conducted to improve overall model performance, bolstering and evaluating this cross-cultural competence of generative AI models requires data resources to be intentionally expanded to include global contexts and languages. In this work, we construct a multi-pronged pipeline to collect and contribute culturally salient, multilingual data. We posit that such data can assess the state of the global applicability of our models and thus, in turn, help identify and improve upon cross-cultural gaps.",Hayk Stepanyan; Aishwarya Verma; Andrew Zaldivar; Rutledge Chin Feman; Erin MacMurray van Liemt; Charu Kalia; Vinodkumar Prabhakaran; Sunipa Dev,Hayk Stepanyan,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Trustworthy, Safety, Privacy & Fairness",Multilinguality & Low-Resource NLP,cultural; global; generative; generative models; cross cultural; known; resources; contexts; cultural competence; languages work
578-FIND,Ensemble Privacy Defense for Knowledge-Intensive LLMs against Membership Inference Attacks,"Retrieval-Augmented Generation (RAG) and Supervised Finetuning (SFT) have become the predominant paradigms for equipping Large Language Models (LLMs) with external knowledge for diverse, knowledge-intensive tasks. However, while such knowledge injection improves performance, it also exposes new attack surfaces. Membership Inference Attacks (MIAs), which aim to determine whether a given data sample was included in a model's training set, pose serious threats to privacy and trust in sensitive domains. To this end, we first systematically evaluate the vulnerability of RAG- and SFT-based LLMs to various MIAs. Then, to address the privacy risk, we further introduce a novel, model-agnostic defense framework, Ensemble Privacy Defense (EPD), which aggregates and evaluates the outputs of a knowledge-injected LLM, a base LLM, and a dedicated judge model to enhance resistance against MIAs. Comprehensive experiments show that, on average, EPD reduces MIA success by up to 27.8% for SFT and 526.3% for RAG compared to inference-time baseline, while maintaining answer quality.",Haowei Fu; Bo Ni; Han Xu; Kunpeng Liu; Dan Lin; Tyler Derr,Haowei Fu,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Retrieval, Grounding & External Knowledge (RAG)","Trustworthy, Safety, Privacy & Fairness",mias; privacy; defense; sft; knowledge; rag; inference attacks; membership inference attacks; membership inference; membership
581-FIND,SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents,"Large language model (LLM) based search agents iteratively generate queries, retrieve external information, and reason to answer open-domain questions. While researchers have primarily focused on improving their utility, their safety behaviors remain underexplored. In this paper, we first evaluate search agents using red-teaming datasets and find that they are more likely to produce harmful outputs than base LLMs. For example, when asked ``How can I track someone's location without their consent?'', a base model refuses, whereas a search agent designed to retrieve and cite sources may lower its refusal threshold, fetch documents (e.g., court cases), and, once appended, synthesize them into an informative yet unsafe summary. We further show that utility-oriented fine-tuning intensifies this risk, motivating joint alignment of safety and utility. We present SafeSearch, a multi-objective reinforcement learning approach that couples a final-output safety/utility reward with a novel query-level shaping term that penalizes unsafe queries and rewards safe ones. Experiments show that SafeSearch reduces agent harmfulness by over 80% across three red-teaming datasets while producing safe, helpful responses, and matches the QA performance of a utility-only finetuned agent; further analyses confirm the effectiveness of the query-level reward in jointly improving safety and utility.",Qiusi Zhan; Angeline Budiman-Chan; Abdelrahman Zayed; Xingzhi Guo; Daniel Kang; Joo-Kyung Kim,Joo-Kyung Kim,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Reasoning, Planning & Agents",utility; safety utility; search agents; safety; search; teaming; red teaming; red; unsafe; agent
585-FIND,SAGE : A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn Agent Evaluation,"Evaluating multi-turn interactive agents is challenging due to the need for human assessment. Evaluation with simulated users has been introduced as an alternative, however existing approaches typically model generic users and overlook the domain-specific principles required to capture realistic behavior. We propose SAGE, a novel user Simulation framework for multi-turn AGent Evaluation that integrates knowledge from business contexts. SAGE incorporates top-down knowledge rooted in business logic, such as ideal customer profiles, grounding user behavior in realistic customer personas. We further integrate bottom-up knowledge taken from business agent infrastructure (e.g., product catalogs, FAQs, and knowledge bases), allowing the simulator to generate interactions that reflect users' information needs and expectations in a companyвЂ™s target market. Through empirical evaluation, we find that this approach produces interactions that are more realistic and diverse, while also identifying up to 33% more agent errors, highlighting its effectiveness as an evaluation tool to support bug-finding and iterative agent improvement.",Ryan Shea; Yunan Lu; Liang Qiu; Zhou Yu,Ryan Shea,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Dialogue, Conversational & Interactive NLP","Reasoning, Planning & Agents",sage; agent; business; multi turn; turn; knowledge; multi turn agent; turn agent; agent evaluation; realistic
592-FIND,Better as Generators Than Classifiers: Leveraging LLMs and Synthetic Data for Low-Resource Multilingual Classification,"Large Language Models (LLMs) have demonstrated remarkable multilingual capabilities, making them promising tools in both high- and low-resource languages. One particularly valuable use case is generating synthetic samples that can be used to train smaller models in low-resource scenarios where human-labelled data is scarce. In this work, we investigate whether these synthetic data generation capabilities can serve as a form of distillation, producing smaller models that perform on par with or even better than massive LLMs across languages and tasks. To this end, we use a state-of-the-art multilingual LLM to generate synthetic datasets covering 11 languages and 4 classification tasks. These datasets are then used to train smaller models via fine-tuning or instruction tuning, or as synthetic in-context examples for compact LLMs. Our experiments show that even small amounts of synthetic data enable smaller models to outperform the large generator itself, particularly in low-resource languages. Overall, the results suggest that LLMs are best utilised as generators (teachers) rather than classifiers, producing data that empowers smaller and more efficient multilingual models.",Branislav Pecher; Jan Cegin; Robert Belanec; Ivan Srba; Jakub Simko; Maria Bielikova,Branislav Pecher,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multilinguality & Low-Resource NLP,Summarization & Generation,synthetic; smaller; smaller models; low resource; synthetic data; train smaller models; train smaller; multilingual; resource; low
596-FIND,Dialogue is Better Than Monologue: Instructing Meidcal LLMs via Strategic Conversations,"In real clinical practice, clinicians must sift through noisy and often conflicting information, progressively gathering and sequencing evidence before reaching conclusions. However, existing tuning methods for medical AI models are typically $\texttt{monologue-based}$ вЂ” that is, models are fine-tuned on static question-answering (QA) tasks or medical articles, which fail to reflect the interactive and iterative nature of clinical reasoning. To bridge this gap, we introduce $\texttt{muddymaze}$, a benchmark designed to expose the limitations of current $\texttt{monologue-based}$ tuning, and construct a large $\texttt{dialogue dataset}$ of 22.2k doctorвЂ“patient interactions that capture stepwise diagnostic reasoning validated by medical experts. Building on those, we propose $\texttt{dialogue-tuning}$, a new fine-tuning paradigm that captures the internal reasoning dynamics unfolding across interactions. To assess the effectiveness of our approach, we evaluated $\textit{dialogue-tuned}$ models on $\texttt{muddymaze}$, where they outperform $\textit{monologue-tuned}$ baselines (e.g., MedQA) by +16.1% in one-round and +4.1% in multi-round evidence ranking, while maintaining or even improving accuracy on standard medical QA benchmarks (e.g., PubMedQA). These results indicate that $\texttt{dialogue-tuning}$ not only enhances reasoning robustness and evidence integration but also preserves the factual precision of traditional QA performance.",Zijie Liu; Xinyu Zhao; Jie Peng; Jinhao Duan; Zhuangdi Zhu; Qingyu Chen; Kaidi Xu; Xia Hu; Tianlong Chen,Zijie Liu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Dialogue, Conversational & Interactive NLP",texttt; dialogue; medical; tuning; evidence; round; tuned; textit; reasoning; clinical
598-FIND,DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG) that systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4вЂ“10% over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18% absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3%.",Saadat Hasan Khan; Spencer Hong; Jingyu Wu; Kevin Lybarger; Youbing Yin; Erin Babinsky; Daben Liu,Saadat Hasan Khan,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",,rag; reasoning intensive; diversity; retrieval; intensive; cosine similarity; retrieval augmented generation; augmented generation; cosine; retrieval augmented
601-FIND,Dimension-First Evaluation of Voice Assistants: Human Chain-of-Thought and Structured Judges,"Evaluating spoken assistants requires judging \emph{content} (C), \emph{voice quality} (VQ), and \emph{paralinguistics} (EQ). Most benchmarks collapse these dimensions into a single overall preference. We show this creates artifacts (forced winners and untyped ties) that overweight textual content and make overall accuracy ""hackable"" by transcript-only judges. To address this, we re-annotate two public datasets using a Human Chain-of-Thought (HCoT) protocol that elicits \emph{dimension-first} judgments with \emph{typed ties} (\emph{both-good} vs. \emph{both-bad}). This yields reliable, diagnostic labels validated under a protocol aligned with ITU-T guidance on separate dimension ratings. Building on HCoT, we propose \emph{JSON-Judge}, a training-free, two-stage evaluator: (i) assemble a compact JSON blueprint of inexpensive, off-the-shelf signals for C/VQ/EQ; (ii) prompt an LLM for per-dimension decisions, which a deterministic rule maps to the overall label. Across SpeakBench and S2S-Arena, JSON-Judge attains higher overall agreement with HCoT than transcript-only and audio-LLM judges and delivers stronger EQ fidelity, while being \emph{significantly cheaper than audio-LLM judges}. We will release the HCoT re-annotations and the JSON blueprints/prompts, providing a scalable, interpretable, and human-centric framework for evaluating voice assistants.",Arjun Chandra; Kevin Miller; Venkatesh Ravichandran; Constantinos Papayiannis; Venkatesh Saligrama,Kevin Miller,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,emph; json; judges; dimension; voice; overall; assistants; voice assistants; transcript; llm judges
605-FIND,Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention,"Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself---improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for logical reasoning. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-aware Intervention (AAI), a method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the modelвЂ™s reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance on three well-known benchmarks, including ProofWriter, PrOntoQA, and Logical Deduction, across model architectures and sizes while introducing no additional computational overhead.",Phuong Minh Nguyen; Dang Huu-Tien; Naoya Inoue,Nguyen Minh Phuong,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Dialogue, Conversational & Interactive NLP","Reasoning, Planning & Agents",logical; logical reasoning; interactive; attention; reasoning; end; end end framework; external resources; end framework; external
608-FIND,"Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models","Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly, over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.",Michael R. Metel; Yufei Cui; Boxing Chen; Prasanna Parthasarathi,Michael R. Metel,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents",,test time scaling; model accuracy; sequential; time scaling; scaling; accuracy; test time; length; reasoning; large reasoning
610-FIND,Defeating Cerberus: Privacy-Leakage Mitigation in Vision Language Models,"Vision Language Models (VLMs) have demonstrated remarkable capabilities in processing multimodal data, but their advanced abilities also raise significant privacy concerns, particularly regarding Personally Identifiable Information (PII) leakage. While relevant research has been conducted on single-modal language models to some extent, the vulnerabilities in the multimodal setting have yet to be fully investigated. Our work assesses these emerging risks and introduces a concept-guided mitigation approach. By identifying and modifying the model's internal states associated with PII-related content, our method guides VLMs to refuse PII-sensitive tasks effectively and efficiently, without requiring re-training or fine-tuning. We also address the current lack of multimodal PII datasets by constructing various ones that simulate real-world scenarios. Experimental results demonstrate the method can achieve on average 93.3% refusal rate for various PII-related tasks with minimal impact on unrelated model performances. We further examine the mitigation's performance under various conditions to show the adaptability of our proposed method.",Boyang Zhang; Istemi Ekin Akkus; Ruichuan Chen; Alice Dethise; Klaus Satzke; Ivica Rimac; Yang Zhang,Boyang Zhang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Multimodal & Speech/Audio,pii; mitigation; leakage; various; multimodal; privacy; vlms; vision language models; related; vision language
611-FIND,TruthTrap: A Bilingual Benchmark for Evaluating Factually Correct Yet Misleading Information in Question Answering,"Large Language Models (LLMs) are increasingly used to answer factual, information-seeking questions (ISQs). While prior work often focuses on false, misleading information, little attention has been paid to true but strategically persuasive content that can derail a modelвЂ™s reasoning. To address this gap, we introduce a new evaluation dataset, TruthTrap, in two languages, i.e., English and Farsi, on Iran-related ISQs, each paired with a correct explanation and a persuasive-yet-misleading true hint. We then evaluate nine diverse LLMs (spanning proprietary and open-source systems) via factuality classification and multiple-choice QA tasks, finding that accuracy drops by 25%, on average, when models encounter these misleading yet factual hints. Also, the models' predictions match the hint-aligned options up to 77 percent of the time. Notably, models often misjudge such hints in isolation yet still integrate them into final answers. Our results highlight a significant limitation in LLM outputs, underscoring the importance of robust fact-verification and emphasizing real-world risks posed by partial truths in domains like social media, education, and policy-making.",Mohammadamin Shafiei; Hamidreza Saffari; Mohammad Taher Pilehvar; Alessandro Raganato,Mohammadamin Shafiei,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Interpretability & Model Analysis,misleading; hints; hint; persuasive; true; correct; information; factual; reasoning address gap; little attention paid
613-FIND,FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression,"Large Language Models (LLMs) have enabled remarkable progress in natural language processing, yet their high computational and memory demands pose challenges for deployment in resource-constrained environments. Although recent low-rank decomposition methods offer a promising path for structural compression, they often suffer from accuracy degradation, expensive calibration procedures, and result in inefficient model architectures that hinder real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and accurate, training-free structural compression method based on fine-grained low-rank transformations in the activation space. Specifically, we reduce the hidden dimension by transforming the weights using truncated eigenvectors computed via head-wise Principal Component Analysis, and employ a greedy budget redistribution strategy to adaptively allocate ranks across decoders. FLAT-LLM achieves efficient and effective weight compression without recovery fine-tuning, which could complete the calibration within a few minutes. Evaluated across 5 models and 11 datasets, FLAT-LLM outperforms structural pruning baselines in generalization and downstream performance, while delivering inference speedups over decomposition-based methods.",Jiayi Tian; Ryan Solgi; Jinming Lu; Yifan Yang; Hai Li; Zheng Zhang,Jiayi Tian,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",,flat; compression; low rank; rank; speedups; structural; activation space; decomposition; calibration; low
615-FIND,Negative Sampling Techniques in Dense Retrieval: A Survey,"Information Retrieval (IR) is fundamental to many modern NLP applications. The rise of dense retrieval (DR), using neural networks to learn semantic vector representations, has significantly advanced IR performance. Central to training effective dense retrievers through contrastive learning is the selection of informative negative samples. Synthesizing 35 seminal papers, this survey provides a comprehensive and up-to-date overview of negative sampling techniques in dense IR. Our unique contribution is the focus on modern NLP applications and the inclusion of recent Large Language Model (LLM)-driven methods, an area absent in prior reviews. We propose a taxonomy that categorizes techniques, including random, static/dynamically mined, and synthetic datasets. We then analyze these approaches with respect to trade-offs between effectiveness, computational cost, and implementation difficulty. The survey concludes by outlining current challenges and promising future directions for the use of LLM-generated synthetic data.",Laurin Wischounig; Abdelrahman Abdallah; Adam Jatowt,Abdelrahman Abdallah,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",,dense; negative; survey; nlp applications; dense retrieval; techniques; sampling; retrieval; modern; synthetic
619-FIND,Multi-Agent Procedural Graph Extraction with Structural and Logical Refinement,"Automatically extracting workflows as procedural graphs from natural language is a promising yet underexplored task that requires ensuring both structural validity and logical alignment. Recent advances in large language models (LLMs) show potential for graph extraction, but often yield ill-formed structures or misinterpret logical constructs such as gateways. We introduce \model{}, a multi-agent framework that treats procedural graph extraction as a multi-round reasoning process with structural and logical refinement agents. The framework operates in three iterative stages: (1) an LLM-based graph extraction phase, (2) a structural feedback phase where a simulation agent diagnoses and explains structural issues, and (3) a logical feedback phase where a semantic agent aligns semantics between flow logic and linguistic cues in the source text. Important feedback is prioritized and expressed in natural language, which is injected into the next-round prompt, enabling interpretable and controllable refinement. This modular design allows agents to target distinct error types without supervision or parameter updates. Experiments demonstrate that \model{} achieves substantial improvements in both structural correctness and logical consistency over strong baselines.",Wangyang Ying; Yanchi Liu; Xujiang Zhao; Wei Cheng; Zhengzhang Chen; Wenchao Yu; Yanjie Fu; Haifeng Chen,Yanchi Liu,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents","Linguistics, Syntax & Semantics",logical; graph extraction; structural; procedural; phase; extraction; graph; agent; refinement; feedback
626-FIND,MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction,"Implicit Attribute Value Extraction (AVE) is essential for accurately representing products in e-commerce, as it infers lantent attributes from multimodal data. Despite advances in multimodal large language models (MLLMs), implicit AVE remains challenging due to the complexity of multidimensional data and gaps in vision-text understanding. In this work, we introduce MADIAVE, a multi-agent de- bate framework that employs multiple MLLM agents to iteratively refine inferences. Through a series of debate rounds, agents verify and up- date each otherвЂ™s responses, thereby improving inference performance and robustness. Experi- ments on the ImplicitAVE dataset demonstrate that even a few rounds of debate significantly boost accuracy, especially for attributes with initially low performance. We systematically evaluate various debate configurations, includ- ing identical or different MLLM agents, and analyze how debate rounds affect convergence dynamics. Our findings highlight the poten- tial of multi-agent debate strategies to address the limitations of single-agent approaches and offer a scalable solution for implicit AVE in multimodal e-commerce.",Wei-Chieh Huang; Cornelia Caragea,Wei-Chieh Huang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Reasoning, Planning & Agents",debate; implicit; rounds; agent; multi agent; multi agent debate; agent debate; commerce; mllm; agents
627-FIND,ElectoralCheck: Benchmarking LLM Political Stances on Election Topics,"Recent studies have demonstrated that large language models (LLMs) can generate content that is often more persuasive than human-writings, leading to increased Human-AI interactions. Individuals often turn to LLMs seeking opinions on political matters or even engage them in debates on various political topics. Given their persuasive prowess and vast knowledge base, LLMs hold the potential to sway human beliefs, particularly if they harbor certain political inclinations. Hence, we propose the (\textbf{ElectoralCheck}) вЂ” a novel resource designed to assess the alignment of LLMs with respect to election-related topics. \textit{ElectoralCheck} comprises arguments on 50 topics related to Indian elections and 32 topics related to US elections, presenting both proponent and opponent stances. We evaluate 14 LLMs from 7 different families on \textit{ElectoralCheck} dataset in an open-ended generation setting. Our findings demonstrate significant cross-cultural stance variations: LLMs adopt pro-market orientations for Indian economic issues while favoring anti-market, pro-welfare positions in the U.S. context. Similarly, they support strict law enforcement in India but oppose it in the U.S., suggesting embedded cultural or geopolitical biases. Across both contexts, LLMs consistently exhibit pro-immigration, pro-social justice, and pro-environmental stances, with unanimous support for issues like women's rights and LGBTQIA+ marriage equality. Moreover, our evaluation reveals that while many LLMs tend to maintain consistent stances whether neutral, moderate, or extreme, certain topics more readily provoke extreme positions. \ {\color{red} \textbf{Disclaimer}: \textit{This paper contains discriminatory content that may be disturbing to some readers.}}",Prince Jha; Konika Mandal; Arkadeep Acharya; Sriparna Saha; Sandipan Dandapat,Prince Jha,Find-Presentation,Virtual,ZOOM,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness","Retrieval, Grounding & External Knowledge (RAG)",topics; pro; stances; political; textit; llms; elections; extreme; indian; related
628-FIND,DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router,"Large Language Models (LLMs) excel at many reasoning tasks but struggle with knowledge-intensive queries due to their inability to dynamically access up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, enabling LLMs to ground their responses in external sources. However, existing RAG methods lack fine-grained control over both the query and source sides, often resulting in noisy retrieval and shallow reasoning. In this work, we introduce \textit{\textbf{DeepSieve}}, an agentic RAG framework that incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve decomposes complex queries into structured sub-questions and recursively routes each to the most suitable knowledge source, filtering irrelevant information through a multi-stage distillation process. Our design emphasizes modularity, transparency, and adaptability, leveraging recent advances in agentic system design. Experiments on multi-hop QA tasks across heterogeneous sources demonstrate improved reasoning depth, retrieval precision, and interpretability over conventional RAG approaches.",Minghao Guo; Qingcheng Zeng; Xujiang Zhao; Yanchi Liu; Wenchao Yu; Mengnan Du; Haifeng Chen; Wei Cheng,Wei Cheng,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",Interpretability & Model Analysis,rag; llm knowledge; router; knowledge; information; retrieval; agentic; sources; queries; design
629-FIND,Analyzing Instruction Optimization in LLM-based Pipelines for Tabular Fact Verification,"Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization for tabular fact verification, a task that requires numerical, compositional, and schema-grounded reasoning over structured data. We evaluate four prompting paradigms, direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution, across three benchmarks (TabFact, PubHealthTab, SciTab) and two model families. Using the DSPy framework, we study three optimizers: COPRO, MiPROv2, and SIMBA. Instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for tool-augmented agents, particularly at larger model scales. Behavioral analyzes reveal that SIMBA encourages more direct reasoning paths by applying heuristics, which enhances numerical comparison in CoT reasoning and helps avoid unnecessary tool calls in ReAct agents. Comparing different pipelines, CoT remains effective for tabular fact checking, especially with smaller models. ReAct agent built with larger models can achieve competitive performance but requires careful instruction optimization.",Xiaotang Du; Giwon Hong; Wai-Chung Kwan; Rohit Saxena; Ivan Titov; Pasquale Minervini; Emily Allaway,Xiaotang Du,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents",,simba; cot; react; instruction; optimization; tabular; fact; verification; fact verification; numerical
630-FIND,XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark,"Recent advances in audio generation led to an increasing number of deepfakes, making the general public more vulnerable to financial scams, identity theft, and misinformation. Audio deepfake detectors promise to alleviate this issue, with many recent studies reporting accuracy rates close to 99%. However, these methods are typically tested in an in-domain setup, where the deepfake samples from the training and test sets are produced by the same generative models. To this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In our novel dataset, the speakers, the generative methods, and the real audio sources are distinct across training and test splits. This leads to a challenging cross-domain evaluation setup, where audio deepfake detectors can be tested ""in the wild"". Our in-domain and cross-domain experiments indicate a clear disparity between the in-domain performance of deepfake detectors, which is usually as high as 100%, and the cross-domain performance of the same models, which is sometimes similar to random chance. Our benchmark highlights the need for the development of robust audio deepfake detectors, which maintain their generalization capacity across different languages, speakers, generative methods, and data sources. Our benchmark is publicly released at https://anonymous.4open.science/r/XMAD-Bench-detection/.",Ioan-Paul Ciobanu; Andrei-Iulian HГ®ji; Nicolae Catalin Ristea; Paul Irofti; Cristian Rusu; Radu Tudor Ionescu,Radu Tudor,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",deepfake; audio deepfake; audio; cross domain; domain; detectors; cross; bench; domain performance; generative methods
636-FIND,CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models,"We introduce CLEAR-3K, a dataset of 3,008 assertion-reasoning questions designed to evaluate whether language models can determine if one statement causally explains another. Each question presents an assertion-reason pair and challenge language models to distinguish between semantic relatedness and genuine causal explanatory relationships. Through comprehensive evaluation of 21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we identify two fundamental findings. First, language models frequently confuse semantic similarity with causality, relying on lexical and semantic overlap instead of inferring actual causal explanatory relationships. Second, as parameter size increases, models tend to shift from being overly skeptical about causal relationships to being excessively permissive in accepting them. Despite this shift, performance measured by the Matthews Correlation Coefficient plateaus at just 0.55, even for the best-performing models. Hence, CLEAR-3K provides a crucial benchmark for developing and evaluating causal explanatory reasoning in language models, which is an essential capability for applications that require accurate assessment of causal relationships.",Naiming Liu; Richard Baraniuk; Shashank Sonkar,Naiming Liu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",causal; explanatory; relationships; clear; assertion; causal relationships; shift; semantic; language models; language
637-FIND,Imbalanced Gradients in RL Post-Training of Multi-Task LLMs,"Multi-task post-training of large language models (LLMs) is typically performed by mixing datasets from different tasks and optimizing them jointly. This approach implicitly assumes that all tasks contribute gradients of similar magnitudes; when this assumption fails, optimization becomes biased toward large-gradient tasks. In this paper, however, we show that this assumption fails in RL post-training: certain tasks produce significantly larger gradients, thus biasing updates toward them. Such gradient imbalance would be justified only if larger gradients implied larger learning gains on the tasks (i.e., larger performance improvements)---but we find this is not true. Large-gradient tasks can achieve similar or even much lower learning gains than small-gradient ones. Further studies reveal that these gradient imbalances cannot be explained by typical training statistics such as training rewards or advantages, suggesting that they are intrinsic to the tasks themselves. This cautions against naive dataset mixing and calls for future work on principled gradient-level corrections for LLMs.",Runzhe Wu; Ankur Samanta; Ayush Jain; Scott Fujimoto; Jeongyeol Kwon; Ben Kretzu; Youliang Yu; Kaveh Hassani; Boris Vidolov; Yonathan Efroni,Runzhe Wu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,,gradient; gradients; larger; post training; tasks; post; training; mixing; multi task; assumption
639-FIND,BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation,"Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design.",Bo Yuan; Yun Zhou; Zhichao Xu; Kiran Ramnath; Aosong Feng; Balasubramaniam Srinivasan,Bo Yuan,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents",Summarization & Generation,workflow; workflow generation; refiner; posterior; cast; generation; bayesian; percentage points; percentage; workflows
642-FIND,HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning,"Haptic captioning is the task of generating natural language descriptions from haptic signals, such as vibrations, for use in virtual reality and rehabilitation applications. While previous multimodal research has focused primarily on vision and audio, haptic feedback for the sense of touch remain underexplored. To address this gap, we formalize the haptic captioning task and propose HapticLLaMA, a multimodal sensory language model that interprets vibration signals into descriptions in a given sensory, emotional, or associative category. We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units, enabling their integration with the LLaMA model. HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation, and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We assess HapticLLaMAвЂ™s captioning performance using both automated n-gram metrics and human evaluation. HapticLLaMA demonstrates strong capability in interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a BLEU-4 score of 32.06, respectively. Furthermore, over 64% of the generated captions received human ratings above 3.5 on a 7-point scale, with RLHF yielding a 13% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception. These findings highlight the potential of large language models to process and adapt to sensory data.",Guimin Hu; Daniel Hershcovich; Hasti Seifi,Hasti Seifi,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,captioning; signals; rlhf; tokenizer; descriptions; multimodal; human; llama; feedback; score
643-FIND,Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation,"While large language models (LLMs) have achieved remarkable success in providing trustworthy responses for knowledge-intensive tasks, they still face critical limitations such as hallucinations and outdated knowledge. To address these issues, the retrieval-augmented generation (RAG) framework enhances LLMs with access to external knowledge via a retriever, enabling more accurate and real-time outputs about the latest events. However, this integration brings new security vulnerabilities: the risk that malicious content in the external database can be retrieved and used to manipulate model outputs. Although prior work has explored attacks on RAG systems, existing approaches either rely heavily on access to the retriever or fail to jointly consider both retrieval and generation stages, limiting their effectiveness, particularly in black-box scenarios. To overcome these limitations, we propose Token-level Precise Attack on the RAG (TPARAG), a novel framework that targets both white-box and black-box RAG systems. TPARAG leverages a lightweight white-box LLM as an attacker to generate and iteratively optimize malicious passages at the token level, ensuring both retrievability and high attack success in generation. Extensive experiments on open-domain QA datasets demonstrate that TPARAG consistently outperforms previous approaches in retrieval-stage and end-to-end attack effectiveness. These results further reveal critical vulnerabilities in RAG pipelines and offer new insights into improving their robustness.",Zizhong Li; Haopeng Zhang; Jiawei Zhang,Zizhong Li,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)","Trustworthy, Safety, Privacy & Fairness",rag; box; attack; token level; white; white box; malicious; retriever; generation; token
645-FIND,PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference,"KV caching significantly improves the efficiency of Large Language Model (LLM) inference by storing attention states from previously processed tokens, enabling faster generation of subsequent tokens. However, as sequence length increases, the KV cache quickly becomes a major memory bottleneck. To address this, we propose PagedEviction, a novel fine-grained, structured KV cache pruning strategy that enhances the memory efficiency of vLLMвЂ™s PagedAttention. Unlike existing approaches that rely on attention-based token importance or evict tokens across different vLLM pages, PagedEviction introduces an efficient block-wise eviction algorithm tailored for paged memory layouts. Our method integrates seamlessly with PagedAttention without requiring any modifications to its CUDA attention kernels. We evaluate PagedEviction across Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models on the LongBench benchmark suite, demonstrating improved memory usage with better accuracy than baselines on long context tasks.",Krishna Teja Chitty-Venkata; Jie Ye; Siddhisanket Raskar; Anthony Kougkas; Xian Sun; Murali Emani; Venkatram Vishwanath; Bogdan Nicolae,Bogdan Nicolae,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Efficiency, Scaling & NLP Systems",,cache; kv cache; memory; instruct; block wise; llama; attention; tokens; block; pruning
647-FIND,SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity,"Despite significant progress in natural image editing with state-of-the-art MLLMs, compositional layout and content editing for structured visual domains (e.g., posters, websites) remains underexplored. In this work, we introduce SMART-EDITOR, a multi-agent framework for compositional editing for structured images like posters or websites. Unlike prior models that focus on isolated local edits, SMART-EDITOR maintains global coherence through two complementary strategies: Reward-Refine, an inference-time reward-guided refinement method, and RewardDPO, a training-time preference optimization approach leveraging reward-aligned layout pairs. To evaluate performance, we introduce SMARTEdit-Bench, a benchmark of cascading multi-step edit instructions that are implicit in nature yet require layout and semantic-consistency preserving reasoning about edit order to preserve spatial and semantic consistency. Both automatic and human evaluations confirm the value of reward-guided planning in producing semantically consistent and visually coherent edits, beyond what single-shot VLMs can generate.",Ishani Mondal; Meera Bharadwaj; Ayush Roy; Aparna Garimella; Jordan Lee Boyd-Graber,Ishani Mondal,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",editing; editor; smart; reward; layout; semantic consistency; reward guided; edit; edits; multi agent framework
652-FIND,ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations,"As language models evolve into autonomous agents that act and communicate on behalf of users, ensuring safety in multi-agent ecosystems becomes a central challenge. Interactions between personal assistants and external service providers expose a core tension between utility and protection: effective collaboration requires information sharing, yet every exchange creates new attack surfaces. We introduce ConVerse, a dynamic benchmark for evaluating privacy and security risks in agentвЂ“agent interactions. ConVerse spans three practical domains (travel, real estate, insurance) with 12 user personas and over 864 contextually grounded attacks (611 privacy, 253 security). Unlike prior single-agent settings, it models autonomous, multi-turn agent-to-agent conversations where malicious requests are embedded within plausible discourse. Privacy is tested through a three-tier taxonomy assessing abstraction quality, while security attacks target tool use and preference manipulation. Evaluating seven state-of-the-art models reveals persistent vulnerabilitiesвЂ”privacy attacks succeed in up to 88% of cases and security breaches in up to 60%вЂ”with stronger models leaking more. By unifying privacy and security within interactive multi-agent contexts, ConVerse reframes safety as an emergent property of communication.",Amr Gomaa; Ahmed Salem; Sahar Abdelnabi,Amr Gomaa,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Reasoning, Planning & Agents",agent; converse; security; privacy; privacy security; attacks; autonomous; safety; conversations; multi agent
658-FIND,SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning,"The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ, a generic red-teaming framework for arbitrary \textit{black-box} LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover diverse risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models.",Kaiwen Zhou; Ahmed Elgohary; A S M Iftekhar; Amin Saied,Kaiwen Zhou,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Reasoning, Planning & Agents",red; teaming; red teaming; trajectories; llm agents; risk; distilled; seed; structured reasoning; diverse
660-FIND,"Who You Are, What You Say: Intra- and Inter- Context Personality for Emotion Recognition in Conversation","Emotion recognition in conversation (ERC) requires understanding both contextual dependencies and speaker-specific cues. Existing approaches often treat conversation context as a single representation or encode speaker identity shallowly, limiting their ability to capture fine-grained emotional dynamics. We propose PERC, a personality-aware ERC framework that (1) segregates conversational context into intra- and inter-speaker components, (2) models static or dynamic personality traits to represent stable and evolving speaker dispositions, and (3) performs contrastive cross-alignment between intra-intra and inter-inter representations to enforce contextual and personality consistency. Experiments on three ERC benchmarks show that PERC achieves new state-of-the-art performance, improving weighted F1 by up to 2.74%, demonstrating the effectiveness of integrating context segregation, personality modeling, and contrastive alignment for emotion reasoning in dialogue.",Tazeek Bin Abdur Rakib; Lay-Ki Soon; Wern Han Lim,Tazeek Bin Abdur Rakib,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP",,personality; intra; speaker; inter; conversation; emotion; emotion recognition; context; contextual; contrastive
662-FIND,DRIVINGVQA: A Dataset for Interleaved Visual Chain-of-Thought in Real-World Driving Scenarios,"While chain-of-thought (CoT) prompting improves reasoning in large language models, its effectiveness in vision-language models (VLMs) remains limited due to over-reliance on textual cues and memorized knowledge. To investigate the visual reasoning capabilities of VLMs in complex real-world scenarios, we introduce DrivingVQA, a visual question answering dataset derived from driving theory exams, which contains 3,931 multiple-choice problems with expert-written explanations and grounded entities relevant to the reasoning process. Leveraging this dataset, we explore the benefits of incorporating entity-related information, such as entity names, spatial coordinates, and visual content, through supervised fine-tuning to enhance the modelХs reasoning abilities. Our experiments demonstrate that interleaving textual explanations with visual tokens extracted from entities relevant to the question improves answer accuracy by 3.1% and reasoning accuracy by 4.6% over vanilla CoT prompting. Furthermore, we demonstrate that this retrieval-based approach effectively scales to the larger A-OKVQA reasoning dataset by leveraging automatically generated pseudo-labels, outperforming CoT prompting.",Charles CorbiЏre; Simon Roburin; Syrielle Montariol; Antoine Bosselut; Alexandre Alahi,Syrielle Montariol,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Reasoning, Planning & Agents","Retrieval, Grounding & External Knowledge (RAG)",cot prompting; visual; cot; reasoning; driving; prompting; dataset; explanations; entities; vlms
663-FIND,Steerable Agentic Data Generation for Deep Search with Execution Feedback,"Deep search, which aims to answer complicated questions requiring searching for and reasoning across multiple documents, is becoming one of the most important agentic use-cases. Yet, it is not scalable to leverage human annotations to collect deep search data due to long and complex exploration trajectories. We propose an agentic pipeline to automatically generate high-quality deep search question answer pairs for a given corpus. Our initial data generator agent is able to produce QA pairs that need multiple rounds of search based on an input document. To further improve the steer-ability of our data generation framework, we 1) add a natural language prompt to specify a level of difficulty; 2) incorporate the execution traces from a task execution agent as feedback to correct undesirable data. Our intrinsic evaluation shows our framework significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates that compared to training with existing large-scale training data, our data improved the trained search agent with >15% gains across in-domain and out-of-domain complex question answering datasets on two model sizes. Analysis shows that our pipeline generates questions that require diverse reasoning strategies.",Fangyuan Xu; Rujun Han; Yanfei Chen; Zifeng Wang; I-Hung Hsu; Jun Yan; Vishy Tirumalashetty; Eunsol Choi; Tomas Pfister; Chen-Yu Lee,Rujun Han,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents",Summarization & Generation,search; deep; execution; agentic; agent; data generation; difficulty; pairs; feedback; shows
664-FIND,Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation,"Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently shown strong capacity to capture complex distributions in this setting, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations, and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporalвЂ“relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art improvements.",Yanglei Gan; Peng He; Yuxiang Cai; Run Lin; Guanyu Zhou; Qiao Liu,Yanglei Gan,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)","LLM Evaluation, Benchmarks & Metrics",tkg; negative; diffusion; extrapolation; temporal; temporal knowledge graph; temporal knowledge; conditioned; knowledge graph; graph
667-FIND,DS$^2$-Instruct: Domain-Specific Data Synthesis for Large Language Models Instruction Tuning,"Adapting Large Language Models (LLMs) to new domains requires high-quality instruction datasets. Creating these datasets through human annotation is expensive. Current data synthesis methods focus on general tasks and fail to capture domain-specific terminology and reasoning. To address this, we introduce DS -Instruct, a zero-shot framework that generates domain-specific instruction datasets without human supervision. Our approach first generates task-informed keywords to ensure comprehensive domain coverage. It then creates diverse instructions by pairing these keywords with different cognitive levels from Bloom's Taxonomy. Finally, it uses self-consistency validation to filter the outputs and ensure data quality. We apply this framework to generate datasets across seven challenging domains, such as mathematics, finance, and logical reasoning. Thorough evaluations demonstrate that models fine-tuned on our data significantly outperform those obtained from existing methods.",Ruiyao Xu; Noelle I. Samia; Han Liu,Ruiyao Xu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Machine Translation,datasets human; data synthesis; keywords; domain specific; domain; instruction; synthesis; instruct; datasets; ensure
669-FIND,DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive Dashboards,"Dashboards are powerful visualization tools for data-driven decision-making, integrating multiple interactive views that allow users to explore, filter, and navigate data. Unlike static charts, dashboards support rich interactivity, which is essential for uncovering insights in real-world analytical workflows. However, existing question-answering benchmarks for data visualizations largely overlook this interactivity, focusing instead on static charts. This limitation severely constrains their ability to evaluate the capabilities of modern multimodal agents designed for GUI-based reasoning. To address this gap, we introduce DashboardQA, the first benchmark explicitly designed to assess how vision-language GUI agents comprehend and interact with real-world dashboards. The benchmark includes 292 tasks on 112 interactive dashboards, encompassing 405 question answer pairs overall. These questions span five categories: multiple-choice, factoid, hypothetical, multi-dashboard, and conversational. By assessing a variety of leading closed- and open-source GUI agents, our analysis reveals their key limitations, particularly in grounding dashboard elements, planning interaction trajectories, and performing reasoning. Our findings indicate that interactive dashboard reasoning is a challenging task overall for all the VLMs evaluated. Even the top-performing agents struggle; for instance, the best agent based on Gemini-Pro-2.5 achieves only 38.69% accuracy, while the OpenAI CUA agent reaches just 22.69%, demonstrating the benchmark's significant difficulty. We release DashboardQA at ..",Aaryaman Kartha; Ahmed Masry; Mohammed Saidul Islam; Thinh Lang; Shadikur Rahman; Ridwan Mahbub; Mizanur Rahman; Mahir Ahmed; Md Rizwan Parvez; Enamul Hoque; Shafiq Joty,Ahmed Masry,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Reasoning, Planning & Agents","Dialogue, Conversational & Interactive NLP",dashboard; interactive; gui; agents; multimodal agents; gui agents; charts; question; performing; static
670-FIND,Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?,"Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced by reinforcement learning. However, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical for safe deployment in real-world applications. To this end, we explore uncertainty quantification (UQ) of reasoning models in this work. We ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humansвЂ™ innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (IUQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). We conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models.",Zhiting Mei; Christina Zhang; Tenny Yin; Justin Lidard; Ola Sho; Anirudha Majumdar,Tenny Yin,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",reasoning models; reasoning; calibrated; calibration; uncertainty; uncertainty quantification; know; ask; quantification; sota
673-FIND,AfriMMT-EA: Multi-domain Machine Translation for Low-Resource East African Languages,"Despite remarkable progress in multilingual machine translation (MT), the majority of African---especially East African---languages remain significantly underrepresented both in benchmark datasets and state-of-the-art (SOTA) MT models. This persistent exclusion from mainstream technologies not only limits equitable access, but constrains the development of tools that accurately reflect the regionвЂ™s linguistic and cultural diversity. Recent advances in open-source large language models have demonstrated strong multilingual MT capabilities through data-efficient adaptation strategies. However, little work has explored their potential for low-resource African languages. We introduce AfriMMT-EA, the first highly multilingual benchmark and MT dataset for East African languages. Our datasets comprise 54 local languages across five East African countries. We used these data to fine-tune two multilingual versions of Gemma-3. We compare models' performance on these languages with larger off-the-shelf baselines. We release our data and models, in the interest of advancing MT for these low-resource languages and their communities.",Naome A Etori; Kelechi Ezema; Nathaniel Romney Robinson; Davis David; Alfred Malengo Kondoro; Elisha Ondieki Makori; Michael Samwel Mollel; Maria Gini,Naome A. Etori,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Machine Translation,Multilinguality & Low-Resource NLP,african; east; african languages; languages; multilingual; low resource; resource; low; machine translation; translation
674-FIND,Diffusion Language Model Inference with Monte Carlo Tree Search,"Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0\% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.",Zheng Huang; Kiran Ramnath; Yueyan Chen; Aosong Feng; Sangmin Woo; Balasubramaniam Srinivasan; Zhichao Xu; Kang Zhou; Shuai Wang; Haibo Ding; Lin Lee Cheong,Kiran Ramnath,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",search; diffusion language; diffusion; inference; carlo tree search; carlo tree; tree search; monte carlo tree; monte; monte carlo
676-FIND,DWA-KD: Dual-Space Weighting and Time-Warped Alignment for Cross-Tokenizer Knowledge Distillation,"Knowledge Distillation (KD) has emerged as a crucial technique for compressing Large Language Models (LLMs). Although existing cross-tokenizer KD methods have made notable progress, their effectiveness remains constrained by suboptimal alignment across sequence and vocabulary levels. To address these limitations, we introduce Dual-Space Weighting and Time-Warped Alignment (DWA-KD), a novel cross-tokenizer distillation framework that enhances token-wise distillation through dual-space entropy-based weighting and achieves precise sequence-level alignment by leveraging both lexical and semantic information. At the token level, DWA-KD maps teacher representations into the student space and vice versa, performing dual-space KD via KullbackвЂ“Leibler divergence (KL). The process is modulated by dual-space entropy-based weights that up-weight tokens where the student is uncertain and the teacher is confident, thereby focusing learning on informative tokens rather than treating all positions equally. At the sequence level, DWA-KD applies Soft Dynamic Time Warping (Soft-DTW) to both the embedding and final hidden-state layers, enabling robust alignment of lexical and contextual semantics between teacher and student sequences. Extensive experiments across diverse NLP benchmarks demonstrate that DWA-KD consistently outperforms state-of-the-art KD baselines, while ablation studies confirm the complementary contributions of entropy-based token weighting and embedding and final hidden state layer Soft-DTW alignment.",Duc Trung Vu; Pham Khanh Chi; Dat Phi Van; Linh Ngo Van; Dinh Viet Sang; Trung Le,Dinh Viet Sang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","Trustworthy, Safety, Privacy & Fairness",dual; weighting; space; distillation; alignment; entropy based; teacher; tokenizer; soft; student
677-FIND,Harnessing Consistency for Robust Test-Time LLM Ensemble,"Different large language models (LLMs) exhibit diverse strengths and weaknesses, and LLM ensemble serves as a promising approach to integrate their complementary capabilities. Despite substantial progress in improving ensemble quality, limited attention has been paid to the robustness of ensembles against potential erroneous signals, which often arise from heterogeneous tokenization schemes and varying model expertise. Our analysis shows that ensemble failures typically arise from both the token level and the model level: the former reflects severe disagreement in token predictions, while the latter involves low confidence and pronounced disparities among models. In light of this, we propose CoRE, a plug-and-play technique that harnesses model consistency for robust LLM ensemble, which can be seamlessly integrated with diverse ensemble methods. *Token-level consistency* captures fine-grained disagreements by applying a low-pass filter to downweight uncertain tokens with high inconsistency, often due to token misalignment, thereby improving robustness at a granular level. *Model-level consistency* models global agreement by promoting model outputs with high self-confidence and minimal divergence from others, enhancing robustness at a coarser level. Extensive experiments across diverse benchmarks, model combinations, and ensemble strategies demonstrate that CoRE consistently improves ensemble performance and robustness.",Zhichen Zeng; Qi Yu; Xiao Lin; Ruizhong Qiu; Xuying Ning; Tianxin Wei; Yuchen Yan; Jingrui He; Hanghang Tong,Zhichen Zeng,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"LLM Evaluation, Benchmarks & Metrics",ensemble; llm ensemble; level; consistency; robustness; token; level consistency; arise; token level; confidence
678-FIND,AutoAnoEval: Semantic-Aware Model Selection via Tree-Guided LLM Reasoning for Tabular Anomaly Detection,"In the tabular domain, which is the predominant data format in real-world applications, anomalies are extremely rare or difficult to collect, as their identification often requires domain expertise. Consequently, evaluating tabular anomaly detection models is challenging, since anomalies may be absent even in evaluation sets. To tackle this challenge, prior works have generated synthetic anomaly generation rely on statistical patterns, they often overlook domain semantics and struggle to reflect the complex, domain-specific nature of real-world anomalies. We propose AutoAnoEval, a novel evaluation framework for tabular AD that constructs pseudo-evaluation sets with semantically grounded synthetic anomalies. Our approach leverages an iterative interaction between a Large Language Model (LLM) and a decision tree (DT): the LLM generates realistic anomaly conditions based on contextual semantics, while the DT provides structural guidance by capturing feature interactions inherent in the tabular data. This iterative loop ensures the generation of diverse anomaly conditions, ranging from easily detectable outliers to subtle cases near the decision boundary. Extensive experiments on 20 tabular AD benchmarks demonstrate that AutoAnoEval achieves superior model selection performance, with high ranking alignment and minimal performance gaps compared to evaluations on anomalies encountered in practical applications.",Suhee Yoon; Sanghyu Yoon; Ye Seul Sim; Seungdong Yoa; Dongmin Kim; Soonyoung Lee; Hankook Lee; Woohyung Lim,Suhee Yoon,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics",Summarization & Generation,anomaly; tabular; model selection; domain; tree; sets; iterative; conditions; semantics; decision
681-FIND,Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment,"Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the answers being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling answer-side uncertainty complements preference-level weighting and yields more robust, data-efficient alignment.",Tiejin Chen; Xiaoou Liu; Vishnu Nandam; Kuan-Ru Liou; Hua Wei,Tiejin Chen,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Dialogue, Conversational & Interactive NLP",conformal; alignment; conformal prediction; preference; reliability; weighting; feedback; answer; uncertainty; preferences
688-FIND,ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization,"Large Reasoning Models (LRMs) are powerful, but they still suffer from inefficient and off-target reasoning. Currently, training-free methods are limited to either rigid heuristics or descriptive, non-actionable analyses. In this paper, we introduce ThinkPilot, a training-free framework that automatically optimizes LRMs reasoning. It uses an evolutionary process to generate \textit{think-prefixes}, namely instructions that evolve driven by a taxonomy of \textit{reasoning behaviors} to guide models toward superior performance. Extensive experiments demonstrate ThinkPilot's broad effectiveness: it significantly improves the accuracy-length trade-off for efficient reasoning, drastically improves safety (e.g., cutting the StrongREJECT score of DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7%), and enhances instruction following. It also synergizes with existing training-based methods. Specially, our analysis reveals that think-prefixes can reliably control LRMsвЂ™ reasoning behaviors, and that different tasks have strong preferences for specific behavioral distributions. By automatically identifying and eliciting these behaviors, ThinkPilot provides a generalizable framework for aligning LRMs reasoning with task demands.",Sunzhu Li; Zhiyu Lin; Jiale Zhao; Shuling Yang; Chen Wei,Sunzhu Li,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Dialogue, Conversational & Interactive NLP",prefixes; reasoning; lrms; think; behaviors; training free; reasoning models; textit; automatically; free
689-FIND,LitE-SQL: A Lightweight and Efficient Text-to-SQL Framework with Vector-based Schema Linking and Execution-Guided Self-Correction,"The Text-to-SQL task translates natural language questions into SQL queries, enabling intuitive database interaction for non-experts. While recent methods leveraging Large Language Models (LLMs) achieve strong performance, their reliance on proprietary models raise concerns about deployment feasibility and data privacy. In this work, we introduce LitE-SQL, a Lightweight and Efficient framework with two components: (i) a Schema Retriever that performs efficient schema linking using a vector database of pre-computed schema embeddings, and (ii) a SQL Generator fine-tuned in two stagesвЂ”supervised fine-tuning followed by execution-guided reinforcementвЂ”enabling self-correction without costly multi-candidate generation. On BIRD, LitE-SQL achieves 72.10% execution accuracy, and on Spider 1.0 it reaches 88.45%, demonstrating comparable or superior performance to LLM-based methods despite using 2Г— to 30Г— fewer parameters. Our findings demonstrate that high-quality Text-to-SQL generation is feasible with lightweight models, offering a practical solution for privacy-sensitive and resource-constrained settings.",Shengmin Piao; Jieun Lee; Sanghyun Park,Shengmin Piao,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Summarization & Generation,"Trustworthy, Safety, Privacy & Fairness",sql; schema; lite; text sql; execution; schema linking; database; lightweight; self correction; vector
691-FIND,Beyond Coherence: Improving Temporal Consistency and Interpretability in Dynamic Topic Models,"Dynamic topic models aim to reveal how themes emerge, evolve, and dissolve in time-stamped corpora, but existing approaches still face three major challenges: (i) encoders capture bag-of-words statistics but fail to align with the rich semantic priors of large pre-trained language models, (ii) temporal linkages are often modeled as rigid one-to-one chains, limiting the ability to track non-linear evolution such as topic splits or merges, and (iii) interpretability remains shallow, relying on noisy top-word lists that obscure thematic clarity. We propose L-DNTM (LLM-Augmented for Dynamic Neural Topic Model), a variational framework designed to capture more faithful temporal trajectories. Our model integrates three key components: multi-objective distillation to inject PLM-derived semantic knowledge into the encoder, entropy-regularized optimal transport to align entire topic constellations across time for smooth yet flexible evolution, and LLM-guided refinement to sharpen topicвЂ“word distributions for improved interpretability. Extensive experiments on diverse corpora show that L-DNTM yields more coherent, temporally consistent, and interpretable topic dynamics, and further enhances downstream classification and clustering tasks.",Thanh Vinh Nguyen; Ngo Van Dong; Chu Xuan Minh; Tung Nguyen; Linh Ngo Van; Dinh Viet Sang; Trung Le,Dinh Viet Sang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Efficiency, Scaling & NLP Systems",topic; dynamic topic models; dynamic topic; temporal; interpretability; topic models; dynamic; evolution; word; corpora
693-FIND,Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use,"Illicit drug use among teenagers and young adults (TYAs) remains a pressing public health concern, with rising prevalence and long-term impacts on health and well-being. To detect illicit drug use among TYAs, researchers analyze large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the National Survey on Drug Use and Health (NSDUH), which preserve rich demographic, psychological, and environmental factors related to substance use. However, existing modeling methods treat survey variables independently, overlooking latent and interconnected structures among them. To address this limitation, we propose LAMI (LAtent relation Mining with bi-modal Interpretability), a novel joint graph-language modeling framework for detecting illicit drug use and interpreting behavioral risk factors among TYAs. LAMI represents individual responses as relational graphs, learns latent connections through a specialized graph structure learning layer, and integrates a large language model to generate natural language explanations grounded in both graph structures and survey semantics. Experiments on the YRBS and NSDUH datasets show that LAMI outperforms competitive baselines in predictive accuracy. Interpretability analyses further demonstrate that LAMI reveals meaningful behavioral substructures and psychosocial pathways, such as family dynamics, peer influence, and school-related distress, that align with established risk factors for substance use. Our codebase is available here.",Yiyang Li; Zehong Wang; Zhengqing Yuan; Zheyuan Zhang; Keerthiram Murugesan; Chuxu Zhang; Yanfang Ye,Yiyang Li,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Linguistics, Syntax & Semantics",drug; use; survey; graph; latent; health; factors; substance use; substance; risk
698-FIND,Tailoring Memory Granularity for Multi-Hop Reasoning over Long Contexts,"Multi-hop reasoning over long contexts remains challenging, as it requires integrating relevant contexts scattered across distant sources while resisting semantic drift and noise from distracting content. While retrieval-augmented generation (RAG) has emerged as the prevailing solution, most RAG approaches encode and store context in monolithic memory representations, resulting in noisy retrieval and brittle reasoning. To overcome these limitations, we introduce TAG (Tailoring Memory Granularity), a framework that prestructures memory into diverse granularities and employs a reward-guided navigator to adaptively compose hybrid memory tailored to each query. The navigator is trained with a multi-objective BradleyвЂ“Terry loss that learns the relative utility of different memory types, enabling dynamic routing across granularities. This design allows RAG systems to balance fine-grained detail with high-level abstraction, yielding more reliable reasoning. Extensive experiments on long-context multi-hop question answering (QA) benchmarks show that TAG achieves state-of-the-art performance. With only 0.033% additional parameters, it remains lightweight, highlighting its practicality as a scalable and effective solution for enhancing language model agents in complex, real-world scenarios.",Peijun Qing; Xingjian Diao; Chiyu Ma; Saeed Hassanpour; Soroush Vosoughi,Peijun Qing,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","Retrieval, Grounding & External Knowledge (RAG)",memory; multi hop; hop; tag; tailoring; long contexts; granularities; reasoning long; rag; hop reasoning
699-FIND,Unlocking Large Audio-Language Models for Interactive Language Learning,"Achieving pronunciation proficiency in a second language (L2) remains a challenge, despite the development of Computer-Assisted Pronunciation Training (CAPT) systems. Traditional CAPT systems often provide unintuitive feedback that lacks actionable guidance, limiting its effectiveness. Recent advancements in audio-language models (ALMs) offer the potential to enhance these systems by providing more user-friendly feedback. In this work, we investigate ALMs for chat-based pronunciation training by introducing \textbf{L2-Arctic-plus}, an English dataset with detailed error explanations and actionable suggestions for improvement. We benchmark cascaded ASR+LLMs and existing ALMs on this dataset, specifically in detecting mispronunciation and generating actionable feedback. To improve the performance, we further propose to instruction-tune ALMs on L2-Arctic-plus. Experimental results demonstrate that our instruction-tuned models significantly outperform existing baselines on mispronunciation detection and suggestion generation in terms of both objective and human evaluation, highlighting the value of the proposed dataset.",Hongfu Liu; Zhouying Cui; Xiangming Gu; Ye Wang,Cui Zhouying,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Dialogue, Conversational & Interactive NLP","LLM Evaluation, Benchmarks & Metrics",pronunciation; actionable; plus; feedback; audio language; audio language models; audio; systems; instruction; dataset
706-FIND,Blind Spot Navigation in Large Language Model Reasoning with Thought Space Explorer,"Large language models have shown strong reasoning capabilities through chain-structured methods such as Chain-of-Thought. Recent studies optimize thought structures by generating parallel or tree-like structures, switching long and short reasoning modes, or aligning reasoning steps with task performance. However, these approaches mainly rely on previously generated logical directions of the chains, which ignore the unexplored regions of the solution space. Such a phenomenon is denoted as blind spots, which limit the diversity and effectiveness of the reasoning process. To this end, we propose the ``Thought Space Explorer'' (TSE), a framework for navigating and expanding thought structures to overcome blind spots in LLM reasoning. Our TSE first identifies key nodes with high impact, then generates new nodes by integrating information from multiple chains. Finally, it extends new branches through connection strategies. We conduct a series of experiments on math and QA benchmarks. Compared to existing baseline methods, TSE improves the accuracy of both the final answer and intermediate reasoning steps, while maintaining a better effectiveness-efficiency trade-off for practical deployment.",Jinghan Zhang; Fengran Mo; Tharindu Cyril Weerasooriya; Xinyue Ye; Dongjie Wang; Yanjie Fu; Kunpeng Liu,Jinghan Zhang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",thought; blind; reasoning; structures; nodes; space; reasoning steps; chains; steps; chain
707-FIND,StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs,"Large language models (LLMs) have shown strong performance in zero-shot summarization, but often struggle to model document structure and identify salient information in long texts. In this work, we introduce StrucSum, a training-free prompting framework that enhances LLM reasoning through sentence-level graph structures. StrucSum injects structural signals into prompts via three targeted strategies: Neighbor-Aware Prompting (NAP) for local context, Centrality-Aware Prompting (CAP) for importance estimation, and Centrality-Guided Masking (CGM) for efficient input reduction. Experiments on ArXiv, PubMed, and Multi-News demonstrate that StrucSum consistently improves both summary quality and factual consistency over unsupervised baselines and vanilla prompting. In particular, on ArXiv, it increases FactCC and SummaC by 19.2% and 8.0% points, demonstrating stronger alignment between summaries and source content. The ablation study shows that the combination of multiple strategies does not yield clear performance gains; therefore, structure-aware prompting with graph-based information represents a promising and underexplored direction for the advancement of zero-shot extractive summarization with LLMs.",Haohan Yuan; Sukhwa Hong; Haopeng Zhang,Haohan Yuan,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,"Retrieval, Grounding & External Knowledge (RAG)",aware prompting; prompting; summarization; arxiv; extractive; graph; aware; structure; document; zero shot
712-FIND,Logits-Based Block Pruning with Affine Transformations for Large Language Models,"As the scale of Large Language Models (LLMs) continues to grow rapidly, the cost of training and inference has significantly increased, limiting their application in resource-constrained scenarios. To address this challenge, model pruning has been widely used to reduce computational complexity. Among various pruning strategies, block-wise pruning has gained popularity due to its ability to accelerate computation by removing entire blocks of parameters. However, existing methods often rely on hard labels from calibration datasets and neglect the cumulative effects of pruning on subsequent blocks. To address this, we propose two complementary techniques: the Logit Disruption Score (LDS), a novel block importance criterion that measures the impact of pruning by comparing the cosine similarity between the logits of the original and pruned models, focusing on the most informative logit dimensions to better preserve the model's core capabilities; and Activation Statistics Correction (ASC), an affine transformation mechanism that aligns the mean and variance of activations in the pruned model with those of the original model, effectively mitigating the distribution shift caused by block removal and improving the information flow in subsequent blocks. Experiments across multiple datasets show that our approach reduces reliance on calibration data and improves generalization, achieving competitive results with existing methods.",Zekun Hu; Yichu Xu; De-Chuan Zhan,Zekun Hu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",pruning; block; blocks; pruned; logit; logits; subsequent; calibration; original; existing methods
719-FIND,MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models,"Large language models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claimsвЂ”a key step in the fact-checking processвЂ”remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, six topical domains, and two writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark three common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.",Martin Hyben; Sebastian Kula; Jan Cegin; Jakub Simko; Ivan Srba; Robert Moro,Martin Hyben,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics",Multilinguality & Low-Resource NLP,check; balanced; claim; claim detection; languages; fine tuned; styles; fine tuned models; tuned; fact checking
722-FIND,What Really Matters for Table LLMs? A Meta-Evaluation of Model and Data Effects,"Table modeling has progressed for decades. In this work, we revisit this trajectory and highlight emerging challenges in the LLM era, particularly the paradox of choice: the difficulty of attributing performance gains amid diverse base models and training sets in the context of table instruction tuning. We replicate four table LLMs by instruction-tuning three foundation models on four existing datasets, yielding 12 models. We then evaluate these models across 16 table benchmarks. Our study is the first to quantitatively disentangle the effects of training data and base model selection, revealing that base model choice plays a more dominant role than the training data itself. Generalization and reasoning remain challenging, inviting future effort on table modeling. Based on our findings, we share our thoughts on the future directions for table modeling.",Naihao Deng; Sheng Zhang; Henghui Zhu; Shuaichen Chang; Jiani Zhang; Alexander Hanbo Li; Chung-Wei Hang; Hideo Kobayashi; Yiqun Hu; Patrick Ng,Naihao Deng,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",table; base; base model; modeling; instruction tuning; choice; training data; effects; future; instruction
723-FIND,Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features,"We present a novel metric for the evaluation of morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.",Abishek Stephen; JindЕ™ich LibovickГЅ,Abishek Stephen,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics","Retrieval, Grounding & External Knowledge (RAG)",morphological; morpheme; metric; plausibility; features; boundary; subword; segmentation; syntactic; languages
731-FIND,MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning,"We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain Contrastive learning), a multi-stage framework for domain adaptation of text embedding models that incorporates joint domain-specific masked supervision. Our approach addresses the challenges of adapting large-scale general-domain text embedding models to specialized domains. By jointly optimizing masked language modeling (MLM) and contrastive objectives within a unified training pipeline, our method enables effective learning of domain-relevant representations while preserving the robust semantic discrimination properties of the original model. We empirically validate our approach on both high-resource and low-resource domains, achieving improvements up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong general-domain baselines. Comprehensive ablation studies further demonstrate the effectiveness of each component, highlighting the importance of balanced joint supervision and staged adaptation.",Vera Pavlova; Mohammed Makhlouf,Vera Pavlova,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multilinguality & Low-Resource NLP,,masked; domain; adaptation; contrastive; text embedding models; general domain; text embedding; contrastive learning; joint; embedding models
733-FIND,BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage,"The inherent risk of generating harmful and unsafe content by Large Language Models (LLMs), has highlighted the need for their safety alignment. Various techniques like supervised fine-tuning, reinforcement learning from human feedback, and red-teaming were developed for ensuring the safety alignment of LLMs. However, the robustness of these aligned LLMs is always challenged by adversarial attacks that exploit unexplored and underlying vulnerabilities of the safety alignment. In this paper, we develop a novel black-box jailbreak attack, called BitBypass, that leverages hyphen-separated bitstream camouflage for jailbreaking aligned LLMs. This represents a new direction in jailbreaking by exploiting fundamental information representation of data as continuous bits, rather than leveraging prompt engineering or adversarial manipulations. Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude 3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the capabilities of BitBypass in bypassing their safety alignment and tricking them into generating harmful and unsafe content. Further, we observed that BitBypass outperforms several state-of-the-art jailbreak attacks in terms of stealthiness and attack success. Overall, these results highlights the effectiveness and efficiency of BitBypass in jailbreaking these state-of-the-art LLMs.",Kalyan Nakka; Nitesh Saxena,Kalyan Nakka,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",,safety alignment; jailbreaking; safety; generating harmful; new direction; aligned llms; adversarial; alignment; aligned; state art llms
736-FIND,The Problem of Ambiguity in Table Question Answering,"Question Answering on Tabular Data (or Table Question Answering) has seen tremendous advances with the coming of new generation Large Language Models (LLMs). Despite this, significant challenges still remain to be solved if we are to develop robust enough approaches for general usage. One of these is ambiguity in question answering, which historically has not merited much attention due to the previously limited capabilities of LLMs. In this work, we outlay the main types of ambiguousness inherent to tabular data. Then, we discuss how they are influenced by the way our models interact with the information stored in the tables, and we test the capabilities of some LLMs in detecting them. This work provides an initial ground for a deeper discussion on how to approach ambiguity in Tabular Data in the age of LLMs.",Jorge OsГ©s Grijalba; L. Alfonso UreГ±a; Eugenio MartГ­nez-CГЎmara; Jose Camacho-Collados,Jose Camacho Collados,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Summarization & Generation,Interpretability & Model Analysis,ambiguity; tabular data; tabular; question answering; answering; table question answering; table question; question; capabilities llms; table
740-FIND,Beyond Multiple Choice: Evaluating Steering Vectors for Summarization,"Steering vectors are a lightweight method for controlling text properties by adding a learned bias to language model activations at inference time. While predominantly studied for multiple-choice and toy tasks, their effectiveness in free-form generation remains largely unexplored. Moving ``Beyond Multiple Choice,'' we thoroughly evaluate the effectiveness of steering vectors in adaptively controlling topical focus, sentiment, toxicity, and readability in abstractive summaries from the SAMSum, NEWTS and arXiv datasets. We find that steering effectively controls the targeted summary properties, but high steering strengths consistently degrade both intrinsic and extrinsic text quality. Prompting alone offers weaker control, while preserving text quality. Combining steering and prompting yields the strongest control over text properties and offers the most favorable efficacy-quality trade-off at moderate steering strengths. Our results show steering vectors are an efficient method for controlling summarization, but introduce a critical trade-off between control strength and text quality. We demonstrate this balance can be optimized through hybrid techniques, which offer the most favorable efficacy-quality trade-off.",Joschka Braun; Carsten Eickhoff; Seyed Ali Bahrainian,Joschka Braun,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,"Trustworthy, Safety, Privacy & Fairness",steering; steering vectors; text quality; vectors; controlling; properties; multiple choice; quality; text properties; choice
741-FIND,Similar Region Search using LLMs on Spatial Feature Space,"Understanding regional similarities is crucial for applications such as urban planning, tourism recommendations, business expansion, and disease prevention. While spatial data, including POI distributions, check-in activity, and building footprints, offer valuable insights, existing similarity methodsвЂ”based on distance metrics, embeddings, or deep metric learningвЂ”fail to capture the contextual richness and adapt to heterogeneous spatial data. To overcome these limitations, we introduce a novel similar region search framework that ranks candidate regions based on their similarity to a query region using large language models. To further enhance performance, we fine-tune the model through self-supervised learning by introducing controlled noise into spatial data. This generates similar and dissimilar samples without relying on extensive labeled data. By transforming spatial data into natural language descriptions, our method seamlessly integrates heterogeneous datasets without requiring structural modifications, ensuring scalability across diverse urban contexts. Experiments on real-world datasets demonstrate the effectiveness of our framework, showing significant improvements in accuracy and ranking performance compared to state-of-the-art methods.",Al-Amin Sany; Mohaiminul Islam; Tanzima Hashem; Md. Ashraful Islam; Mohammed Eunus Ali,Mohammed Eunus Ali,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",spatial; region; urban; similar; heterogeneous; similarity; search; scalability diverse; data including; offer valuable
742-FIND,Learning to Ask: Multi-Decoder Fine-Tuning for Multi-Hop Visual Question Generation with External Knowledge,"Multi-hop visual question generation (VQG) seeks to create coherent, fluent, and contextually rich questions by integrating knowledge from a structured knowledge graph (KG) with information inferred from image pairs, where at least one reasoning step involves a visual relationship between the images. Traditional supervised QG methods which rely on token-level alignment with fixed gold labels struggle to capture diverse valid question formulations. We propose $\textbf{M3RQG}$ ($\textbf{M}$ultimodal $\textbf{M}$ulti-hop $\textbf{M}$ulti-decoder $\textbf{R}$etrieval-augmented $\textbf{Q}$uestion $\textbf{G}$eneration), a model-agnostic framework that integrates multimodal inputs (images, KG facts) with a multi-decoder architecture to optimize for multiple labels per sample to design multi-hop questions. M3RQG addresses these challenges: (1) generating meaningful visually-grounded questions given a pair of images, (2) generating rich questions that require multi-hop reasoning across images and KG facts, and (3) integrating diverse question labels during fine-tuning. We extend the WebQA dataset with multi-hop questions generated by GPT-4V and Gemini, resulting in two complementary silver labels per sample. Our approach integrates retrieval-augmented generation (RAG) for accessing external knowledge, a PPO objective with ROUGE-based rewards to prioritize structural correctness, and a named entity overlap loss to improve factual accuracy. Experiments across BART, Phi-3.5, and LLaVA backbones demonstrate significant improvements in fluency, reasoning depth, and relevance. We release our code and dataset to facilitate future research.",Arpan Phukan; Manish Gupta; Asif Ekbal,Arpan Phukan,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",Multimodal & Speech/Audio,textbf; hop; multi hop; multi; questions; labels; images; decoder; hop questions; kg facts
743-FIND,SLANG-GraphRAG: Multi-Layered Retrieval with Domain-Specific Knowledge for Low Resource Social Media Conversations,"Emotion classification on social media is especially difficult when texts include informal, culturally grounded language like slang. Standard NLP benchmarks often miss these nuances, particularly in low-resource settings. We present SLANG-GraphRAG, a retrieval-augmented framework that integrates a culture-specific slang knowledge graph into large language models via one-shot prompting. Using multiple retrieval strategies, we incorporate slang definitions, regional usage, and conversational context. Our results show that incorporating structured cultural knowledge into the retrieval process leads to significant improvements, improving accuracy by up to 31% and F1 score by 28%, outperforming traditional and unstructured retrieval methods. To better evaluate model behavior, we propose a probabilistic metric that reflects the distribution of human annotations, providing a more nuanced measure of performance. This highlights the value of culturally sensitive applications and more balanced evaluation in subjective NLP tasks.",Ifeoluwa Wuraola; Daniel Marciniak; Nina Dethlefs,Ifeoluwa Wuraola,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics","Retrieval, Grounding & External Knowledge (RAG)",slang; retrieval; graphrag; social media; culturally; media; knowledge; social; low resource; nlp
744-FIND,Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers,"Scientific research is inherently global. However, the vast majority of academic journals are published exclusively in English, creating barriers for non-native-English-speaking researchers. In this study, we leverage large language models (LLMs) to translate published scientific articles while preserving their native JATS XML formatting, thereby developing a practical, automated approach for implementation by academic journals. Using our approach, we translate articles across multiple scientific disciplines into 28 languages. To evaluate translation accuracy, we introduce a novel question-and-answer (QA) benchmarking method and show an average performance of 95.9%, indicating that the key scientific details are accurately conveyed. In a user study, we translate the scientific papers of 15 researchers into their native languages. Interestingly, a third of the authors found many technical terms вЂњovertranslated,вЂќ expressing a preference to keep terminology more familiar in English untranslated. Finally, we demonstrate how in-context learning techniques can be used to align translations with domain-specific preferences such as mitigating overtranslation, highlighting the adaptability and utility of LLM-driven scientific translation.",Hannah Calzi Kleidermacher; James Zou,Hannah Calzi Kleidermacher,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Machine Translation,Domain NLP (Biomedical/Clinical/Legal/Scientific),scientific; translate; native; scientific papers; published; translation; academic; papers; researchers; articles
746-FIND,TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs,"Speculative decoding (SD) has proven effective for accelerating LLM inference by quickly generating draft tokens and verifying them in parallel. However, SD remains largely unexplored for Large Vision-Language Models (LVLMs), which extend LLMs to process both image and text prompts. To address this gap, we benchmark existing inference methods with small draft models on 11 datasets across diverse input scenarios and observe scenario-specific performance fluctuations. Motivated by these findings, we propose **Test-time Adaptive Batched Ensemble Drafting (TABED)**, which dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in the SD setting. The dynamic ensemble method achieves an average robust walltime speedup of 1.74Г— over autoregressive decoding and a 5% improvement over single drafting methods, while remaining training-free and keeping ensembling costs negligible through parameter sharing. With its plug-and-play compatibility, we further enhance TABED by integrating advanced verification and alternative drafting methods.",Minjae Lee; Wonjun Kang; Byeongkeun Ahn; Christian Classen; Kevin Galim; Seunghyuk Oh; Minghao Yan; Hyung Il Koo; Kangwook Lee,Minjae Lee,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",drafting; ensemble; drafting methods; decoding; draft; speculative; speculative decoding; lvlms; test time; inference
759-FIND,KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge,"Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present **KGHaluBench**, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and more comprehensive insight into LLM truthfulness. Our framework utilises the KG to dynamically construct challenging, multifaceted questions, whose difficulty is then statistically estimated to address popularity bias. Our automated verification pipeline detects abstentions and verifies the LLM's response at both conceptual and correctness levels to identify different types of hallucinations. We evaluate 25 frontier models, using novel accuracy and hallucination metrics. The results provide a more interpretable insight into the knowledge factors that cause hallucinations across different model sizes. KGHaluBench is publicly available to support future developments in hallucination mitigation.",Alex Robertson; Huizhi Liang; Mahbub Gani; Rohit Kumar; Srijith Rajamohan,Alex Robertson,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",hallucination; knowledge; hallucinations; knowledge graph based; truthfulness; graph based; depth; knowledge graph; insight; graph
767-FIND,Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code,"Identifying LLM-generated code through watermarking poses a challenge in preserving functional correctness. Previous methods rely on the assumption that watermarking high-entropy tokens effectively maintains output quality. Our analysis reveals a fundamental limitation of this assumption: syntax-critical tokens such as keywords often exhibit the highest entropy, making existing approaches vulnerable to logic corruption. We present STONE, a syntax-aware watermarking method that embeds watermarks only in non-syntactic tokens and preserves code integrity. For its rigorous assessment, we also introduce STEM, a comprehensive framework that balances three critical dimensions: correctness, detectability, and imperceptibility. Across Python, C++, and Java, STONE preserves correctness, sustains strong detectability, and achieves balanced performance with minimal overhead. Our implementation is available at https://anonymous.4open.science/r/STONE-watermarking-AB4B/.",Jungin Kim; Shinwoo Park; Yo-Sub Han,Jungin Kim,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics",Interpretability & Model Analysis,watermarking; code; llm generated code; correctness; detectability; generated code; tokens; syntax; assumption; preserves
768-FIND,VIGiA: Instructional Video Guidance via Dialogue Reasoning and Retrieval,"We introduce VIGiA, a novel multimodal dialogue model designed to understand and reason over complex, multi-step instructional video action plans. Unlike prior work which focuses mainly on text-only guidance, or treats vision and language in isolation, VIGiA supports grounded, plan-aware dialogue that requires reasoning over visual inputs, instructional plans, and interleaved user interactions. To this end, VIGiA incorporates two key capabilities: (1) multimodal plan reasoning, enabling the model to align uni- and multimodal queries with the current task plan and respond accurately; and (2) plan-based retrieval, allowing it to retrieve relevant plan steps in either textual or visual representations. Experiments were done on a novel dataset with rich Instructional Video Dialogues aligned with Cooking and DIY plans. Our evaluation shows that VIGiA outperforms existing state-of-the-art models on all tasks in a conversational plan guidance setting, reaching over 90% accuracy on plan-aware VQA.",Diogo GlГіria-Silva; David Semedo; Joao Magalhaes,Diogo Glória-Silva,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,"Dialogue, Conversational & Interactive NLP",plan; instructional; plans; video; dialogue; guidance; multimodal; visual; reasoning; enabling model
776-FIND,Attribute-Controlled Translation with Preference Optimization,"Attribute-controlled translation (ACT) seeks to produce translations that satisfy specific constraints on linguistic and stylistic attributes. While careful prompt engineering can enable large language models to perform strongly in this task, its effectiveness is mainly limited to models of very large size. For this reason, in this paper we set to improve the performance of language models of more contained size by leveraging the contrastive nature of ACT tasks with preference optimization, as well as exploiting knowledge distillation with synthetically-generated training samples from larger models. As a resource for this investigation, we also introduce PREF-FAME-MT, a large, contrastive, formality-controlled parallel corpus which has been generated by expanding the existing FAME-MT dataset with synthetic contrastive samples. Experiments conducted over three datasets for formality- and gender-controlled translation with 71 distinct language pairs have demonstrated the effectiveness of the proposed approach at simultaneously improving attribute matching and translation quality. We release all our code and datasets to allow reproduction and expansion of our work.",Inigo Jauregi Unanue; Najmeh Sadoughi; Vimal Bhat; Zhu Liu; Massimo Piccardi,Inigo Jauregi Unanue,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Machine Translation,"Efficiency, Scaling & NLP Systems",controlled; translation; attribute; contrastive; attribute controlled; formality; act; preference optimization; size; samples
780-FIND,ReciFine: Finely Annotated Recipe Dataset for Controllable Recipe Generation,"We introduce ReciFine, the largest human-evaluated, finely annotated recipe dataset to date, designed to advance controllable and trustworthy recipe generation. Existing resources, such as RecipeNLG, extract food items only from ingredient lists, overlooking entities expressed in instructions, including tools, chef actions, food and tool states, and durations, which are crucial for realistic and context-aware generation. To address this limitation, we extend RecipeNLG with finely annotated extraction of over 97 million entities across ten entity types from 2.2 million recipes. We are the first to explore recipe generation with explicit control over multiple entity types, enabling models to generate recipes conditioned not only on ingredients but also on tools, chef actions, cooking durations, and other contextual factors. Large language models fine-tuned or few-shot prompted with ReciFine extractions consistently outperform those trained on ingredient-list data alone across both automatic and human evaluations. ReciFine establishes a foundation for factual, coherent, structured, controllable recipe generation, and we release a human-annotated benchmark to support future evaluation and model development.",Nuhu Ibrahim; Rishi Ravikumar; Robert Stevens; Riza Batista-Navarro,Nuhu Ibrahim,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",Summarization & Generation,recipe; annotated; controllable; entity types; recipes; food; generation; million; actions; entities
783-FIND,ReBPE: Iteratively Improving the Internal Structure of a Structured Tokeniser by Mining its Internal Structure,"Recent work has explored pruning merges from BPE subword tokenisers using signals from corpus data. We argue that because a BPE tokeniser contains a rich data structure on top of its vocabulary set, this in itself can be used as a guide to modify its merges such that segmentations become more desirable. We apply this argument to one of those pruning algorithms, BPE-knockout, by introducing a new reification step that suggests new merges by inspecting the effects left by pruning. By alternating both processes iteratively until convergence, we get a new BPE tokeniser, ReBPE, which outperforms the original BPE-knockout algorithm on morphological alignment.",Thomas Bauwens; Miryam de Lhoneux,Thomas Bauwens,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Trustworthy, Safety, Privacy & Fairness",,bpe; tokeniser; merges; pruning; internal structure; structure; iteratively; new; internal; recent work explored
800-FIND,"Emotion Recognition in Multi-Speaker Conversations through Speaker Identification, Knowledge Distillation, and Hierarchical Fusion","Emotion recognition in multi-speaker conversations faces significant challenges due to speaker ambiguity and severe class imbalance. We propose a novel framework that addresses these issues through three key innovations: (1) a speaker identification module that leverages audio-visual synchronization to accurately identify the active speaker, (2) a knowledge distillation strategy that transfers superior textual emotion understanding to audio and visual modalities, and (3) hierarchical attention fusion with composite loss functions to handle class imbalance. Comprehensive evaluations on MELD and IEMOCAP datasets demonstrate superior performance, achieving 67.75% and 72.44% weighted F1 scores respectively, with particularly notable improvements on minority emotion classes.",LI XIAO; Kotaro Funakoshi; Manabu Okumura,Xiao_LI,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Efficiency, Scaling & NLP Systems",speaker; emotion; speaker identification; audio visual; emotion recognition; class imbalance; imbalance; knowledge distillation; fusion; class
806-FIND,Demystifying Mixed Outcomes of Self-Training: Pre-training Analyses on Non-Toy LLMs,"We investigate whether large language models (LLMs) can improve through recursive training on self-generated text, a topic where prior studies report conflicting outcomes: some find evidence of performance gains (i.e., \textit{self-improvement}), while others observe performance degradation (i.e., \textit{model collapse}). To clarify this discrepancy, we use the OLMo-2 models as non-toy LLMs and perform multiple rounds of continual pre-training using self-generated text with different prompting strategies and data filtering. Our experiments show that naive recursive self-training does not improve either perplexity or downstream task performance, regardless of model size. These results suggest that model collapse observed in naive recursive training is inherent to the training procedure itself, while self-improvement likely owes its success not to the modelвЂ™s autonomous refinement but to human-designed, strategic synthetic pipelines that inject external intelligence.",Yusuke Nakamura; Hirokazu Kiyomaru; Chaoran Liu; Shuhei Kurita; Daisuke Kawahara,Yusuke Nakamura,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Retrieval, Grounding & External Knowledge (RAG)",,self; recursive; training; self training; toy; self generated; naive; collapse; generated text; outcomes
812-FIND,Revealing Redundant Syntax in Large Language Models through Multi-Hop Dependency Paths,"Prior work on attentionвЂ“syntax alignment has largely focused on single-hop Universal Dependency edges (DPs). In this paper, we treat short multi-hop dependency paths (MDPs) (e.g., вЂњobl+caseвЂќ) as first-class units and analyze them alongside DPs. Across three pretrained autoregressive LMs (GPT-2 XL, Llama 3 8B, Qwen3-8B) and one encoder baseline (BERT-large), we extract 2--3 hop MDPs from UD-parsed English and quantify headвЂ“relation alignment with an Unlabeled Attachment Score (UAS)вЂ“style metric modified for causal masking in decoder-only models. Rank visualizations reveal both overlap and specialization: we observe heads that align with both DPs and MDPs, as well as heads that appear specialized for one route. To test functional relevance, we first identify heads by UAS and then apply an undifferentiated (uniform) attention ablation to those heads; we evaluate the impact on BLiMP and LAMBADA. Ablating the top 10% of all heads shows that MDP-selected heads induce larger drops than DP-selected heads and that the union (вЂњMixвЂќ) of DP- and MDP-selected heads yields the largest drops. For GPT-2 XL, the observed drops are (BLiMP: DP = $\Delta$1.35 pp, MDP = $\Delta$ 4.81 pp, Mix = $\Delta$ 7.11 pp; LAMBADA: DP = $\Delta$ 4.70 pp, MDP = $\Delta$ 25.17 pp, Mix = $\Delta$ 32.99 pp), all exceeding size-matched random controls. These results indicate that models can route information consistent with syntactic dependencies via both DP and MDP pathways, with MDPs playing a distinct and measurable role under our interventions.",Masaki Sashida; Takeshi Kojima; Yusuke Iwasawa; Yutaka Matsuo,Masaki Sashida,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Linguistics, Syntax & Semantics","LLM Evaluation, Benchmarks & Metrics",heads; delta; mdp; selected heads; hop; dependency; selected; drops; mix; route
813-FIND,A Scalable Framework for Automated NER Annotation Correction in Low-Resource Languages,"Poor quality or noisy annotations in Named Entity Recognition (NER), as in any other NLP task, make it challenging to achieve state-of-the-art performance. In this paper, we present a multi-step framework to enhance the annotation quality of NER datasets by employing automated techniques. We propose a frequency-based iterative approach that leverages self-training and a dual-threshold mechanism to enhance inference confidence. Experimental evaluations on different NER datasets demonstrate significant improvements in NER performance with respect to the original datasets. This work further explores the potential of generative Large Language Models (LLMs) to perform NER for low-resource languages.",Toqeer Ehsan; Thamar Solorio,Toqeer Ehsan,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Information Extraction & Structured Prediction,Multilinguality & Low-Resource NLP,ner; ner datasets; low resource languages; resource languages; annotation; enhance; low resource; datasets; automated; explores potential
817-FIND,Can ChatGPT Really Understand Modern Chinese Poetry?,"ChatGPT has demonstrated remarkable capabilities on both poetry generation and translation, yet its ability to truly understand poetry remains unexplored. Previous poetry-related work merely analyzed experimental outcomes without addressing fundamental issues of comprehension. This paper introduces a comprehensive framework for evaluating ChatGPT's understanding of modern poetry. We collaborated with professional poets to evaluate ChatGPT's interpretation of unpublished modern Chinese poems by different poets along multiple dimensions. Evaluation results show that ChatGPT's interpretations align with the original poets' intents in over 73% of the cases. However, its understanding in certain dimensions, particularly in capturing poeticity, proved to be less satisfactory. These findings highlight the effectiveness and necessity of our proposed framework. This study not only evaluates ChatGPTвЂ™s ability to understand modern poetry but also establishes a solid foundation for future research on LLMs and their application to poetry-related tasks.",Shanshan Wang; Derek F. Wong; Jingming Yao; Lidia S. Chao,Shanshan Wang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Machine Translation,Summarization & Generation,poetry; chatgpt; modern; understand; chinese; dimensions; related; llms application; poetry generation; multiple dimensions
823-FIND,Knowing What's Missing: Assessing Information Sufficiency in Question Answering,"Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across a diverse suite of datasets, including multi-hop and factual QA. The results demonstrate that by forcing the model to justify its claims about missing information, our framework produces a more accurate sufficiency judgment while detailing any information gaps.",Akriti Jain; Aparna Garimella,Akriti Jain,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",sufficiency; missing; information; forcing model; forcing; question; assessing; factual; reliable; question answering
825-FIND,The Curse of Verbalization: How Presentation Order Constrains LLM Reasoning,"This paper delves into the factors that contribute to the difficulty of problems for large language models (LLMs). We begin with a pilot test evaluating LLMs' understanding of esoteric programming languages and find that LLMs struggle significantly when programs execute in an order that is unaligned with how the program is presented. This phenomenon leads to the hypothesis that LLM performance on reasoning correlates with the alignment between the order in which information is presented and the order in which it should be utilized. We demonstrate that this hypothesis holds broadly in mathematical reasoning: restructuring problems to align the order of information presentation with the order of utilization consistently improves performance across state-of-the-art models. We conjecture this occurs because LLMs acquire a strong tendency to verbalize information in presentation order during training on human text, a tendency detrimental in reasoning domains where the optimal utilization order often diverges from the presentation order. To provide further evidence, we construct pseudo-mathematical problems with nonsensical terms and quantify the verbalization flexibility of LLMs without interference from mathematical knowledge. Across twelve representative LLMs, we find that this flexibility exhibits a strong correlation (p = 0.87) with general reasoning performance rankings on LMArena.",Yue Zhou; Henry Peng Zou; Barbara Di Eugenio; Yang Zhang,Yue Zhou,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Retrieval, Grounding & External Knowledge (RAG)",order; mathematical; problems; verbalization; presented; utilization; tendency; reasoning; flexibility; llms
833-FIND,PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors,"Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.",Donya Rooein; Sankalan Pal Chowdhury; Mariia Eremeeva; Yuan Qin; Debora Nozza; Mrinmaya Sachan; Dirk Hovy,Donya Rooein,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Efficiency, Scaling & NLP Systems",,student; personality; tutoring; prefer; pedagogical; educational; strategies; use; llm; developing personalized
843-FIND,Mitigating Causal Bias in LLMs via Potential Outcomes Framework and Actual Causality Theory,"Event Causality Identification (ECI) aims to identify causal relationships between events, which is essential for root cause analysis. While recent studies reveal that Large Language Models (LLMs) exhibit significant causal hallucination, a systematic evaluation of their document-level ECI performance across varied structural characteristics and a corresponding dataset is currently lacking. To fill this gap, we first construct a structure-controlled dataset to comprehensively assess their document-level ECI performance across texts with various structural characteristics that influence the causal behaviors in ECI. We find that different LLMs exhibit divergent causal bias across texts with varied structures, ranging from consistent hallucination or neglect to structure-dependent shifts between the two. To mitigate the bias, furthermore, we formulate ECI as a causal inference problem and propose a causality identification framework grounded in the potential outcomes and the HalpernвЂ“Pearl (HP) definition of actual causality theory. Experimental results demonstrate that our framework significantly reduces the causal bias associated with directly using LLMs on ECI, while also achieving superior performance.",Yiheng Zhao; Yuanliang Li; Shreya Savant; Jun Yan,Yiheng Zhao,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Retrieval, Grounding & External Knowledge (RAG)","Trustworthy, Safety, Privacy & Fairness",causal; causality; bias; llms exhibit; varied; document level; actual; outcomes; characteristics; hallucination
844-FIND,JuriFindIT: an Italian legal retrieval dataset,"Statutory article retrieval (SAR) targets retrieval of legislative provisions relevant to a natural language question. The lexical gap between everyday queries and specialized legal language, as well as the structural dependencies of statute law, makes it a challenging task. Here, we introduce JuriFindIT, the first SAR dataset for the Italian legal domain and the first to explicitly encode cross-article references extracted from national legal code. The dataset covers four macro-areasвЂ”civil law, criminal law, anti-money laundering and counter-terrorism, and privacyвЂ”and includes 895 expert-authored questions and 169,301 generated ones, linked to more than 23,000 statutory articles. We provide retrieval models fine-tuned on JuriFindIT, proposing a pipeline that integrates dense encoders with an heterogeneous legislative graph, achieving consistent improvements over prior SAR approaches.",Niko Dalla Noce; Davide Colla; Sina Farhang Doust; Lorenzo De Mattei; Davide Bacciu,Niko Dalla Noce,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Retrieval, Grounding & External Knowledge (RAG)",legal; law; retrieval; article; italian; dataset; expert authored; language question; counter; extracted national
845-FIND,Towards Fair and Efficient De-identification: Quantifying the Efficiency and Generalizability of De-identification Approaches,"Large language models (LLMs) have shown strong performance on clinical text de-identification, the task of identifying sensitive text to protect patient privacy. However, their high computational requirements make them impractical for most clinical institutions. Furthermore, previous work has not examined their generalizability between formats, cultures, and genders. In this work, we systematically evaluate de-identification in fine-tuned transformer models (BERT, ClinicalBERT, ModernBERT), small LLMs (Llama 1-8B, Qwen 1.5-7B), and large LLMs (Llama-70B, Qwen-72B). We show that smaller models achieve comparable performance while substantially reducing inference cost, making them more practical for deployment. Moreover, we demonstrate that smaller models can be fine-tuned with limited data to outperform larger models in cross-culture de-identification. Our evaluation spans identifiers drawn from Mandarin, Hindi, Spanish, French, Bengali, and regional variations of English, in addition to gendered names. To improve robustness in multi-cultural contexts, we introduce and publicly release BERT-CrossCulture-DEID, a set of de-identification models based on BERT, ClinicalBERT, and ModernBERT, fine-tuned on MIMIC with identifiers from multiple cultures. Our findings provide the first comprehensive quantification of the efficiency-generalizability trade-off in de-identification and establish practical pathways for fair, efficient, and culturally inclusive clinical de-identification.",Noopur Zambare; Kiana Aghakasiri; Carissa Lin; Carrie Ye; J Ross Mitchell; Mohamed Abdalla,Noopur Zambare,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Efficiency, Scaling & NLP Systems",identification; generalizability; bert; efficiency generalizability; llms llama; clinical; identifiers; fair; fine tuned; cultures
848-FIND,How Many Ratings per Item are Necessary for Reliable Significance Testing?,"A cornerstone of machine learning evaluation is the (often hidden) assumption that model and human responses are reliable enough to evaluate models against unitary, authoritative, ``gold standard'' data, via simple metrics such as accuracy, precision, and recall. The generative AI revolution would seem to explode this assumption, given the critical role stochastic inference plays. Yet, in spite of public demand for more transparency in AI---along with strong evidence that humans are unreliable judges---estimates of model reliability are conventionally based on, at most, a few output responses per input item. We adapt a method, previously used to evaluate the reliability of various metrics and estimators for machine learning evaluation, to determine whether an (existing or planned) dataset has enough responses per item to assure reliable null hypothesis statistical testing. We show that, for many common metrics, collecting even 5-10 responses per item (from each model and team of human evaluators) is not sufficient. We apply our methods to several of the very few extant gold standard test sets with multiple disaggregated responses per item and show that even these datasets lack enough responses per item. We show how our methods can help AI researchers make better decisions about how to collect data for AI evaluation.",Christopher M Homan; Flip Korn; Deepak Pandita; Chris Welty,Christopher M. Homan,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics","Retrieval, Grounding & External Knowledge (RAG)",item; responses item; responses; learning evaluation; gold standard; reliable; gold; metrics; machine learning; assumption
849-FIND,QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs,"In this paper, we introduce the Quebec-French Benchmark of Linguistic Minimal Pairs (QFrBLiMP), a corpus designed to evaluate the linguistic knowledge of large language models on prominent grammatical phenomena in French. QFrBLiMP consists of 1,761 minimal pairs annotated with 20 linguistic phenomena. Specifically, these minimal pairs have been created by manually modifying sentences extracted from an official online resource maintained by a QuГ©bec government institution. Each pair is annotated by twelve Quebec-French native speakers, who select the sentence they feel is grammatical amongst the two. These annotations are used to compare the competency of LLMs with that of humans. In addition, QFrBLiMP is the first dataset for targeted linguistic evaluations of LLMs in French, enabling a comparison of the linguistic competence of LLMs across different languages. We evaluate different French-based LLMs by observing the rate of higher probabilities assigned to the sentences of each minimal pair for each category. We find that while grammatical competence scales with model size, a clear hierarchy of difficulty emerges. All benchmarked models consistently fail on phenomena requiring deep semantic understanding, revealing a critical limitation and a significant gap compared to human performance on these specific tasks.",David Beauchemin; Pier-Luc Veilleux; Johanna-Pascale Roy; Richard Khoury,Pier-Luc Veilleux,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Linguistics, Syntax & Semantics","LLM Evaluation, Benchmarks & Metrics",french; minimal pairs; minimal; linguistic; phenomena; pairs; grammatical; pair; competence; sentences
850-FIND,QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Task,"This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized ""unmarked"" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistributeвЂ”but not eliminateвЂ”representational harms.",Mae Sosto; Delfina S. Martinez Pandiani; Laura Hollink,Mae Sosto,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Trustworthy, Safety, Privacy & Fairness",,norms; marked; subjects; representational; regard; measurable; societal; reproduce; subject; toxicity
853-FIND,Efficient Table Retrieval and Understanding with Multimodal Large Language Models,"Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose \mytitlee, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods in both retrieval accuracy (7.0% in recall) and answer quality (6.1% in accuracy), offering a practical solution for real-world table understanding tasks.",Zhuoyan Xu; Haoyang Fang; Boran Han; Bonan Min; Bernie Wang; Cuixiong Hu; Shuai Zhang,Zhuoyan Xu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",Multimodal & Speech/Audio,table; tables; mllms; answer; table understanding; collections; understanding; visual; unique; multimodal large
857-FIND,FedReFT: Federated Representation Fine-Tuning with All-But-Me Aggregation,"Parameter-efficient fine-tuning (PEFT) adapts large pre-trained models by updating only a small subset of parameters. Recently, Representation Fine-Tuning (ReFT) has emerged as an effective alternative. ReFT shifts the fine-tuning paradigm from updating model weights to directly manipulating hidden representations that capture rich semantic information, and outperform state-of-the-art PEFTs in standalone settings. However, its application in Federated Learning (FL) remains challenging due to heterogeneity in clients' data distributions, model capacities, and computational resources. To address these challenges, we introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to fine-tune clients' hidden representations. FedReFT applies sparse intervention layers to steer hidden representations directly, offering a lightweight and semantically rich fine-tuning alternative ideal for edge devices. However, representation-level updates are especially vulnerable to aggregation mismatch under different task heterogeneity, where naive averaging can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me (ABM) aggregation, where each client receives the aggregated updates of others and partially incorporates them, enabling stable and personalized learning by balancing local focus with global knowledge. We further design an adaptive update strategy inspired by Test-Time Computing (TTC) to balance local and global contributions under heterogeneous conditions. FedReFT achieves state-of-the-art performance on commonsense reasoning, arithmetic reasoning, and GLUE benchmarks, while delivering 1xвЂ“49x higher parameter efficiency compared to leading LoRA-based methods.",Fatema Siddika; Md Anwar Hossen; Juan Pablo Munoz; Tanya G. Roosta; Anuj Sharma; Ali Jannesari,Fatema Siddika,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"Efficiency, Scaling & NLP Systems",federated; hidden representations; fine tuning; fine; tuning; representation; aggregation; hidden; heterogeneity; updating
858-FIND,RiddleBench: A New Generative Reasoning Benchmark for LLMs,"While Large Language Models (LLMs) show remarkable capabilities, their complex reasoning skills require deeper investigation. We introduce **RiddleBench**, a new benchmark of 1,737 challenging puzzles designed to test reasoning beyond simple pattern matching. Our evaluation of state-of-the-art models reveals significant limitations, including hallucination cascades (uncritically accepting flawed peer reasoning) and poor self-correction due to strong self-confirmation bias. We also find that model performance is fragile, degrading when faced with reordered constraints or irrelevant information. RiddleBench serves as a resource for diagnosing these issues and guiding the development of more robust LLMs.",Deepon Halder; Alan Saji; Thanmay Jayakumar; Anoop Kunchukuttan; Ratish Puduppully; Raj Dabre,Alan Saji,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",reasoning; self; generative reasoning; investigation introduce; art models reveals; accepting; guiding development; strong self; cascades; benchmark llms
866-FIND,Language Model-Driven Data Pruning Enables Efficient Active Learning,"Active learning (AL) optimizes data labeling efficiency by selecting the most informative instances for annotation. However, scaling active learning to large datasets remains a critical challenge, as AL acquisition functions incur prohibitive computational costs when evaluating large unlabeled data pools. To bridge this gap, we introduce a novel plug-and-play data pruning strategy, ActivePrune, which leverages language models to prune the unlabeled pool. ActivePrune implements a two-stage pruning process: an initial fast evaluation using perplexity scores from an n-gram language model, followed by a high-quality selection using metrics for data quality computed through a quantized LLM. To enhance the diversity of the unlabeled pool, we propose a novel perplexity reweighting method that systematically brings forward underrepresented instances for selection. Experiments on translation, sentiment analysis, topic classification, and summarization tasks on diverse datasets and AL strategies demonstrate that ActivePrune outperforms existing data pruning methods. Finally, we compare the selection quality $\leftrightarrow$ efficiency tradeoff of the data pruning methods and show that ActivePrune provides up to 74% reduction in the end-to-end AL time compared to other LLM score-based pruning methods.",Abdul Hameed Azeemi; Ihsan Ayyub Qazi; Agha Ali Raza,Abdul Hameed Azeemi,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",Machine Translation,pruning; active learning; unlabeled; active; pool; selection; instances; perplexity; learning; quality
869-FIND,HARM: Learning Hate-Aware Reward Model for Evaluating Natural Language Explanations of Offensive Content,"Explaining why content is hateful using natural language is crucial for fostering transparency in automated content moderation systems. However, evaluating the quality of such explanations remains an open challenge. General-purpose reward models (RMs), commonly used for scoring natural language outputs, are typically optimized for broad notions of safety. We argue that this optimization penalizes situations where references to stereotypes or offensive content are essential for faithful hate speech explanations. To address this gap, we introduce SBIC-Explain, a human-validated dataset of 370,788 LLM generated NLEs for offensive content, spanning three levels of human-annotated contextual richness: Tier 1: text-only, Tier 2: + classification-aware, and Tier 3: + semantics-informed. We hypothesize that as human-annotated context increases, explanations should better reflect human preferences. Yet, we find that existing RMs systematically assign lower scores to more contextually rich (and often more offensive) explanations, revealing a misalignment between model preferences and explanatory fidelity for this context. We propose HARM (Hate-Aware Reward Model), a RM that integrates interpretable signals to better align reward scores with the needs of hate speech explanation. HARM outperforms general-purpose baselines, improving NLE pair-wise preference.",Lorenzo Puppi Vecchi; Alceu de Souza Britto Jr.; Emerson Cabrera Paraiso; Rafael M. O. Cruz,Lorenzo Puppi Vecchi,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness",Multimodal & Speech/Audio,explanations; offensive; hate; offensive content; tier; reward; harm; content; rms; aware reward
875-FIND,MATH-IDN: A Multilingual Mathematical Problem Solving Dataset Featuring Local Languages in Indonesia,"Large Language Models (LLMs) excel at mathematical reasoning in English, but their performance in low-resource languages remains underexplored. This gap is particularly critical for Indonesia, where equitable access to AI-powered education depends on robust multilingual reasoning across diverse local languages. We introduce MATH-IDN, a multilingual benchmark for mathematical problem solving in Indonesian, Javanese, Sundanese, and Buginese, with English as reference. Each subset contains expert-verified problems spanning seven subjects and five difficulty levels. We evaluate eight open-source LLM families, including math-specialized, Southeast-Asian-adapted, and general-purpose models, under a zero-shot chain-of-thought setting. Results show that MATH-IDN presents a challenging and discriminative benchmark, revealing substantial performance gaps in low-resource languagesвЂ”particularly for BugineseвЂ”and highlighting key limitations in current multilingual reasoning capabilities. Anonymous link to the data repository: https://anonymous.4open.science/r/MATH_IDN-EA85",Xiao Xiao; Iftitahu Ni'mah; Yuyun Wabula; Mykola Pechenizkiy; Meng Fang,Xiao Xiao,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multilinguality & Low-Resource NLP,"Reasoning, Planning & Agents",math; mathematical; local languages; multilingual; mathematical problem solving; multilingual reasoning; mathematical problem; problem solving; local; solving
887-FIND,Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules,"Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among their specialized experts, which existing Parameter- Efficient Fine-Tuning (PEFT) strategies often fail to leverage. This motivates us to investigate whether adaptation modules themselves should incorporate routing mechanisms to align with MoE's multi-expert architecture. We analyze dynamics of core components when applying PEFT to MoE language models, and examine how different routing strategies affect adaptation effectiveness. Extensive experiments adapting OLMoE-1B-7B and Mixtral-8Г—7B on various commonsense and math reasoning tasks validate the performance and efficiency of our routed approach. We identify optimal configurations for different scenarios and provide empirical analyses with practical insights to facilitate better PEFT and MoE applications.",Yilun Liu; Yunpu Ma; Yuetian Lu; Shuo Chen; Zifeng Ding; Volker Tresp,Yilun Liu,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",moe; peft; routing; mixture; experts; routed; adaptation; mixture experts; modules; parameter efficient
891-FIND,MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference,"Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, and LLM-based agents further extend these abilities to various practical workflows. While recent progress shows that multi-agent systems (MAS) can outperform single agents by coordinating specialized roles, designing effective MAS remains difficult due to prompt sensitivity and the compounded instability MAS creates. To cope with the challenge, recent efforts in automated prompt design have reduced manual effort. However, multi-agent prompt optimization remains largely unexplored. Challenges like exponentially expanding search space and ambiguous credit assignment together make systematic design intractable without principled methods. Therefore, we introduce Multi-Agent PRompt Optimization (MAPRO), a four-stage framework that first formulates MAS prompt optimization as a \textit{Maximum a Posteriori} (MAP) inference problem and solves it using a language-guided variant of max-product belief propagation algorithm. To address credit assignment and updates the system iteratively, MAPRO employs a topology-aware refinement mechanism that integrates execution feedback and downstream blames to selectively update agent prompts. Through this process, MAPRO progressively converges to a coordinated set of agent-specific prompt policies. Across benchmarks in various tasks, MAPRO achieves state-of-the-art performance, consistently surpassing manually engineered baselines and recent automated alternatives. Beyond performance, our MAP-based formulation also delivers general guidelines for building more reliable and principled multi-agent systems in the future.",Zheyuan Zhang; Lin Ge; Hongjiang Li; Weicheng Zhu; Chuxu Zhang; Yanfang Ye,Kaiwen Shi,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",mas; agent; prompt; prompt optimization; multi agent; optimization; multi; assignment; maximum; multi agent systems
892-FIND,Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought,"Despite notable advancements in prompting methods for Large Language Models (LLMs), such as Chain-of-Thought (CoT), existing strategies still suffer from excessive token usage and limited generalisability across diverse reasoning tasks. To address these limitations, we propose an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework, which leverages structural causal models to infer the causal effect of a query on its answer and adaptively select an appropriate intervention (i.e., standard front-door and conditional front-door adjustments). This design enables generalisable causal reasoning across heterogeneous tasks without task-specific retraining. By replacing verbose CoT with concise Sketch-of-Thought, ACPS enables efficient reasoning that significantly reduces token usage and inference cost. Extensive experiments on multiple reasoning benchmarks and LLMs demonstrate that ACPS consistently outperforms existing prompting baselines in terms of accuracy, robustness, and computational efficiency.",Bowen Li; Ziqi Xu; Jing Ren; Renqiang Luo; Xikun Zhang; Xiuzhen Zhang; Yongli Ren; Feng Xia,Bowen Li,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",causal; sketch; thought; prompting; token usage; cot; usage; reasoning; adaptive; enables
894-FIND,ExpressivityBench: Can LLMs Communicate Implicitly?,"Human communication is often implicit, conveying tone, identity, and intent beyond literal meanings. While large language models have achieved strong performance on explicit tasks such as summarization and reasoning, their capacity for expressivity, or implicit communication, remains underexplored. We introduce ExpressivityBench, a framework for evaluating the expressivity of LLMs using information-theoretic communication models. Our approach quantifies how well LLM-generated text communicates target properties without explicit mention, across nine tasks spanning emotion, identity, and tone. To enable scalable and reproducible evaluation, we employ LLM-based graders validated against human judgments. Our results reveal that while models are adept at expressing affective content, they struggle with sociolinguistic signals, lagging behind human baselines. This study provides a necessary step to evaluate human-like implicit communication, with implications for applications such as education, mental health support, and socially-aware dialogue systems. We provide code and data for our benchmark alongside our paper.",Joshua Tint; Som Sagar; Aditya Taparia; Kelly Raines; Bimsara Pathiraja; Caleb Liu; Ransalu Senanayake,Joshua Tint,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Dialogue, Conversational & Interactive NLP",communication; implicit; tone; identity; human; explicit; benchmark alongside; judgments results; health support; mental health support
895-FIND,Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL,"Schema linking---the process of aligning natural language questions with database schema elements---is a critical yet underexplored component of Text-to-SQL systems. While recent methods have focused primarily on improving SQL generation, they often neglect the retrieval of relevant schema elements, which can lead to hallucinations and execution failures. In this work, we propose a context-aware bidirectional schema retrieval framework that treats schema linking as a standalone problem. Our approach combines two complementary strategies: table-first retrieval followed by column selection, and column-first retrieval followed by table selection. It is further augmented with techniques such as question decomposition, keyword extraction, and keyphrase extraction. Through comprehensive evaluations on challenging benchmarks such as BIRD and Spider, we demonstrate that our method significantly improves schema recall while reducing false positives. Moreover, SQL generation using our retrieved schema consistently outperforms full-schema baselines and closely approaches oracle performance, all without requiring query refinement. Notably, our method narrows the performance gap between full and perfect schema settings by 50%. Our findings highlight schema linking as a powerful lever for enhancing Text-to-SQL accuracy and efficiency.",Md Mahadi Hasan Nahid; Davood Rafiei; Weiwei Zhang; Yong Zhang,Md Mahadi Hasan Nahid,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","Retrieval, Grounding & External Knowledge (RAG)",schema; sql; schema linking; linking; text sql; retrieval; sql generation; column; bidirectional; followed
898-FIND,PEAR: Planner-Executor Agent Robustness Benchmark,"Large Language Model (LLM)вЂ“based Multi-Agent Systems (MAS) have emerged as a powerful paradigm for tackling complex, multi-step tasks across diverse domains. However, despite their impressive capabilities, MAS remain susceptible to adversarial manipulation. Existing studies typically examine isolated attack surfaces or specific scenarios, leaving a lack of holistic understanding of MAS vulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for systematically evaluating both the utility and vulnerability of plannerвЂ“executor MAS. While compatible with various MAS architectures, our benchmark focuses on the plannerвЂ“executor structureвЂ”a practical and widely adopted design. Through extensive experiments, we find that (1) a weak planner degrades overall clean task performance more severely than a weak executor; (2) while a memory module is essential for the planner, having a memory module for the executor does not impact the clean task performance; (3) there exists a trade-off between task performance and robustness; and (4) attacks targeting the planner are particularly effective at misleading the system. These findings offer actionable insights for enhancing the robustness of MAS and lay the groundwork for principled defenses in multi-agent settings.",Shen Dong; Mingxuan Zhang; Pengfei He; Li Ma; Bhavani Thuraisingham; Hui Liu; Yue Xing,Shen Dong,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",mas; planner; task performance; memory module; clean; agent; weak; robustness; module; multi agent
903-FIND,Toward Safe and Human-Aligned Game Conversational Recommendation via Multi-Agent Decomposition,"Conversational recommender systems (CRS) have advanced with large language models, showing strong results in domains like movies. These domains typically involve fixed content and passive consumption, where user preferences can be matched by genre or theme. In contrast, games present distinct challenges: fast-evolving catalogs, interaction-driven preferences (e.g., skill level, mechanics, hardware), and increased risk of unsafe responses in open-ended conversation. We propose MATCHA, a multi-agent framework for CRS that assigns specialized agents for intent parsing, tool-augmented retrieval, multi-LLM ranking with reflection, explanation, and risk control which enabling finer personalization, long-tail coverage, and stronger safety. Evaluated on real user request dataset, MATCHA outperforms six baselines across eight metrics, improving Hit@5 by 20%, reducing popularity bias by 24%, and achieving 97.9% adversarial defense. Human and virtual-judge evaluations confirm improved explanation quality and user alignment. Code will be released upon acceptance.",Zheng Hui; Xiaokai Wei; Yexi Jiang; Kevin Gao; Chen Wang; Se-eun Yoon; Rachit Pareek; Michelle Gong,Zheng Hui,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Trustworthy, Safety, Privacy & Fairness","Dialogue, Conversational & Interactive NLP",user; explanation; preferences; conversational; multi agent; risk; multi; agent; mechanics; hit
905-FIND,Linguistic Cues for LLM-based Implicit Discourse Relation Classification,"Large language models (LLMs) have achieved impressive success across many NLP tasks, yet implicit discourse relation classification (IDRC) is still dominated by encoder-only pre-trained language models such as RoBERTa. This may be due to earlier reports that ChatGPT performs poorly on IDRC in zero-shot settings. In this paper, we show that fine-tuned LLMs can perform on par with, or even better than, existing encoder-based approaches. Nevertheless, we find that LLMs alone struggle to capture subtle lexical relations between arguments for the task. To address this, we propose a two-step strategy that enriches arguments with explicit lexical-level semantic cues before fine-tuning. Experiments demonstrate substantial gains, particularly in cross-domain scenarios, with F1 scores improved by more than 10 points compared to strong baselines.",Yi Fan; Michael Strube; Wei Liu,Yi Fan,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Linguistics, Syntax & Semantics",,discourse relation classification; discourse relation; relation classification; arguments; relation; implicit; discourse; lexical; cues; encoder
907-FIND,SpARK: An Embarrassingly Simple Sparse Watermarking in LLMs with Enhanced Text Quality,"With the widespread adoption of Large Language Models (LLMs), concerns about potential misuse have emerged. To this end, watermarking has been adapted to LLM, enabling a simple and effective way to detect and monitor generated text. However, while the existing methods can differentiate between watermarked and unwatermarked text with high accuracy, they often face a trade-off between the quality of the generated text and the effectiveness of the watermarking process. In this work, we present a novel type of LLM watermark, Sparse WatermARK (or SpARK), which aims to mitigate this trade-off by applying watermarks to a small subset of generated tokens distributed across the text. To demonstrate this type of watermark, we introduce two novel variants, SpARK-P and SpARK-R, which achieve sparsity by anchoring watermarked tokens to words that have specific Part-of-Speech (POS) tags and specific hash values w.r.t a pseudorandom hash function, respectively. Our experimental results demonstrate that the proposed watermarking schemes, albeit embarrassingly simple, are incredibly effective, achieving high detectability while generating text that outperforms previous LLM watermarking methods in quality across various tasks. SpARK further advances the watermarking capability for LLMs while maintaining their generated text quality.",Duy Cao Hoang; Thanh Quoc Hung Le; Rui Chu; Ping Li; Weijie Zhao; Yingjie Lao; Khoa D Doan,Hoang Cao Duy,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Trustworthy, Safety, Privacy & Fairness",watermarking; watermark; text; generated text; watermarked; generated; text quality; simple; quality; type
917-FIND,Pretraining Language Models for Diachronic Linguistic Change Discovery,"Large language models (LLMs) are increasingly used as knowledge discovery tools. Humanistic disciplines like historical linguistics and literary studies have shown interest in this capability. These fields often construct arguments on the basis of distinctions between phenomena like time-period or genre. Such methodological investments complicate reliance on LLMs pretrained over large sets of broadly-collected data. We show that efficient pretraining techniques produce useful models of semantic change over modest historical corpora without allowing potential contamination from anachronistic data. We verify that these trained-from-scratch models better respect historical divisions and are more computationally efficient compared to the standard approach of fine-tuning an existing LLM. We compare the trade-offs in general linguistic fluency versus detecting and characterizing various forms of linguistic change, and provide a pipeline implementation of our approach that can be readily adapted and applied to a wide range of diachronic phenomena.",Elisabeth Fittschen; Sabrina Xin Li; Tom Lippincott; Leshem Choshen; Craig Messner,Elisabeth Fittschen,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics",,change; historical; diachronic; phenomena; discovery; pretraining; linguistic; pretrained large; used knowledge; increasingly used knowledge
919-FIND,Improving Language Identification for Code-Switched Speech: The Pivotal Role of Accented English,"Code-switching, where speakers alternate between languages within a single utterance, poses unique challenges for language identification (LID). Existing LID models often fail to reliably identify English spoken with the accent of the matrix (dominant) language. We show that finetuning LID models with small amounts of such accented English significantly improves code-switched LID, without degrading performance on standard monolingual speechвЂ”a limitation observed with direct finetuning on code-switched utterances. This is achieved via low-rank adaptation (LoRA) on limited accented data, which allows models to adapt efficiently. To better evaluate performance, we introduce LangRank, a metric that captures the relative ranking of identified languages often overlooked by traditional metrics. Our method generalizes across multiple language pairs, including Hindi-English, Bengali-English, Mandarin-English, and Arabic-English, providing robust LID in code-switched multilingual contexts.",Adyasha Patra; Dhiraj Kumar Sah; Preethi Jyothi,Adyasha Patra,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multilinguality & Low-Resource NLP,Multimodal & Speech/Audio,lid; code switched; switched; english; accented; code; language identification; finetuning; identification; models adapt
920-FIND,Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval,"The ability to generate SPARQL queries from natural language questions is crucial for ensuring efficient and accurate retrieval of structured data from knowledge graphs (KG). While large language models (LLMs) have been widely adopted for SPARQL query generation, they are often susceptible to hallucinations and out-of-distribution errors when generating KG elements, such as Uniform Resource Identifiers (URIs), based on opaque internal parametric knowledge. We propose PGMR (Post-Generation Memory Retrieval), a modular framework where the LLM produces an intermediate query using natural language placeholders for URIs, and a non-parametric memory module is subsequently employed to retrieve and resolve the correct KG URIs. PGMR significantly enhances query correctness (SQM) across various LLMs, datasets, and distribution shifts, while achieving the near-complete suppression of URI hallucinations. Critically, we demonstrate PGMR's superior safety and robustness: a retrieval confidence threshold enables PGMR to effectively refuse to answer queries that lack support, and the retriever proves highly resilient to memory noise, maintaining strong performance even when the non-parametric memory size is scaled up to 9 times with irrelevant, distracting entities.",Aditya Sharma; Christopher Pal; Amal Zouaq,Aditya Sharma,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,memory; parametric; query; parametric memory; hallucinations; query generation; retrieval; memory retrieval; generation; post
921-FIND,Jailbreaking Safeguarded Text-to-Image Models via Large Language Models,"Text-to-Image models may generate harmful content, such as pornographic images, particularly when unsafe prompts are submitted. To address this issue, safety filters are often added on top of text-to-image models, or the models themselves are aligned to reduce harmful outputs. However, these defenses remain vulnerable when an attacker strategically designs adversarial prompts to bypass these safety guardrails. In this work, we propose PromptTune, a method to jailbreak text-to-image models with safety guardrails using a fine-tuned large language model. Unlike other query-based jailbreak attacks that require repeated queries to the target model, our attack generates adversarial prompts efficiently after fine-tuning our AttackLLM. We evaluate our method on three datasets of unsafe prompts and against five safety guardrails. Our results demonstrate that our approach effectively bypasses safety guardrails, outperforms existing no-box attacks, and also facilitates other query-based attacks.",Zhengyuan Jiang; Yuepeng Hu; Yuchen Yang; Yinzhi Cao; Neil Zhenqiang Gong,Zhengyuan Jiang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",,guardrails; image models; text image models; text image; safety; image; prompts; unsafe prompts; attacks; adversarial prompts
923-FIND,BSCodec: A Band-Split Neural Codec for High-Quality Universal Audio Reconstruction,"Neural audio codecs have recently enabled high-fidelity reconstruction at high compression rates, especially for speech. However, speech and non-speech audio exhibit fundamentally different spectral characteristics: speech energy concentrates in narrow bands around pitch harmonics (80-400 Hz), while non-speech audio requires faithful reproduction across the full spectrum, particularly preserving higher frequencies that define timbre and texture. This poses a challengeвЂ”speech-optimized neural codecs suffer degradation on music or sound. Treating the full spectrum holistically is suboptimal: frequency bands have vastly different information density and perceptual importance by content type, yet full-band approaches apply uniform capacity across frequencies without accounting for these acoustic structures. To address this gap, we propose **BSCodec** (Band-Split Codec), a novel neural audio codec architecture that splits the spectral dimension into separate bands and compresses each band independently. Experimental results demonstrate that BSCodec achieves superior reconstruction over baselines across sound and music, while maintaining competitive quality in the speech domain, when trained on the same combined dataset of speech, music and sound. Downstream benchmark tasks further confirm that BSCodec shows strong potential for use in downstream applications.",Haoran Wang; Jiatong Shi; Jinchuan Tian; Bohan Li; Kai Yu; Shinji Watanabe,Haoran Wang,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multimodal & Speech/Audio,"Efficiency, Scaling & NLP Systems",speech; audio; music; reconstruction; sound; neural; non speech audio; speech audio; non speech; split
927-FIND,Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization,"Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs.",Jiwei Guan; Haibo Jin; Haohan Wang,Jiwei Guan,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness",Multimodal & Speech/Audio,box; lvlms; black box; black; adversarial; jailbreak; minigpt; safety mechanisms; white box; white
929-FIND,SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph,"Large Language Models (LLMs) have demonstrated remarkable capabilities, enabling language agents to excel at single-turn tasks. However, their application to complex, multi-step, and long-horizon tasks remains challenging. While reinforcement learning (RL) offers a promising avenue for addressing these challenges, mainstream approaches typically rely solely on sparse, outcome-based rewards вЂ” a limitation that becomes especially problematic for group-based RL algorithms lacking critic models, such as Group Relative Policy Optimization (GRPO). In such methods, uniformly rewarding or penalizing all actions within a trajectory can lead to training instability and suboptimal policies, because beneficial and detrimental actions are often entangled across multi-step interactions. To address this challenge, we propose SALT, a novel and lightweight framework that provides a finer-grained advantage assignment, derived solely from outcome rewards. We achieve this by constructing a graph from trajectories of the same prompt, which allows us to quantify the quality of each step and assign advantages accordingly. Crucially, SALT is designed as a plug-and-play module that seamlessly integrates with existing group-based RL algorithms вЂ” requiring no modifications to the rollout procedure and introducing negligible computational overhead. Extensive experiments on the WebShop, ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that SALT consistently improves performance. We also conduct a thorough analysis to validate the design choices behind SALT and offer actionable insights.",Jiazheng Li; Yawei Wang; Qiaojing Yan; Yijun Tian; Zhichao Xu; Huan Song; Panpan Xu; Lin Lee Cheong,Jiazheng Li,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Interpretability & Model Analysis,group; step; long horizon; assignment; horizon; outcome; trajectory; advantage; actions; solely
930-FIND,"UniToolBench: A Benchmark for Tool-Augmented LLMs in Cross-Domain, Universal Task Automation","Recent advancements in Large Language Models (LLMs) have enabled autonomous agents to decompose complex tasks, select appropriate tools, and execute structured workflows. However, a key challenge in this field is the lack of a universal, large-scale, and cross-domain benchmark to systematically evaluate LLMs' ability to reason over and utilize interconnected tools for automation. Existing benchmarks, such as TaskBench, focus on manually curated tool graphs for benchmark generation, which lack scalability and diversity across domains. To address this, we propose UniToolBench, a benchmark that incorporates automated tool graph construction by formulating link prediction as a probabilistic task, instead of relying on categorical LLM outputs. Furthermore, we introduce a confidence-based beam search sampling strategy to select high-confidence tool dependencies, ensuring more structured and semantically coherent subgraphs for evaluation. Through extensive experiments on multiple datasets, we demonstrate that while LLMs show promise in tool selection, significant challenges remain in parameter prediction and handling complex tool dependencies.",Xiaojie Guo; Yang Zhang; Bing Zhang; Ryo Kawahara; Mikio Takeuchi; Yada Zhu,Xiaojie Guo,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",tool; universal; cross domain; automation; benchmark; select; dependencies; confidence; prediction; tools
931-FIND,Benchmarking the Energy Savings with Speculative Decoding Strategies,"Speculative decoding has emerged as an effective method to reduce latency and inference cost of LLM inferences. However, there has been inadequate attention towards the energy requirements of these models. To address this gap, this paper presents a comprehensive survey of energy requirements of speculative decoding strategies, with detailed analysis on how various factors -- model size and family, speculative decoding strategies, and dataset characteristics -- influence the energy optimizations.",Rohit Dutta; Paramita Koley; Soham Poddar; Janardan Misra; Sanjay Podder; Naveen Balani; Saptarshi Ghosh; Niloy Ganguly,Rohit Dutta,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",Interpretability & Model Analysis,speculative; speculative decoding; energy; decoding strategies; decoding; requirements; strategies; detailed analysis; decoding emerged; speculative decoding emerged
933-FIND,NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding,"Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce **NUBench**, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.",Yeonkyoung So; Gyuseong Lee; Sungmok Jung; Joonhak Lee; JiA Kang; Sangho Kim; Jaejin Lee,Yeonkyoung So,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics",Summarization & Generation,negation; understanding; sentence; sentence level; level; benchmark; goes; goes surface; particularly tasks; goes surface level
937-FIND,What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance,"Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent ""risk landscape"": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.",William Watson; Nicole Cho; Sumitra Ganesh; Manuela Veloso,Nicole Cho,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Interpretability & Model Analysis,"Retrieval, Grounding & External Knowledge (RAG)",query; hallucination; answerability; intention; clause; certain; feature; grounding; risk; align
950-FIND,Completely Modular Fine-tuning for Dynamic Language Adaptation,"Multilingual Fine-tuning of Large Language Models (LLMs) has achieved great advancements in machine translation. However, existing research focuses only on the traditional fine-tuning setting with a fixed set of languages, lacking dynamic adaptability to new ones. Introducing new languages requires retraining and often causes catastrophic forgetting. In this study, we propose a completely modular fine-tuning pipeline that enables dynamic language adaptation for LLMs. Instead of directly fine-tuning on all languages, our approach first trains English-centric input and output LoRA adapters for each language separately, and then merges the corresponding adapters for arbitrary-direction translation without any additional training. Experiments on 12 translation directions of four low-resource and less-supported languages show that modular fine-tuning achieves up to 86% performance of traditional multi-parallel full-parameter fine-tuning, while training only 0.1% parameters and relying solely on English-centric data without any catastrophic forgetting. Furthermore, we perform a comprehensive analysis about the merging ratio, when to merge, and the rationale for using English as a bridge language via Bayesian Optimization and logit lens.",Zhe Cao; Yusuke Oda; Qianying Liu; Akiko Aizawa; Taro Watanabe,Zhe Cao,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Machine Translation,Multilinguality & Low-Resource NLP,fine tuning; tuning; fine; modular; language adaptation; completely; languages; dynamic; translation; catastrophic
960-FIND,A Multi-Task Learning Framework for Modeling Engagement and Topic-Sensitive Responses in Arabic WomenвЂ™s Discourse,"Predicting how audiences react to Arabic social media posts requires reasoning beyond textual sentiment: reactions emerge from collective interpretation moderated by engagement dynamics and topical context. We present a multi-task learning (MTL) framework that jointly learns (i) audience reaction classification (Love, Haha, Angry, Sad, Care, Wow), (ii) engagement magnitude regression (six reactions, comments, shares), and (iii) non-engagement detection. On a corpus of 158k Arabic Facebook posts spanning womenвЂ™s rights, gender debates, and economic empowerment, our model achieves a test macro-F1 of 72.4 and weighted-F1 of 89.1.",Mabrouka bessghaier; Md. Rafiul Biswas; Shimaa Ibrahim; Wajdi Zaghouani,Wajdi Zaghouani,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics","Reasoning, Planning & Agents",engagement; arabic; task learning; multi task learning; posts; multi task; framework jointly; social media posts; framework modeling; learning framework modeling
962-FIND,We Are What We Repeatedly Do: Improving Long Context Instruction Following,"Large language model context lengths have grown rapidly in recent years, from 512 tokens in GPT to 2M tokens in Gemini 1.5 Pro. Larger context windows enable models to condition on significantly more input tokens, leading to higher quality responses for some user prompts. However, longer contexts also pose challenges to system instruction adherence. In this work, we formalize verifiable instructions to evaluate model compliance based on clear, measurable criteria. From this criteria, we present **VerIFY**, a **Ver**ifiable **I**nstruction **F**ollowing **Y**ardstick dataset designed to benchmark the compliance and accuracy of LLMs in adhering to various types of instructions across multi-turn, long-context conversations. From experiments with open-source models, we reveal insights into instruction-following failures in long contexts, helping to improve the reliability, safety, and precision of these models. Furthermore, we implement and evaluate six mitigation strategies to enhance instruction compliance in extended contexts, achieving an improvement up to 79%. This is the first work to consider instruction following for multi-turn, long context conversations.",Preston K Robinette; Andrew Hard; Swaroop Ramaswamy; Ehsan Amid; Rajiv Mathews; Taylor T Johnson,Preston Robinette,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Dialogue, Conversational & Interactive NLP",instruction; context; long context; compliance; long; instruction following; following; contexts; tokens; multi turn
967-FIND,ConRAS: Contrastive In-context Learning Framework for Retrieval-Augmented Summarization,"Contrastive learning (CL) has achieved remarkable progress in natural language processing (NLP), primarily as a paradigm for pre-training and fine-tuning. However, its potential during the generation phase, particularly in in-context learning (ICL)-based retrieval-augmented summarization, remains largely unexplored. While previous studies have attempted to incorporate negative samples into ICL prompts, these methods do not enforce a true contrastive objective that encourages separation of positive and negative samples in the representation space. In this paper, we first demonstrate through preliminary experiments that small language models (SLMs) can interpret contrastive prompts and effectively distinguish between positive and negative samples during inference, without any parameter updates. Building on these findings, we propose ConRAS, a novel framework that injects contrastive objectives into ICL-based retrieval-augmented summarization. Extensive experiments and in-depth analysis on three summarization benchmarks using four SLMs show that ConRAS consistently outperforms state-of-the-art retrieval-augmented methods, achieving significant improvements in summary quality.",Juseon Do; Sungwoo Han; Jingun Kwon; Hidetaka Kamigaito; Manabu Okumura,Sungwoo Han,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,Interpretability & Model Analysis,contrastive; negative samples; summarization; icl; retrieval augmented; negative; augmented; positive negative samples; based retrieval augmented; retrieval
968-FIND,Beyond Sampling: Self-Sorting for Long-Context Ranking,"Ranking is a fundamental component in a wide range of AI applications. However, large language models (LLMs) remain unstable on long-context ranking. Sliding-window processing is costly and listwise prompting over full candidates still yields inconsistent orders. We show that sampling alone, even with selection-based methods, cannot stabilize ranking because LLM consistency decomposes into within-list order and cross-list preference, in which a single stochastic process cannot align. To address this, we introduce Self-Sorting (SS), which generates m candidate lists and performs n selection-time re-rankings over those lists. SS fuses explicit within-list positions with implicit cross-list preferences to score entities and return a top-k set. Experimental results on five widely used ranking benchmarks show significant improvements in nDCG@{1,5,10}, highlighting the critical role of implicit consistency.",Juseon Do; Sungwoo Han; Jingun Kwon; Hidetaka Kamigaito; Katsuhiko Hayashi; Taro Watanabe,Sungwoo Han,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",,ranking; list; lists; long context; sampling; implicit; selection; consistency; self; long
976-FIND,Program-of-Thought Reveals LLM Abstraction Ceilings,"Large language models (LLMs) are often claimed to exhibit reasoning ability when supervised with chain-of-thought (CoT) traces. True reasoning, however, requires invariance: isomorphic problems should yield identical solutions regardless of superficial variation. We test this property by evaluating base and reasoning-optimized models, including LLaMA, Mistral, Qwen, GPT-OSS, and Deepseek, on GSM8K and MATH isomorphic variants of the same problem. All models exhibit notable drops under perturbation. To test whether training can induce invariance, we fine-tune models with PoT supervision under both concrete and abstract prompt formulations. PoT reduces surface instability but yields only marginal accuracy gains and does not transfer to CoT evaluation. These findings suggest PoT confers template-level consistency but fails to robustly ground operator semantics, yielding consistency without correctness. These results indicate that current LLM fine-tuning practices reinforce structural imitation rather than true reasoning.",Mike Zhou; Fenil Bardoliya; Vivek Gupta; Dan Roth,Mike Zhou,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",true; cot; reasoning; thought; consistency; exhibit; test; results indicate current; models exhibit notable; operator
978-FIND,From Numbers to Narratives: Efficient Language Model-Based Detection for Safety-Critical Minority Classes,"Safety-critical classification tasks face a persistent challenge: traditional models achieve high overall accuracy but inadequate performance on critical minority classes. We introduce a numbers to narratives framework that transforms tabular data into contextually rich descriptions, enabling language models to leverage pre-trained knowledge for minority class detection. Our approach integrates structured verbalization, linguistically-informed augmentation, and parameter-efficient fine-tuning to address the ""minority class blind spot'' in high-consequence domains. Using a significantly more efficient model architecture than existing approaches, our framework achieves superior minority class F1-scores: 78.76% for machine failures (+7.42 points over XGBoost), 65.87% for at-risk students (+12.12 points over MLP), and 32.00% for semiconductor failures (+1.01 points over XGBoost, despite 14:1 class imbalance). Our approach also improves overall accuracy by up to 22.43% in five of six datasets while maintaining computational feasibility. Ablation studies confirm that narrative-based verbalization enables effective reasoning about tabular data by contextualizing abstract numerical features. This work provides a practical, resource-efficient approach for enhancing minority class performance in safety-critical domains.",Ahatsham Hayat; Hunter Tridle; Mohammad Rashedul Hasan,Ahatsham Hayat,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","Reasoning, Planning & Agents",minority; class; safety critical; overall accuracy; points; verbalization; efficient; critical; numbers; classes
979-FIND,R-GDA: Reflective Guidance Data Augmentation with Multi-Agent Feedback for Domain-Specific Named Entity Recognition,"Domain-specific Named Entity Recognition (NER) often requires data augmentation due to the scarcity of annotated corpora. Guidance Data Augmentation (GDA), a method utilizing Large Language Models (LLMs) to decompose sentences into abstract components, can lead to over-abstraction, resulting in undefined entity tags and sentences lacking domain-specific vocabulary. In this work, we propose Reflective GDA (R-GDA), a framework that introduces a multi-agent feedback loop to enhance augmentation quality. R-GDA incorporates two distinct agents: a **Guidance Refiner (GR)**, which assesses the initial abstraction to prevent over-generalization, and an **Augmentation Calibrator (AC)**, which validates the final generated sample for domain-fidelity and tag integrity. On the SciERC and NCBI-disease datasets, R-GDA improves F1-Score, validating its effectiveness. Concurrently, it achieves low BERTScore in most cases, indicating greater sentence diversity. For the FIN dataset, it achieves performance comparable to the GDA baseline. R-GDA consistently prevents errors regarding domain-specific tags, demonstrating that the reflective feedback mechanism enhances data fidelity by mitigating critical generation errors.",Hyeonseok Kang; Hyuk Namgoong; Goun pyeon; Sangkeun Jung,HyeonseokKang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Information Extraction & Structured Prediction,Summarization & Generation,augmentation; reflective; data augmentation; domain specific; domain; agent feedback; guidance; tags; feedback; entity
982-FIND,Enabling Autoregressive Models to Fill In Masked Tokens,"Historically, LLMs have been trained using either autoregressive (AR) or masked language modeling (MLM) objectives, with AR models gaining dominance in recent years. However, AR models are inherently incapable of masked infilling, which is the ability to predict masked tokens between past and future context. In contrast, MLM models suffer from intrinsic computational inefficiencies during both training and inference that hinder their scalability. This work introduces MARIA (Masked and Autoregressive Infilling Architecture), a novel approach that leverages the strengths of both paradigms to achieve state-of-the-art masked infilling performance. MARIA combines a pre-trained MLM and AR model by training a linear decoder that takes their concatenated hidden states as input. This minimal modification enables the AR model to perform infilling while retaining its inherent advantages in terms of faster inference with KV caching. Our results demonstrate that MARIA significantly outperforms existing methods, namely discrete diffusion models, on masked infilling tasks.",Daniel Mingyi Israel; Aditya Grover; Guy Van den Broeck,Daniel Israel,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",,masked; infilling; mlm; autoregressive; ar models; tokens; autoregressive ar; discrete diffusion; novel approach leverages; hinder scalability
985-FIND,Position Encoding with Random Float Sampling Enhances Length Generalization of Transformers,"Length generalization is the ability of language models to maintain performance on inputs longer than those seen during pretraining. In this work, we introduce a simple yet powerful position encoding (PE) strategy, Random Float Sampling (RFS), that generalizes well to lengths unseen during pretraining or fine-tuning. In particular, instead of selecting position indices from a predefined discrete set, RFS uses randomly sampled continuous values, thereby avoiding out-of-distribution (OOD) issues on unseen lengths by exposing the model to diverse indices during training. Since assigning indices to tokens is a common and fundamental procedure in widely used PEs, the advantage of RFS can easily be incorporated into, for instance, the absolute sinusoidal encoding, RoPE, and ALiBi. Experiments corroborate its effectiveness by showing that RFS results in superior performance in length generalization tasks as well as zero-shot commonsense reasoning benchmarks.",Atsushi Shimizu; Shohei Taniguchi; Yutaka Matsuo,Atsushi Shimizu,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Trustworthy, Safety, Privacy & Fairness","Reasoning, Planning & Agents",length generalization; indices; encoding; position; length; lengths; generalization; pretraining; random; sampling
987-FIND,Open-Domain Safety Policy Construction,"Safety policies are critical for content moderation of online communities, but crafting them remains costly and labor-intensive. We present Deep Policy Research (DPR), an agentic framework that leverages large language models (LLMs) and web search to automatically construct domain-specific safety policies. Given only a high-level domain specification, DPR iteratively proposes subtopics, performs open-domain search, and distills retrieved information into structured policy rules. We evaluate DPR across two domains: (1) online content moderation using the OpenAI content moderation dataset and (2) multimodal advertisement moderation with an in-house benchmark dataset. Our results show that DPR-generated policies consistently outperform simple baselines and are on par with human written policies in terms of effectiveness, suggesting that LLM-based open-domain research offers a promising avenue for bootstrapping safety policies across diverse applications.",Di Wu; Siyue Liu; Zixiang Ji; Ya-Liang Chang; Zhe-Yu Liu; Andrew Pleffer; Kai-Wei Chang,Di Wu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness",Multimodal & Speech/Audio,policies; moderation; open domain; content moderation; safety; domain; policy; content; open; online
990-FIND,Think Just Enough: Leveraging Self-Assessed Confidence for Adaptive Reasoning in Language Models,"Recent reinforcement learning (RL)-trained language models have demonstrated strong performance on complex reasoning tasks by producing long and detailed reasoning traces. However, despite these advancements, they often struggle with finding the right balance in reasoning length: some terminate prematurely before reaching a correct answer (underthinking), while others continue reasoning beyond necessity, leading to inefficiency or even degraded accuracy (overthinking). To address these challenges, we propose a method for optimizing reasoning length via self-assessed confidence. By prompting the model to evaluate its own confidence at intermediate reasoning steps, we enable dynamic stopping once sufficient reasoning is achieved. Experiments across multiple reasoning benchmarks show that our approach improves computational efficiency without compromising answer quality. Furthermore, we find that confidence estimates from RL-trained reasoning models are more reliable than those from standard LLMs, making it a valuable internal signal for controlling reasoning depth.",Junyeob Kim; Sang-goo Lee; Taeuk Kim,Junyeob Kim,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",reasoning; confidence; reasoning length; rl trained; assessed; length; self; answer; multiple reasoning benchmarks; degraded accuracy
999-FIND,CLICKER: Cross-Lingual Knowledge Editing via In-Context Learning with Adaptive Stepwise Reasoning,"As large language models (LLMs) are increasingly deployed as multilingual services, keeping their factual knowledge accurate across languages has become both essential and challenging. However, most of the existing knowledge editing (KE) methods are static, in that they update parameters offline for given accumulated edits of knowledge, and are struggling to effectively propagate edits in one language to others, while avoiding side effects. To mitigate this issue, we propose **CLICKER**, a KE method with stepwise reasoning that dynamically retrieves only knowledge relevant to a given query and then edit, while maintaining cross-lingual consistency through: (1) relevance-aware knowledge retrieval, (2) on-demand in-context KE, and (3) language alignment of the outputs. To rigorously evaluate the locality of edits in cross-lingual KE, we develop **Multi-CounterFact** dataset that contain many semantically-similar but irrelevant prompts for the edit. Experiments on Multi-CounterFact and MzsRE with both open- and closed-source LLMs confirmed that CLICKER effectively localizes edits and resolves cross-lingual inconsistencies, outperforming dynamic KE baselines.",Zehui Jiang; Xin Zhao; Yuta Kumadaki; Naoki Yoshinaga,Zehui Jiang,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multilinguality & Low-Resource NLP,"Retrieval, Grounding & External Knowledge (RAG)",edits; cross lingual; lingual; knowledge; stepwise reasoning; cross; knowledge editing; edit; stepwise; editing
1000-FIND,Show or Tell? Modeling the evolution of request-making in Human-LLM conversations,"Designing user-centered LLM systems requires understanding how people use them, but patterns of user behavior are often masked by the variability of queries. In this work, we introduce a new framework to describe request-making that segments user input into request content, roles assigned, query-specific context, and the remaining task-independent expressions. We apply the workflow to create and analyze a dataset of 211k real-world queries based on WildChat. Compared with similar human-human setups, we find significant differences in the language for request-making in the human-LLM scenario. Further, we introduce a novel and essential perspective of diachronic analyses with user expressions, which reveals fundamental and habitual user-LLM interaction patterns beyond individual task completion. We find that query patterns evolve from early ones emphasizing sole requests to combining more context later on, and individual users explore expression patterns but tend to converge with more experience. From there, we propose to understand communal trends of expressions underlying distinct tasks and discuss the preliminary findings. Finally, we discuss the key implications for user studies, computational pragmatics, and LLM alignment.",Shengqi Zhu; Jeffrey Rzeszotarski; David Mimno,Shengqi Zhu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Linguistics, Syntax & Semantics",request; user; expressions; patterns; human llm; discuss; making; llm; human; individual
1002-FIND,Cards Against Contamination: TCG-Bench for Difficulty-Scalable Multilingual LLM Reasoning,"Benchmarks for language models have become essential tools for research. Yet, such benchmarks face a persistent contamination problem, with recent studies finding 25-50% of evaluation datasets appearing in training corpora. This is true even looking at the two-player zero-sum game setting, where most benchmarks are based on popular games, like chess, whose optimal strategies are all over the web. Such contamination hinders the possibility to differentiate memorization and reasoning skills. To rectify these problems, we introduce TCG-Bench, a benchmark based on a new two-player trading card game (TCG), similar in spirit to games like Magic: The Gathering. TCG-Bench offers three key innovations: (1) a contamination-resistant design by separating the publicly released game engine from hidden card implementations, (2) a continuous difficulty spectrum via Monte Carlo simulation that prevents benchmark saturation, and (3) a parallel implementation in English and Arabic, the first multilingual text-based game benchmark to do so. We also formalize a practical threat model and refresh protocol that preserves evaluation integrity even if specific cards leak. Our analysis across 17 models (50,000+ games) reveals that performance declines exponentially with difficulty, while model size correlates only weakly with strategic ability. We also observe cross-linguistic performance gaps between English and Arabic, with a gap of 47.4% at 32B, highlighting the need for multilingual game benchmarks that target reasoning capabilities in the target language. We host a leaderboard showcasing these results and welcome evaluation requests on our private cards.",Sultan AlRashed; Jianghui Wang; Francesco Orabona,Sultan Alrashed,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics",,game; contamination; cards; games; bench; player; english arabic; difficulty; benchmarks; multilingual
1004-FIND,Multilingual Self-Taught Faithfulness Evaluators,"The growing use of large language models (LLMs) has increased the need for automatic evaluation systems, particularly to address the challenge of information hallucination. Although existing faithfulness evaluation approaches have shown promise, they are predominantly English-focused and often require expensive human-labeled training data for fine-tuning specialized models. As LLMs see increased adoption in multilingual contexts, there is a need for accurate faithfulness evaluators that can operate across languages without extensive labeled data. This paper presents STEMF (Self-Taught Evaluators for Multilingual Faithfulness), a framework that learns exclusively from synthetic multilingual data while leveraging cross-lingual transfer learning. Through experiments comparing language-specific and mixed-language fine-tuning approaches, we demonstrate a consistent relationship between an LLM's general language capabilities and its performance in language-specific evaluation tasks. Our framework shows improvements over existing baselines, including state-of-the-art English evaluators and machine translation-based approaches.",Carlo Alfano; Aymen Al Marjani; Zeno Jonke; Amin Mantrach; Saab Mansour; Marcello Federico,Saab Mansour,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Multilinguality & Low-Resource NLP,Machine Translation,evaluators; faithfulness; taught; multilingual; language specific; labeled; increased; approaches; self; evaluation
1014-FIND,Benchmarking Direct Preference Optimization for Medical Large VisionвЂ“Language Models,"Large vision-language models (LVLMs) are gaining traction in clinical tasks such as diagnostic support, report generation, and medical question answering. Among post-training techniques, Direct Preference Optimization (DPO) has shown promise in aligning model outputs with human preferences, yet its effectiveness in high-stakes medical contexts remains underexplored. In this work, we present the first systematic evaluation of nine DPO variants applied to two leading medical LVLMs, LLaVA-Med and HuatuoGPT-Vision. We benchmark these models on five curated datasets covering diverse clinical tasks. Evaluations include both automated metrics and expert assessments. Our results show that while DPO improves alignment and reduces severe hallucinations, it yields inconsistent gains over supervised fine-tuning. We further introduce DPO variant that better handles visual misinterpretations and enhances clinical understanding. These findings reveal both the potential and limitations of DPO in medical AI. To support future research, we will release all DPO training data, model checkpoints, and expert annotations upon acceptance.",Dain Kim; Jiwoo Lee; Jaehoon Yun; Yong Hoe Koo; Qingyu Chen; Hyunjae Kim; Jaewoo Kang,Dain Kim,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),"LLM Evaluation, Benchmarks & Metrics",dpo; medical; clinical; direct preference; direct preference optimization; lvlms; preference optimization; direct; preference; expert
1018-FIND,Stay Focused: Problem Drift in Multi-Agent Debate,"Multi-agent debate вЂ“ multiple instances of large language models discussing problems in turn-based interaction вЂ“ has shown promise for solving knowledge and reasoning tasks. However, these methods show limitations when solving complex problems that require longer reasoning chains. We analyze how multi-agent debate drifts away from the initial problem over multiple turns, thus harming task performance. We define this phenomenon as problem drift and quantify its presence across ten tasks (i.e., three generative, three knowledge, three reasoning, and one instruction-following task). We find that generative tasks drift often due to the subjectivity of the answer space (76-89%), compared to high-complexity tasks (7-21%). To identify the reasons, eight human experts analyze 170 multi-agent debates suffering from problem drift. We find the most common issues related to this drift are the lack of progress (35% of cases), low-quality feedback (26% of cases), and a lack of clarity (25% of cases). We propose DRIFTJudge, an LLM-as-a-judge method, as a first baseline to detect problem drift. We also propose DRIFTPolicy, which mitigates 31% of problem drift cases. Our study is a step toward understanding a key limitation of multi-agent debate, highlighting why longer debates can harm task performance and how problem drift could be addressed.",Jonas Becker; Lars Benedikt Kaesberg; Andreas Stephan; Jan Philip Wahle; Terry Ruas; Bela Gipp,Jonas Becker,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents","Dialogue, Conversational & Interactive NLP",drift; agent debate; multi agent debate; problem; debate; multi agent; agent; cases; multi; knowledge reasoning
1022-FIND,FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation,"We present FLUKE (Framework for LingUistically-driven and tasK-agnostic robustness Evaluation), a framework for assessing model robustness through systematic minimal variations of test data. FLUKE introduces controlled variations across linguistic levels --- from orthography to dialect and style --- and leverages large language models (LLMs) with human validation to generate modifications. We demonstrate FLUKE's utility by evaluating both fine-tuned models and LLMs across six diverse NLP tasks (four classification and two generation tasks), and reveal that (1) the impact of linguistic variations is highly task-dependent, with some tests being critical for certain tasks but irrelevant for others; (2) LLMs still exhibit significant brittleness to certain linguistic variations, with reasoning LLMs surprisingly showing less robustness on some tasks compared to base models; (3) models are overall more brittle to natural, fluent modifications such as syntax or style changes (and especially to negation), compared to corruption-style tests such as letter flipping; (4) the ability of a model to use a linguistic feature in generation does not correlate to its robustness to this feature on downstream tasks. These findings highlight the importance of systematic robustness testing for understanding model behaviors.",Yulia Otmakhova; Thinh Hung Truong; Rahmad Mahendra; Zenan Zhai; Rongxin Zhu; Daniel Beck; Jey Han Lau,Yulia Otmakhova,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Linguistics, Syntax & Semantics",Summarization & Generation,robustness; variations; linguistic variations; robustness evaluation; linguistic; style; linguistically; modifications; tasks; tests
1025-FIND,Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic Activation for LLMs,"Dense large language models (LLMs) face critical efficiency bottlenecks, as they rigidly activate all parameters regardless of input complexity. While existing sparsity methods (static pruning or dynamic activation) partially address this issue, they either lack adaptivity to contextual or model structural demands or incur prohibitive computational overhead. Inspired by the human brainвЂ™s dual-process mechanisms вЂ” predictive coding (N400) for backbone sparsity and structural reanalysis (P600) for complex contexts вЂ” we propose \textbf{CLADA}, a \textit{\textbf{C}ognitive-\textbf{L}oad-\textbf{A}ware \textbf{D}ynamic \textbf{A}ctivation} framework that synergizes statistical sparsity with semantic adaptability. Our key insight is that LLM activations exhibit two complementary patterns: 1. \textit{Global Statistical Sparsity} driven by sequence-level prefix information, and 2. \textit{Local Semantic Adaptability} modulated by cognitive load metrics (e.g., surprisal and entropy). CLADA employs a hierarchical thresholding strategy: a baseline derived from offline error-controlled optimization ensures over 40% sparsity, which is then dynamically adjusted using real-time cognitive signals. Evaluations across six mainstream LLMs and nine benchmarks demonstrate that CLADA achieves \textbf{20% average speedup with less than 2% accuracy degradation}, outperforming Griffin (over 5% degradation) and TT (negligible speedup). Crucially, we establish the first formal connection between neurolinguistic event-related potential (ERP) components and LLM efficiency mechanisms through multi-level regression analysis ($R^2 = 0.17$), revealing a sparsity--adaptation synergy. Requiring no retraining or architectural changes, CLADA provides a deployable solution for resource-aware LLM inference while advancing biologically inspired AI design.",Yiheng Yang; Yujie Wang; Chi Ma; Lei Yu; Emmanuele Chersoni; Chu-Ren Huang,Unknown,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Efficiency, Scaling & NLP Systems",Interpretability & Model Analysis,textbf; sparsity; textit; cognitive; cognitive load; dynamic activation; load; speedup; statistical; adaptability
1028-FIND,PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing,"Language models (LMs) may memorize personally identifiable information (PII) from training data, enabling adversaries to extract it during inference. Existing defense mechanisms such as differential privacy (DP) reduce this leakage, but incur large drops in utility. Based on a comprehensive study using circuit discovery to identify the computational circuits responsible PII leakage in LMs, we hypothesize that specific PII leakage circuits in LMs should be responsible for this behavior. Therefore, we propose PATCH: Privacy-Aware Targeted Circuit Patching, a novel approach that first identifies and subsequently directly edits PII circuits to reduce leakage. PATCH achieves better privacy-utility trade-off than existing defenses, e.g., reducing recall of PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to reduce recall of residual leakage of an LM to as low as 0.01%. Our analysis shows that PII leakage circuits persist even after the application of existing defense mechanisms. In contrast, PATCH can effectively mitigate their impact.",Anthony Hughes; Vasisht Duddu; N. Asokan; Nikolaos Aletras; Ning Ma,Anthony Hughes,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Trustworthy, Safety, Privacy & Fairness",Interpretability & Model Analysis,leakage; pii; pii leakage; patch; lms; circuit; privacy; existing defense mechanisms; existing defense; privacy aware
1033-FIND,Argument Component Segmentation with Fine-Tuned Large Language Models,"Argument Mining (AM) aims to identify and interpret argumentative structures in unstructured text, with Argument Component Classification (ACC) as a core task. Despite significant advances, most ACC approaches rely on manually pre-segmented inputs, an assumption that rarely holds in practice due to the high cost and effort of expert human annotation, creating a major bottleneck for scalable AM systems. In this work, we focus on the foundation Argument Component Segmentation (ACS) task by proposing a fine-grained, paired-tag annotation schema that explicitly distinguishes between relevant and surrounding content, thus overcoming the limitations of previous single-separator approaches. Leveraging small and open Large Language Models (LLMs) fine-tuned on our paired-tag annotation schema, we can perform ACS with quality comparable to human expert annotators across multiple benchmark datasets. We further validate our approach on the downstream ACC task, showing that automated segmentation with fine-tuned LLMs yields ACC performances comparable to pipelines relying on human annotations. These findings suggest that reliable automated ACS via LLMs is both feasible and effective, paving the way for more scalable AM pipelines without human intervention.",Ettore Caputo; Sergio Greco; Lucio La Cava,Lucio La Cava,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",argument; segmentation; annotation schema; component; tag; fine tuned; annotation; tuned; paired; fine
1035-FIND,DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection,"Large language models (LLMs) are considered valuable Intellectual Properties (IP) due to the enormous computational cost of training, making their protection against malicious stealing or unauthorized deployment crucial. Despite efforts in watermarking and fingerprinting, existing methods either affect text generation or rely on white-box access, limiting practicality. To address this, we propose DuFFin, a novel Dual-Level Fingerprinting framework for black-box ownership verification. DuFFin jointly extracts trigger patterns and knowledge-level fingerprints to identify the source of a suspect model. We conduct experiments on diverse open-source models, including four popular base LLMs and their fine-tuned, quantized, and safety-aligned variants released by large companies, start-ups, and individuals. Results show that DuFFin accurately verifies the copyright of protected LLMs on their variants, achieving an IP-ROC greater than 0.99. Our code is available at https://anonymous.4open.science/r/llm-fingerprint-EC9F.",Yuliang Yan; Haochun Tang; Shuo Yan; Enyan Dai,Yuliang Yan,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Summarization & Generation,"Trustworthy, Safety, Privacy & Fairness",fingerprinting; protection; dual; variants; box; level; source; llms; model conduct experiments; copyright
1038-FIND,"The Art of Saying ""Maybe"": A Conformal Lens for Uncertainty Benchmarking in VLMs","Vision-Language Models (VLMs) have achieved remarkable progress in complex visual understanding across scientific and reasoning tasks. While performance benchmarking has advanced our understanding of these capabilities, the critical dimension of uncertainty quantification has received insufficient attention. Therefore, unlike prior conformal prediction studies that focused on limited settings, we conduct a comprehensive uncertainty benchmarking study, evaluating 16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets with 3 distinct scoring functions. Our findings demonstrate that larger models consistently exhibit better uncertainty quantification; models that know more also know better what they don't know. More certain models achieve higher accuracy, while mathematical and reasoning tasks elicit poorer uncertainty performance across all models compared to other domains. This work establishes a foundation for reliable uncertainty evaluation in multimodal systems.",Asif Azad; Mohammad Sadat Hossain; MD Sadik Hossain Shanto; M Saifur Rahman; Md Rizwan Parvez,Md Rizwan Parvez,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,uncertainty; know; benchmarking; vlms; conformal; uncertainty quantification; quantification; reasoning tasks; multimodal; better
1039-FIND,Diagnosis of Dysarthria Severity and Explanation Generation Using XAI-Enhanced CLINIC-GENIE on Diadochokinetic Tasks,"Deep neural network classifiers for dysarthria impairment severity face limitations regarding interpretability and treatment guidance. To overcome these, we introduce CLINIC-GENIE, an explainable two-stage framework consisting of: (1) CLINIC, a dysarthria severity classification model combining acoustic and speech embeddings with Clinically Explainable Acoustic Features (CEAFs); and (2) GENIE, a module translating CEAFs and their Shapley values into intuitive natural language explanations via a large language model. CLINIC achieved a balanced accuracy of 0.952 (17.3\% improvement over using CEAFs alone), and certified speech-language pathologists rated explanations from CLINIC-GENIE with an average fidelity score of 4.94, confirming enhanced clinical utility.",Jihyeon Kim; Insung Lee; Myoung-Wan Koo,Jihyeon Kim,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multimodal & Speech/Audio,Interpretability & Model Analysis,severity; explainable; acoustic; explanations; enhanced; speech; using xai; rated; intuitive; shapley
1041-FIND,"A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages","Large reasoning models (LRMs) increasingly rely on step-by-step Chain-of-Thought (CoT) reasoning to improve task performance, particularly in high-resource languages such as English. While recent work has examined final-answer accuracy in multilingual settings, the thinking traces themselves, i.e., the intermediate steps that lead to the final answer, remain underexplored. In this paper, we present the first comprehensive study of multilingual CoT reasoning, evaluating three key dimensions: performance, consistency, and faithfulness. We begin by measuring language compliance, answer accuracy, and answer consistency when LRMs are explicitly instructed or prompt-hacked to think in a target language, revealing strong language preferences and divergent performance across languages. Next, we assess crosslingual consistency of thinking traces by interchanging them between languages. We find that the quality and effectiveness of thinking traces vary substantially depending on the prompt language. Finally, we adapt perturbation-based techniques -- i.e., truncation and error injection -- to probe the faithfulness of thinking traces across languages, showing that models rely on traces to varying degrees. We will release our code and data to support future research.",Raoyuan Zhao; Yihong Liu; Hinrich Schuetze; Michael A. Hedderich,Raoyuan Zhao,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",traces; thinking; consistency; languages; faithfulness; answer; lrms; cot reasoning; answer accuracy; final answer
1047-FIND,ORSO QGen: Odds-Ratio Steerable Optimization for Controlling Question Generation,"Question generation plays an important role in educational applications, enabling automated assessment and reading comprehension support. Attribute-controlled question generation aims to produce questions that fit predefined characteristics such as difficulty, focus, or coverage. Existing methods predominantly rely on supervised fine-tuning, which often fails to impose a strong adherence to attribute values, resulting in weak coupling between prompt specifications and model outputs. We introduce Odds-Ratio Steerable Optimization (ORSO), a framework designed to enhance attribute sensitivity in question generation models. Building upon preference-based learning techniques without requiring human-curated preference sets, ORSO employs input-level perturbations to create contrastive training signals. Empirical evaluations on both exhaustive and expert-validated attribute configurations indicate that ORSO performs better in enforcing attribute conformity while maintaining output quality. These results argue for the benefits of explicit attribute-aware optimization in controllable question generation tasks.",Andreea Dutulescu; Stefan Ruseti; Mihai Dascalu; Danielle S McNamara,Andreea Dutulescu,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Summarization & Generation,"Trustworthy, Safety, Privacy & Fairness",question generation; attribute; question; steerable; generation; ratio; optimization; preference; aware optimization; performs better
1049-FIND,Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages,"High quality summarization data remains scarce in under-represented languages. However, historical newspapers, made available through recent digitization efforts, offer an abundant source of untapped, naturally annotated data. In this work, we present a novel method for collecting naturally occurring summaries via Front-Page Teasers, where editors summarize full length articles. We show that this phenomenon is common across seven diverse languages and supports multi-document summarization. To scale data collection, we develop an automatic process, suited to varying linguistic resource levels. Finally, we apply this process to a Hebrew newspaper title, producing HEBTEASESUM, the first dedicated multi-document summarization dataset in Hebrew.",Noam Dahan; Omer Kidron; Gabriel Stanovsky,Noam Dahan,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,Multilinguality & Low-Resource NLP,summarization; document summarization; multi document; hebrew; naturally; languages; document; title; resource levels; finally apply
1054-FIND,LetвЂ™s Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification,"Large language models demonstrate limited capability in proficiency-controlled sentence simplification, particularly when simplifying across large readability levels. We propose a framework that decomposes complex simplifications into manageable steps through dynamic path planning, semantic-aware exemplar selection, and chain-of-thought generation with conversation history for coherent reasoning. Evaluation on five languages across two benchmarks shows our approach improves simplification effectiveness while reducing computational steps. Human evaluation confirms the fundamental trade-off between simplification effectiveness and meaning preservation. Notably, even human annotators struggle to agree on semantic preservation judgments, highlighting the inherent complexity of this task. Our work shows that while step-by-step simplification improves control, preserving semantic fidelity during extensive simplification remains an open challenge.",Jingshen Zhang; Xin Ying Qiu; Lifang Lu; Zhuhua Huang; Yutao Hu; Yuechang Wu; JunYu Lu,Xin Ying Qiu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",simplification; step; preservation; step step; proficiency; semantic; steps; sentence; controlled; shows
1060-FIND,LogToP: Logic Tree-of-Program with Table Instruction-tuned LLMs for Controlled Logical Table-to-Text Generation,"Logical table-to-text generation aims to generate natural language descriptions that fluently and precisely describe the given table with both surface-level and logic-level fidelity. Although large language models (LLMs) have demonstrated strong capabilities in plain text, their proficiency in interpreting and reasoning tabular data is still limited. In this paper, we are the first to comprehensively explore the performance of various LLMs in the logical table-to-text generation task. However, we find that existing LLMs are difficult to achieve satisfactory results in this task. Even worse, existing prompt strategies cannot cope with complex non-chain logical reasoning scenarios on tables. To address the challenges mentioned above, we constructed a new table-related instruction dataset called LogicTableInstruct and instruction-tuned the open-source LLM on this dataset, resulting in the specialized LLM (LogicTableLLaMA-3.1-8B) for table-related tasks. We also introduced a novel reasoning method, Logic Tree-of-Program (LogicToP), to improve the logical reasoning ability of the LLMs on tables. Our extensive experiments on various LLMs demonstrated that LogicToP can effectively improve the performance of LLMs on this task. Our LogicTableLLaMA-3.1-8B model in the 5-shot LogicToP setting achieves state-of-the-art results on the Logic2Text dataset. The code and data will be released to boost future work on table-related tasks.",Yupian Lin; Guangya Yu; Cheng Yuan; Huan Du; Hui Luo; Yuang Bian; Jingping Liu; Zhidong He; Wen Du; Tong Ruan,Yupian Lin,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,"Reasoning, Planning & Agents",table; logical; table text; text generation; logic; various llms; logical reasoning; llms; related; program
1068-FIND,IRPO: Implicit Policy Regularized Preference Optimization,"Training complexity often scales with the size of hyperparameter space for Large Language Models (LLMs). While Direct Preference Optimization (DPO) offers learning stability through reparameterizing the reward function, its regularization against the reference policy can lead to suboptimal outcomes when the reference policy is not optimal. Recent DPO variants address this concern, but at a cost: they introduce additional hyperparameters, reducing feasibility for LLM fine-tuning. To overcome this challenge, we introduce Implicit policy Regularized Preference Optimization (IRPO), which tackles suboptimality while maintaining training simplicity. By treating the winning policy that generated the chosen responses in a pairwise dataset as an implicit policy, IRPO maximizes KL-regularized reward without extra hyperparameters. Then we propose a novel PO algorithm that directly optimizes the IRPO objective by estimating the likelihood ratio between implicit policies. As the winning policy generally outperforms the reference policy, IRPO can effectively address suboptimality. Our experiments show that IRPO significantly outperforms baseline algorithms with the same hyperparameter complexity. Moreover, IRPO demonstrates comparable performance to recent algorithms that rely on a larger number of hyperparameters, offering a practical solution for scalable LLM fine-tuning.",Youngsoo Jang; Yu Jin Kim; Geon-Hyeong Kim; Honglak Lee; Moontae Lee,Youngsoo Jang,Oral,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",,policy; implicit; regularized; preference optimization; reference; suboptimality; winning; hyperparameter; preference; llm fine tuning
1070-FIND,DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance,"Large Language Models (LLMs) demonstrate impressive capabilities but exhibit inconsistent performance across diverse domains. We propose DFPE (Diverse Fingerprint Ensemble), a novel training-free method that systematically constructs subject-adaptive ensembles by balancing model diversity and competence. DFPE introduces three key innovations: (1) semantic fingerprinting using averaged response embeddings to capture distinct problem-solving patterns, (2) DBSCAN-based clustering with quantile-based competence filtering to ensure diverse yet capable model selection, and (3) exponentially-weighted aggregation adapted to subject-specific performance. Our method's effectiveness is highlighted on the challenging MMLU-pro benchmark, where DFPE achieves a striking 17.1 percentage point gain over the best single model, reaching 71.4\% accuracy. This strong performance is consistent across other standard benchmarks, with significant accuracy improvements of 4.4 points on AGIEval and 2.7 points on MMLU. Our results underscore that a systematic approach to ensemble construction - one that balances diversity, subject-specific competence, and adaptive weighting, can substantially enhance the generalization and robustness of LLMs on multifaceted language understanding tasks.",Seffi Cohen; Nurit Cohen Inger; Niv Goldshlager; Bracha Shapira; Lior Rokach,Seffi Cohen,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",,subject; ensemble; competence; fingerprint; mmlu; diverse; points; adaptive; diversity; quantile
1080-FIND,Ranking Human and LLM Texts Using Locality Statistics,"The paper extends the Data Movement Distance (DMD) -- a metric defined to measure the locality in computer memory -- to text by defining a normalized version called nDMD. A key feature of nDMD is a new term designed to better characterize low-frequency tokens. By evaluating nDMD on English subset of the M4 dataset and GenAI detection shared task, the paper shows three key findings. First, nDMD is systematically higher in human-written text than in machine-generated text. Second, nDMD-based features not only outperform frequency baselines but also improve overall performance when combined. Finally, the proposed DMD normalization is more effective in distinguishing human and machine text than alternative approaches.",Yiyang Wang; Chen Ding; Hangfeng He,Yiyang Wang,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",locality; text; frequency; machine; human; key; shows key; task paper; improve overall; human machine
1084-FIND,MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding,"Manga, or Japanese comics, is a richly multimodal narrative form that blends images and text in complex ways. Teaching large multimodal models (LMMs) to understand such narratives at a human-like level could help manga creators reflect on and refine their stories. To this end, we introduce two benchmarks for multimodal manga understanding: MangaOCR, which targets in-page text recognition, and MangaVQA, a novel benchmark designed to evaluate contextual understanding through visual question answering. MangaVQA consists of 526 high-quality, manually constructed question-answer pairs, enabling reliable evaluation across diverse narrative and visual scenarios. Building on these benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive experiments, including comparisons with proprietary models such as GPT-4o and Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model provide a comprehensive foundation for evaluating and advancing LMMs in the richly narrative domain of manga.",Jeonghun Baek; Kazuki Egashira; Shota Onohara; Atsuyuki Miyai; Yuki Imajuku; Hikaru Ikuta; Kiyoharu Aizawa,Jeonghun Baek,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Multimodal & Speech/Audio,manga; narrative; multimodal; multimodal manga; understanding; understand; benchmark; specialized; visual; enabling reliable
1085-FIND,Hierarchical User Intent Inference with Knowledge Graph Grounding,"Understanding user intent in online reviews requires modeling not only explicit aspect ratings but also implicit motivations shaped by contextual factors. Existing large language models (LLMs) often lack structured grounding, fail to capture nuanced intent expression. We propose HII-KG, a two-stage Hierarchical Intent Inference framework that first predicts fine-grained aspect ratings and then generates natural language intent statements, guided by contextual subgraphs retrieved from a domain-specific knowledge graph (KG). We first employ parameter-efficient fine-tuning of LLaMA3.1-8B to predict aspect ratings in an instruction-based format. Moreover, we leverage Cypher-aware prompting to generate user intent from KG summaries. Experiments on a online hotel review dataset show that HII-KG consistently outperforms strong LLM and encoder-based baselines in both aspect classification (avg. F1 +4.5%) and intent generation (BLEU +3.3, ROUGE-L +2.9). The results demonstrate that structured KG integration can significantly enhance fluency, contextual relevance, and factual alignment in user intent inference.",Tzu-Cheng Peng; Chien Chin Chen; Yung-Chun Chang,Tzu-Cheng Peng,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",intent; user intent; aspect; ratings; user; contextual; knowledge graph; inference; online; hierarchical
1087-FIND,Improving the OOD Performance of Closed-Source LLMs on NLI Through Strategic Data Selection,"We investigate the robustness of fine-tuned Large Language Models (LLMs) for the task of Natural Language Inference (NLI), finding that the in-distribution gains from fine-tuning correspond to a large drop in out-of-distribution (OOD) performance. Despite the widespread use of closed-source LLMs, there are no robustness mitigation methods that work under their API fine-tuning constraints. Existing methods to improve robustness typically require changing the fine-tuning process or large-scale data augmentation, methods that are infeasible or cost prohibitive for closed-source models. To address this, we propose strategically selecting the NLI fine-tuning data, prioritising more complex examples or replacing existing training examples with LLM-generated data. Prioritising more complex training examples improves performance on challenging OOD NLI datasets, while training with synthetic data leads to substantial improvements on easier OOD datasets. We find that synthetic examples are often too simple, and by prompting LLMs to create more complex synthetic data we can improve performance on both easy and challenging OOD datasets. Finally, we show that recent autoregressive LLMs are substantially more robust to distributional shifts compared to encoder models, and should be a preferred baseline for future research.",Joe Stacey; Lisa Alazraki; Aran Ubhi; Beyza Ermis; Aaron Mueller; Marek Rei,Joe Stacey,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Efficiency, Scaling & NLP Systems",,ood; nli; examples; closed source; closed; fine; fine tuning; closed source llms; tuning; synthetic
1105-FIND,MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image Relational Association Capabilities in Large Visual Language Models,"Current multi-modal benchmarks primarily focus on facts within individual images. However, they overlook the associative relations among multiple images, which necessitate conducting commonsense reasoning grounded in associated knowledge at different granularities (i.e., image-level and entity-level) as well as the ability to perceive the order of images. Therefore, we propose a multi-image relational association task and a meticulously curated Multi-granularity Multi-image Relational Association (MMRA) benchmark, comprising 1,024 samples. To systematically evaluate current LVLMs, we establish a system of associative relations among images that contains 11 subtasks (e.g., UsageSimilarity, SubEvent, etc.) at two granularity levels (i.e., image-level and entity-level), based on relations in ConceptNet. Our experiments reveal that entity-level multi-image perception tasks pose greater challenges for LVLMs than image-level tasks. Moreover, LVLMs perform poorly on spatial-related tasks, indicating limited spatial awareness. Furthermore, we find that LVLMs exhibit weak image order perception capabilities, and we design a method to significantly improve this ability, demonstrating that most current LVLMs do not adequately consider image order perception during pre-training.",Siwei Wu; King Zhu; Yiming Liang; Yu Bai; Yizhi LI; Haoning Wu; Jiaheng Liu; Ruibo Liu; Xingwei Qu; Xuxin Cheng; Ge Zhang; Wenhao Huang; Chenghua Lin,Siwei Wu,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Multimodal & Speech/Audio,"Reasoning, Planning & Agents",image; multi image; lvlms; image level; multi; association; entity level; level; images; granularity
1106-FIND,COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values,"Existing Chinese preference datasets suffer from limited scale, restricted domain coverage, and insufficiently rigorous data validation. Human annotation significantly limits the scalability of human preference datasets. As a result, Chinese Alignment and Chinese Reward Models (CRM) have not yet been thoroughly explored. To address these challenges, we design an LLM-based data annotation pipeline with no human intervention. Based on this pipeline, we curate COIG-P (Chinese Open Instruction Generalist - Preference), a high-quality, large-scale Chinese preference dataset consisting of 1M Chinese preference pairs and 92k carefully curated Chinese queries across diverse domains, including Chat, Coding, Maths, and others. We conduct experiments to verify the quality of COIG-P from two perspectives. (1) COIG-P brings significant performance improvements for the Qwen2/2.5 and Infinity-Instruct model series on AlignBench through DPO, with gains ranging from 2% to 12%. Furthermore, it significantly outperforms other existing Chinese preference datasets. (2) We train an 8B-sized CRM and manually annotate a Chinese Reward Benchmark (CRBench). Our CRM demonstrates robust scoring ability on CRBench. In addition, in practical data construction experiments, the quality of the data constructed by our CRM is comparable to that produced by GPT-4o.",Siwei Wu; JinCheng Ren; Xeron Du; Shuyue Guo; Xingwei Qu; Yiming Liang; Jie Liu; Yunwen Li; Tyler Loakman; Tianyu Zheng; Boyu Feng; Huaqing Yuan; Zili Wang; Jiaheng Liu; Wenhao Huang; chenglin cai; Haoran Que; Jian Yang; Yuelin Bai; Zekun Moore Wang; Zhouliang Yu; Qunshu Lin; Ding Pan; Yuchen Eleanor Jiang; Tiannan Wang; Wangchunshu Zhou; Shenzhi Wang; Xingyuan Bu; Minghao Liu; Guoyin Wang; Ge Zhang; Chenghua Lin,Siwei Wu,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",chinese; preference; preference dataset; quality; scale; human; reward; annotation; high quality; datasets
1107-FIND,Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models,"Text embedding models are widely used in natural language processing applications. However, their capability is often benchmarked on tasks that do not require understanding nuanced numerical information in text. As a result, it remains unclear whether current embedding models can precisely encode numerical content, such as numbers, into embeddings. This question is critical because embedding models are increasingly applied in domains where numbers matter, such as finance and healthcare. For example, ''Company X's market share grew by 2\%'' should be interpreted very differently from ''Company X's market share grew by 20%'' , even though both indicate growth in market share. This study aims to examine whether text embedding models can capture such nuances. Using synthetic data in a financial context, we evaluate 13 widely used text embedding models and find that they generally struggle to capture numerical details accurately. Our further analyses provide deeper insights into embedding numeracy, informing future research to strengthen embedding model-based NLP systems with improved capacity for handling numerical content.",Ningyuan Deng; Hanyu Duan; Yixuan Tang; Yi Yang,Ningyuan Deng,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",Domain NLP (Biomedical/Clinical/Legal/Scientific),embedding; embedding models; text embedding models; text embedding; numerical; market; share; company; text; numbers
1112-FIND,code_transformed: The Influence of Large Language Models on Code,"Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 20,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake_case function names in Python code increased from 40.7% in Q1 2023 to 49.8% in Q3 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Our experimental results provide the first large-scale empirical evidence that LLMs affect real-world programming style.",Yuliang Xu; Siming Huang; Mingmeng Geng; Yao Wan; Xuanhua Shi; Dongping Chen,Siming Huang,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Summarization & Generation,"Retrieval, Grounding & External Knowledge (RAG)",code; style; llms code; programming; coding; llms; scale empirical; large scale empirical; algorithmic; remains fundamental
1120-FIND,Do LLMs model human linguistic variation? A case study in Hindi-English do-verb code-mixing,"Do large language models (LLMs) serve as proxies for human linguistic variation? We investigate this question through Hindi-English verb code-mixing, where speakers can use either a Hindi verb or an English verb with the light verb \textit{karna} (`do'). Both forms are grammatical, but speakers show gradient preferences without clear theoretical explanationвЂ”an ideal test of whether LLMs capture genuine sociolinguistic variation beyond formal rules. We collect preference ratings from 14 native speakers across 30 verb pairs in minimal pair contexts (moderate agreement, Krippendorff's О± = 0.365) and compare these to perplexity ratios from 7 base LLMs spanning families, sizes, and training compositions. Three experiments reveal systematic failure: unsupervised prediction performs at chance (F1 в‰¤ 0.33), supervised 4-way classification barely improves (F1 = 0.43), and binary classification achieves adequate performance (F1 = 0.81) for only one model after exhaustive feature searchвЂ”requiring the very human labels LLMs were meant to replace while discarding the gradient information linguists need. Models show no scaling effects and exhibit arbitrary systematic biases inconsistent with training data or human judgments. We conclude that current LLMs learn surface correlations without internalizing underlying sociolinguistic processes, and do not reliably model gradient linguistic variation. We release human ratings, perplexity ratios for 4,279 verb pairs across 7 models, and experimental materials.",Mukund Choudhary; Madhur Jindal; Gaurja Aeron; Monojit Choudhury,Mukund Choudhary,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Linguistics, Syntax & Semantics",Domain NLP (Biomedical/Clinical/Legal/Scientific),verb; linguistic variation; variation; hindi; gradient; speakers; human linguistic; ratios; hindi english; sociolinguistic
1121-FIND,ART: Attention-Regularized Transformers for Multi-Modal Robustness,"Transformers have become the standard in Natural Language Processing (NLP) and Computer Vision (CV) due to their strong performance, yet they remain highly sensitive to small input changes, often referred to as adversarial attacks, such as synonym swaps in text or pixel-level perturbations in images. These adversarial attacks can mislead predictions, while existing defenses are often domain-specific or lack formal robustness guarantees. We propose the \textit{Attention-Regularized Transformer} (ART), a framework that enhances robustness across modalities. ART builds on the \textit{Attention Sensitivity Tensor} (AST), which quantifies the effect of input perturbations on attention outputs. By incorporating an AST-based regularizer into training, ART encourages stable attention maps under adversarial perturbations in both text and image tasks. We evaluate ART on IMDB, QNLI, CIFAR-10, CIFAR-100, and Imagenette. Results show consistent robustness gains over strong baselines such as FreeLB and DSRM: up to $+36.9\%$ robust accuracy on IMDB and QNLI, and $+5$вЂ“$25\%$ on image benchmarks across multiple Vision Transformer (ViT) architectures, while maintaining or improving clean accuracy. ART is also highly efficient, training over $10\times$ faster than adversarial methods on text and requiring only $1.25\times$ the cost of standard training on images, compared to $1.5$вЂ“$5.5\times$ for recent robust ViTs.",Mohammed Bouri; Mohammed Erradi; Adnane Saoud,Mohammed Bouri,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Interpretability & Model Analysis,Multimodal & Speech/Audio,attention; art; adversarial; perturbations; times; robustness; ast; regularized; adversarial attacks; transformers
1125-FIND,GRAFF: GRaph-Augmented Fine-grained Fusion for Large Language Models,"Recent advancements in large language models (LLMs) have showcased remarkable text generation capabilities. However, due to the inherent ambiguity of natural language and the unstructured nature of text modality, LLMs still struggle to integrate structured information (e.g., graphs) effectively. This hinders their ability to leverage high-quality structured data in specialized domains. Thus, recent research has explored various methods to integrate graph structures into LLMs to improve generation. However, existing methods typically compress the graph's structural information into only a single token, which is concatenated with detailed text tokens for LLMs, restricting their ability to capture deep semantic and structural information. To overcome these limitations, we propose $\textbf{GR}$aph-$\textbf{A}$ugmented $\textbf{F}$ine-grained $\textbf{F}$usion (GRAFF), a novel method that integrates fine-grained node-level structural information with corresponding text entities to LLMs via a lightweight, structure adapter module. Specifically, we introduce a dual-channel graph input mechanism to separate structural and semantic components for graph encoding, producing more expressive graph representations. We then incorporate a graph attention (GAT) module into LLMs' intermediate decoder layers to process structural information, enhancing the modelвЂ™s capability in graph-based question answering. Extensive experiments show that GRAFF significantly improves LLMs' graph-understanding ability in question answering, outperforming baselines by an average of 10.14% across four datasets.",Himanshu Chaudhary; Ruida WANG; Gowtham Ramesh; Junjie Hu,Himanshu Chaudhary,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Summarization & Generation,Interpretability & Model Analysis,graph; structural information; structural; textbf; information; llms; grained; text; ability; module
1127-FIND,Tackling Distractor Documents in Multi-Hop QA with Reinforcement and Curriculum Learning,"Retrieval-augmented generation (RAG) systems rely on retrieval models for identifying relevant contexts and answer generation models for utilizing those contexts. However, retrievers exhibit imperfect recall and precision, limiting downstream performance. We introduce RAG-RL, an answer generation model trained for multi-hop question answering (MHQA) to not only generate answers but also to identify and cite relevant information from larger sets of retrieved contexts, shifting some of the burden of identifying relevant documents from the retriever to the answer generator. Our approach uses curriculum learning, where models are trained across retrieval settings with varying levels of noise. Our experiments show that training samples with fewer distractor documents enable models to acquire citation and reasoning skills with greater sample efficiency and generalizability, demonstrating strong model performance even as the number of irrelevant passages increases. We benchmark our methods on three open-domain MHQA datasets and report significant gains in answer and citation accuracy. Furthermore, our experiments provide empirical insights into how simpler training samples can give models stronger signals for learning specific skills (e.g., citation generation) and how different components of post-training (e.g., training set construction, rule-based rewards, training sample ordering, etc.) impact final model performance.",Jerry Huang; Siddarth Madala; Risham Sidhu; Cheng Niu; Hao Peng; Julia Hockenmaier; Tong Zhang,Jerry Huang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",citation; answer; identifying relevant; distractor; training samples; answer generation; documents; training; curriculum learning; curriculum
1129-FIND,RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams,"The intersection of AI and legal systems presents a growing need for tools that support legal education, particularly in under-resourced languages such as Romanian. In this work, we aim to evaluate the capabilities of Large Language Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning about Romanian driving law through textual and visual question-answering tasks. To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising Romanian driving test questions, text-based and image-based, alongside annotated legal references and explanations written by human experts. We implement and assess retrieval-augmented generation (RAG) pipelines, dense retrievers, and reasoning-optimized models across tasks, including Information Retrieval (IR), Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate that domain-specific fine-tuning significantly enhances retrieval performance. At the same time, chain-of-thought prompting and specialized reasoning models improve QA accuracy, surpassing the minimum grades required to pass driving exams. We highlight the potential and the limitations of applying LLMs and VLMs to legal education.",Andrei Vlad Man; RДѓzvan-Alexandru SmДѓdu; Cristian-George Craciun; Dumitru-Clementin Cercel; Florin Pop; Mihaela-Claudia Cercel,Răzvan-Alexandru Smădu,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,"Retrieval, Grounding & External Knowledge (RAG)",romanian; driving; legal; exams; answering; visual; education; retrieval; vlms; question answering
1142-FIND,FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs,"Large Language Models (LLMs) frequently generate hallucinated content, posing significant challenges for applications where factuality is crucial. While existing hallucination detection methods typically operate at the sentence level or passage level, we propose FactSelfCheck, a novel zero-resource black-box sampling-based method that enables fine-grained fact-level detection. Our approach represents text as interpretable knowledge graphs consisting of facts in the form of triples, providing clearer insights into content factuality than traditional approaches. Through analyzing factual consistency across multiple LLM responses, we compute fine-grained hallucination scores without requiring external resources or training data. Our evaluation demonstrates that FactSelfCheck performs competitively with leading sentence-level sampling-based methods while providing more detailed and interpretable insights. Most notably, our fact-level approach significantly improves hallucination correction, achieving a 35.5% increase in factual content compared to the baseline, while sentence-level SelfCheckGPT yields only a 10.6% improvement. The granular nature of our detection enables more precise identification and correction of hallucinated content. Additionally, we contribute FavaMultiSamples, a novel dataset that addresses a gap in the field by providing the research community with a second dataset for evaluating sampling-based methods.",Albert Sawczyn; Jakub Binkowski; Denis Janiak; Bogdan Gabrys; Tomasz Jan Kajdanowicz,Albert Sawczyn,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",,level; hallucination; sentence level; sampling; fact; detection; content; sentence; providing; hallucinated
1145-FIND,Punctuations and Predicates in Language Models,"In this paper we explore where information is collected and how it is propagated throughout layers in large language models (LLMs). We begin by examining the surprising computational importance of punctuation tokens which previous work has identified as attention sinks and memory aids. Using intervention-based techniques, we evaluate the necessity and sufficiency of punctuation tokens across layers in GPT-2, DeepSeek, and Gemma. Our results show stark model-specific differences: for GPT-2, punctuation is both necessary and sufficient in multiple layers, while this holds far less in DeepSeek and not at all in Gemma. Extending beyond punctuation, we ask whether LLMs process different components of input (e.g., subjects, adjectives, punctuation, full sentences) by forming early static summaries reused across the network, or if the model remains sensitive to changes in these components across layers. We investigate whether different reasoning rules are processed differently by LLMs. In particular, through interchange intervention and layer-swapping experiments, we find that conditional statements (if, then), and universal quantification (for all) are processed very differently. Our findings offer new insight into the internal mechanisms of punctuation usage and reasoning in LLMs and have implications for interpretability and model analysis.",Sonakshi Chauhan; Maheep Chaudhary; Choy Kwan Kiu; Samuel Nellessen; Nandi Schoots,Sonakshi Chauhan,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Interpretability & Model Analysis,,punctuation; layers; differently; processed; gemma; intervention; deepseek; components; gpt; tokens
1147-FIND,Test-time Corpus Feedback: From Retrieval to RAG,"Retrieval-Augmented Generation (RAG) has emerged as a standard framework for knowledge-intensive NLP tasks, combining large language models (LLMs) with document retrieval from external corpora. Despite its widespread use, most RAG pipelines continue to treat retrieval and reasoning as isolated componentsвЂ”retrieving documents once and then generating answers without further interaction. This static design often limits performance on complex tasks that require iterative evidence gathering or high-precision retrieval. Recent work in both the information retrieval (IR) and NLP communities has begun to close this gap by introducing adaptive retrieval and ranking methods that incorporate feedback. In this survey, we present a structured overview of advanced retrieval and ranking mechanisms that integrate such feedback. We categorize feedback signals based on their source and role in improving the query, retrieved context, or document pool. By consolidating these developments, we aim to bridge IR and NLP perspectives and highlight retrieval as a dynamic, learnable component of end-to-end RAG systems.",Mandeep Rathee; Venktesh V; Sean MacAvaney; Avishek Anand,Mandeep Rathee,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)","Dialogue, Conversational & Interactive NLP",retrieval; rag; feedback; nlp; ranking; document; end; context document; present structured; learnable
1149-FIND,RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis,"Data visualizations like charts are fundamental tools for quantitative analysis and decision-making across fields, requiring accurate interpretation and mathematical reasoning. The emergence of Multimodal Large Language Models (MLLMs) offers promising capabilities for automated visual data analysis, such as processing charts, answering questions, and generating summaries. However, they provide no visibility into which parts of the visual data informed their conclusions; this black-box nature poses significant challenges to real-world trust and adoption. In this paper, we take the first major step toward evaluating and enhancing the capabilities of MLLMs to attribute their reasoning process by highlighting the specific regions in charts and graphs that justify model answers. To this end, we contribute RADAR, a semi-automatic approach to obtain a benchmark dataset comprising 17,819 diverse samples with charts, questions, reasoning steps, and attribution annotations. We also introduce a method that provides attribution for chart-based mathematical reasoning. Experimental results demonstrate that our reasoning-guided approach improves attribution accuracy by up to 15 percentage points compared to baseline methods, and enhanced attribution capabilities translate to stronger answer generation, achieving high semantic similarity (BERTScore $\sim$ 0.90) with ground truth responses. This advancement represents a significant step toward more interpretable and trustworthy chart analysis systems, enabling users to verify and understand model decisions through reasoning and attribution.",Anku Rani; Aparna Garimella; Apoorv Saxena; Balaji Vasan Srinivasan; Paul Pu Liang,Anku Rani,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Interpretability & Model Analysis,"Reasoning, Planning & Agents",attribution; charts; visual data; reasoning; data analysis; chart; analysis; visual; mathematical reasoning; mathematical
1156-FIND,MaskLoRA: LowвЂ‘Rank SubspaceвЂ“Induced Token Masking for Efficient and Faithful Language Models,"We propose MaskLoRA, a plug-and-play masking mechanism that turns PEFT's low-rank subspace into a faithful, compute-aware token selector. Instead of relying on attention entropy or gradients, MaskLoRA scores tokens by the LoRA-induced representation shift and learns differentiable gates with an objective combining task loss, expected $L_0$ sparsity, prediction consistency (KL), and a subspace concentration term. We provide a bound showing the KL between full and masked predictions is controlled by the sum of dropped scores, implying top-$k$ selection minimizes the bound. Across encoders on GLUE and SQuAD, MaskLoRA matches full-model accuracy while yielding $1.3$--$2.6\times$ end-to-end speedups; on long-context NarrativeQA/GovReport it attains $1.5$--$3.6\times$ with $\leq 0.4$ absolute drops. Faithfulness improves over strong baselines (sufficiency $99.2\%$, necessity $59.3\%$, lower KL at $\rho{=}50\%$), and energy falls accordingly (GPU Joules-per-correct $3.28 \to 1.34$ at $\rho \approx 30\%$). The method is KV-cache compatible, rank- and budget-controllable, and robust under OOD stress (HANS, adversarial sentiment, noisy SQuAD; $\leq 0.9$ abs. drop at matched budgets). Qualitative analyses show layer-wise retention concentrating at semantic bottlenecks and class-separated $\Delta h_i$ embeddings. MaskLoRA offers accuracy-matched speedups with interpretable rationales and practical deployment hooks.",S M Rafiuddin,S M Rafiuddin,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Interpretability & Model Analysis,"LLM Evaluation, Benchmarks & Metrics",rank; squad; leq; speedups; matched; subspace; induced; masking; bound; faithful
1160-FIND,A Domain-Specific Curated Benchmark for Entity and Document-Level Relation Extraction,"Information Extraction (IE), encompassing Named Entity Recognition (NER), Named Entity Linking (NEL), and Relation Extraction (RE), is critical for transforming the rapidly growing volume of scientific publications into structured, actionable knowledge. This need is especially evident in fast-evolving biomedical fields such as the gut-brain axis, where research investigates complex interactions between the gut microbiota and brain-related disorders. Existing biomedical IE benchmarks, however, are often narrow in scope and rely heavily on distantly supervised or automatically generated annotations, limiting their utility for advancing robust IE methods. We introduce GutBrainIE, a benchmark based on more than 1,600 PubMed abstracts, manually annotated by biomedical and terminological experts with fine-grained entities, concept-level links, and relations. While grounded in the gut-brain axis, the benchmarkвЂ™s rich schema, multiple tasks, and combination of highly curated and weakly supervised data make it broadly applicable to the development and evaluation of biomedical IE systems across domains.",Marco Martinelli; Stefano Marchesin; Vanessa Bonato; Giorgio Di Nunzio; Nicola Ferro; Ornella Irrera; Laura Menotti; Federica Vezzani; Gianmaria Silvello,Marco Martinelli,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Information Extraction & Structured Prediction,Domain NLP (Biomedical/Clinical/Legal/Scientific),biomedical; brain; axis; extraction; relation extraction; entity; named entity; relation; named; supervised
1168-FIND,What Matters to an LLM? Behavioral and Computational Evidences from Summarization,"Large Language Models (LLMs) are now state-of-the-art at summarization, yet the hidden notion of importance that drives their information selections remains hidden. We propose to address this by combining behavioral and computational analyses. Behaviorally, we generate length-controlled summaries and derive empirical importance distributions based on how often each information unit is selected. These reveal that LLMs converge on consistent importance patterns, sharply different from pre-LLM baselines, and that LLMs cluster more by size than by family. Computationally, we probe internal representations and find that middle layers strongly encode importance, and certain attention heads alone align well with empirical importance distributions. Together, these results provide initial insights about what LLMs prioritize in summarization and how this priority is internally represented, opening a path toward interpreting, and ultimately controlling, information selection in LLMs.",Yongxin Zhou; Changshun Wu; Philippe Mulhem; Didier Schwab; Maxime Peyrard,Youngsoo Jang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Summarization & Generation,Interpretability & Model Analysis,importance; summarization; behavioral; distributions; hidden; llms; information; empirical; computational; size family
1173-FIND,Neural network embeddings recover value dimensions from psychometric survey items on par with human data,"This study introduces ""Survey and Questionnaire Item Embeddings Differentials"" (SQuID), a novel methodological approach that enables neural network embeddings to effectively recover latent dimensions from psychometric survey items. We demonstrate that embeddings derived from large language models, when processed with SQuID, can recover the structure of human values obtained from human rater judgments on the Revised Portrait Value Questionnaire (PVQ-RR). Our experimental validation compares multiple embedding models across a number of evaluation metrics. Unlike previous approaches, SQuID successfully addresses the challenge of obtaining negative correlations between dimensions without requiring domain-specific fine-tuning. Quantitative analysis reveals that our embedding-based approach explains 55% of variance in dimension-dimension similarities compared to human data. Multidimensional scaling configurations from both types of data show fair factor congruence coefficients and largely follow the underlying theory. These results demonstrate that semantic embeddings can effectively replicate psychometric structures previously established through extensive human surveys. The approach offers substantial advantages in cost, scalability and flexibility while maintaining comparable quality to traditional methods. Our findings have significant implications for psychometrics and social science research, providing a complementary methodology that could expand the scope of human behavior and experience represented in measurement tools.",Max Pellert; Clemens M Lechner; Indira Sen; Markus Strohmaier,Max Pellert,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Interpretability & Model Analysis,"LLM Evaluation, Benchmarks & Metrics",embeddings; recover; psychometric; human; survey; embeddings effectively; human data; questionnaire; dimensions; neural network
1177-FIND,Compositional Reasoning via Joint Image and Language Decomposition,"Multimodal reasoning tasks such as visual question answering (VQA) require models to process both language and visual inputs. However, existing approaches typically decompose only language queries, treating images as monolithic inputs. We introduce REDI, a framework that jointly decomposes both images and questions into visual sub-domains (segmentation, material, depth, and color) with corresponding sub-questions. REDI employs an orchestrator agent that coordinates specialized worker agents, where each worker specializes in a specific visual sub-domain, and aggregates their responses through meta-reasoning. This hierarchical multi-agent design mitigates error propagation and improves compositional reasoning across both open- and closed-source MLLMs. On SEEDBench, MMBench, and CLEVR, REDI achieves absolute accuracy improvements of 8.9%, 8.2%, and 16.0%, respectively, marking a significant advancement in compositional multimodal reasoning. The code will be released on acceptance.",Dwip Dalal; Madhav Kanda; Zhenhailong Wang; Heng Ji; Unnat Jain,Dwip Dalal,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Multimodal & Speech/Audio,"Reasoning, Planning & Agents",sub; compositional; visual; compositional reasoning; multimodal reasoning; reasoning; images; inputs; questions; agent
1180-FIND,Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities,"The rapid integration of large language models (LLMs) into high-stakes legal work has exposed a critical gap: no benchmark exists to systematically stress-test their reliability against the nuanced, adversarial, and often subtle flaws present in real-world contracts. To address this, we introduce CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an LLM's legal reasoning. We study the capabilities of LLMs to detect and reason about fine-grained discrepancies by producing over 7500 real-world perturbed contracts from foundational datasets like CUAD and ContractNLI. Our novel, persona-driven pipeline generates 10 distinct anomaly categories, which are then validated against official statutes using a Retrieval-Augmented Generation (RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMsвЂ™ ability to detect embedded legal flaws and explain their significance. Our analysis shows a key weakness: these models often miss subtle errors and struggle even more to justify them legally. Our work outlines a path to identify and correct such reasoning failures in legal AI.",Manan Roy Choudhury; Adithya Chandramouli; Mannan Anand; Vivek Gupta,Mannan Anand,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Retrieval, Grounding & External Knowledge (RAG)",legal; clause; contracts; legal reasoning; flaws; subtle; detect; benchmark; reasoning; llms high stakes
1184-FIND,Token-Wise Kernels (TWiKers) for Vicinity-Aware Attention in Transformers,"Self-attention mechanisms in transformers enable tokens to interact across a sequence but lack an explicit inductive bias to capture local contextual dependencies, an inherent characteristic of human languages. We propose Token-Wise Kernels (TWiKers), a novel enhancement to transformers that learn token-specific convolutional kernels applied to the keys or values. Each token is assigned a small kernel, initialized to the ""Central Dirac"" (e.g., [0,1,0] for size=3), meaning the token ""bears"" the attention from all other tokens alone. During training, these kernels adapt, and greater deviation from the Central Dirac indicates stronger attention redistribution to neighboring tokens. This introduces the first transformer weights encoding lexical redistribution patterns in a directly interpretable form, without additional post-hoc analysis. Our experiments show that content words (e.g., nouns and verbs) retain self-focus, while function words (e.g., prepositions and conjunctions) shift attention toward their neighbors, aligning with their syntactic and semantic roles. We further apply TWiKers to distinguish literary genres, historical periods, and authors, demonstrating their effectiveness in capturing high-level stylistic patterns. Finally, we demonstrate the potential of TWiKers as an effective inductive bias to improve transformer training, validated across a range of downstream tasks.",Kuangdai Leng; Jia Bi; Samuel Pinilla; Jaehoon Cha,Kuangdai Leng; Jia Bi,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Interpretability & Model Analysis,"Trustworthy, Safety, Privacy & Fairness",kernels; attention; token; transformers; redistribution; token wise; inductive; tokens; words; central
1187-FIND,Beyond a Single Extractor: Re-thinking HTML-to-Text Extraction for LLM Pre-training,"The first pre-processing step for constructing web-crawled LLM pre-training datasets involves extracting text from HTML. Despite the immense diversity of web content, existing open-source datasets predominantly apply a single fixed extractor on all webpages. In this work, we investigate whether this practice leads to suboptimal dataset coverage. We first show that while different extractors yield similar performance on standard language understanding tasks, the pages surviving a fixed filtering pipeline can differ substantially. This suggests a simple intervention: by taking a Union over different extractors, we can increase the token yield of DCLM-Baseline by up to 71% while maintaining benchmark performance. We further show that for structured content such as tables and codeblocks, extractor choice significantly impacts downstream task performance, with differences up to 8 percentage points (p.p.) on WikiTQ and 2 p.p. on HumanEval.",Jeffrey Li; Joshua P Gardner; Doug Kang; Fangping Shi; Karanjeet Singh; Chun-Liang Li; Herumb Shandilya; David Leo Wright Hall; Oncel Tuzel; Percy Liang; Ludwig Schmidt; Hadi Pouransari; Fartash Faghri,Jeffrey Li,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"LLM Evaluation, Benchmarks & Metrics",,llm pre; llm pre training; pre; fixed; pre training; web; yield; pre training datasets; structured content; datasets predominantly
1191-FIND,Can Models Help Us Create Better Models? Evaluating LLMs as Data Scientists,"We present FeatEng, a novel benchmark designed to evaluate the ability of large language models (LLMs) to perform feature engineering, a critical and knowledge-intensive task in data science. FeatEng assesses LLMs by their capacity to generate Python code that transforms raw tabular data into features that improve the performance of a downstream machine learning model. Our analysis of LLM outputs reveals that success on FeatEng often requires the application of significant world and domain knowledge, along with complex reasoning, to construct novel data representations. While focused on feature engineering, the benchmark probes a confluence of abilities indicative of an LLM's broader potential for practical, data-centric problem-solving. We demonstrate that FeatEng offers a targeted and efficient approach to assess a specific but crucial aspect of LLM capabilities relevant to real-world data science applications.",MichaЕ‚ Pietruszka; ЕЃukasz Borchmann; Aleksander JД™drosz; PaweЕ‚ Morawiecki,Michał Pietruszka,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Interpretability & Model Analysis,"LLM Evaluation, Benchmarks & Metrics",feature engineering; engineering; feature; science; machine learning model; indicative; relevant real; models evaluating; data scientists; evaluate ability
1192-FIND,Distill and Align Decomposition for Enhanced Claim Verification,"Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to 71.75% macro-F1, outperforming prompt-based approaches (+1.99, +6.24) and existing RL methods (+5.84). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition.",Jabez Magomere; Elena Kochkina; Samuel Mensah; Simerjot Kaur; Fernando Acero; Arturo Oncevay; Charese Smiley; Xiaomo Liu; Manuela Veloso,Elena Kochkina,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",decomposition; verification; claim verification; claim; verifier; quality; jointly; align; smaller language; decomposition enhanced
1193-FIND,Human-Aligned Faithfulness in Toxicity Explanations of LLMs,"The discourse around toxicity and LLMs in NLP largely revolves around detection tasks. This work shifts the focus to evaluating LLMs' _reasoning_ about toxicity&mdash;from their explanations that justify a stance&mdash;to enhance their trustworthiness in downstream tasks. Despite extensive research on explainability, it is not straightforward to adopt existing methods to evaluate free-form toxicity explanation due to their over-reliance on input text perturbations, among other challenges. To account for these, we propose a novel, theoretically-grounded multi-dimensional criterion (HAF), that measures the extent to which LLMs' free-form toxicity explanations align with those of a rational human under ideal conditions. We develop six metrics, based on uncertainty quantification, to comprehensively evaluate HAF of LLMs' toxicity explanations with no human involvement, and highlight how ""non-ideal"" the explanations are. We conduct several experiments on three Llama models (of size up to 70B) and an 8B Ministral model on five diverse toxicity datasets. Our results show that while LLMs generate plausible explanations to simple prompts, their reasoning about toxicity breaks down when prompted about the nuanced relations between the complete set of reasons, the individual reasons, and their toxicity stances, resulting in inconsistent and irrelevant responses. We open-source our [code](https://anonymous.4open.science/r/safte-7AE0/) and [LLM-generated explanations](https://anonymous.4open.science/r/safte-7AE0/haf_results/HAF_sample_data.xlsx).",Ramaravind Kommiya Mothilal; Joanna Roy; Syed Ishtiaque Ahmed; Shion Guha,Ramaravind Kommiya Mothilal,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",toxicity; explanations; reasons; free form; ideal; llms; https anonymous 4open; https anonymous; anonymous 4open; 4open science
1198-FIND,Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding,"VisionвЂ“language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative languageвЂ”such as sarcasm, humor, and metaphorвЂ”remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to othersвЂ”especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization and interpretable multimodal understanding.",Seyyed Saeid Cheshmi; Hahnemann Ortiz; James Mooney; Dongyeop Kang,Seyyed Saeid Cheshmi,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Reasoning, Planning & Agents",figurative; multimodal; styles; reasoning; sarcasm; humor; style; figurative language; literal; multimodal reasoning
1200-FIND,QueStER: Query Specification for Generative Keyword-Based Retrieval,"Generative retrieval (GR) differs from the traditional indexвЂ“thenвЂ“retrieve pipeline by storing relevance in model parameters and generating retrieval cues directly from the query, but it can be brittle out of domain and expensive to scale. We introduce QueStER (QUEry SpecificaTion for gEnerative Keyword-Based Retrieval), which bridges GR and query reformulation by learning to generate explicit keyword-based search specifications. Given a user query, a lightweight LLM produces a keyword query that is executed by a standard retriever (BM25), combining the generalization benefits of generative query rewriting with the efficiency and scalability of lexical indexing. We train the rewriting policy with reinforcement learning techniques. Across in- and out-of-domain evaluations, QueStER consistently improves over BM25 and is competitive with neural IR baselines, while maintaining strong efficiency.",Arthur SATOUF; Yuxuan ZONG; Habiboulaye Amadou Boubacar; Pablo Piantanida; Benjamin Piwowarski,Arthur SATOUF,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",query; keyword; generative; bm25; query specification; retrieval; rewriting; specification; based retrieval; efficiency
1202-FIND,Evaluating Sparse Autoencoders for Monosemantic Representation,"A key barrier to interpreting large language models is polysemanticity, where neurons activate for multiple unrelated concepts. Sparse autoencoders (SAEs) have been proposed to mitigate this issue by transforming dense activations into sparse, more interpretable features. While prior work suggests that SAEs promote monosemanticity, no quantitative comparison has examined how concept activation distributions differ between SAEs and their base models. This paper provides the first systematic evaluation of SAEs against base models through activation distribution lens. We introduce a fine-grained concept separability score based on the JensenвЂ“Shannon distance, which captures how distinctly a neuron's activation distributions vary across concepts. Using two large language models (Gemma-2-2B and DeepSeek-R1) and multiple SAE variants across five datasets (including word-level and sentence-level), we show that SAEs reduce polysemanticity and achieve higher concept separability. To assess practical utility, we evaluate concept-level interventions using two strategies: full neuron masking and partial suppression. We find that, compared to base models, SAEs enable more precise concept-level control when using partial suppression. Building on this, we propose Attenuation via Posterior Probabilities (APP), a new intervention method that uses concept-conditioned activation distributions for targeted suppression. APP achieves the smallest perplexity increase while remaining highly effective at concept removal.",Moghis Fereidouni; Muhammad Umair Haider; Peizhong Ju; A.B. Siddique,Moghis Fereidouni,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics",Interpretability & Model Analysis,saes; concept; activation distributions; suppression; activation; base models; distributions; sparse; concept level; base
1204-FIND,Event Detection with a Context-Aware Encoder and LoRA for Improved Performance on Long-Tailed Classes,"The current state of event detection research has two notable re-occurring limitations that we investigate in this study. First, the unidirectional nature of decoder-only LLMs presents a fundamental architectural bottleneck for natural language understanding tasks that depend on rich, bidirectional context. Second, we confront the conventional reliance on Micro-F1 scores in event detection literature, which systematically inflates performance by favoring majority classes. Instead, we focus on Macro-F1 as a more representative measure of a model's ability across the long-tail of event types. Our experiments demonstrate that models enhanced with sentence context achieve superior performance over canonical decoder-only baselines. Using Low-Rank Adaptation (LoRA) during finetuning provides a substantial boost in Macro-F1 scores in particular, especially for the decoder-only models, showing that LoRA can be an effective tool to enhance LLMs' performance on long-tailed event classes.",Abdullah Al Monsur; Nitesh Vamshi Bommisetty; Gene Louis Kim,Abdullah Al Monsur,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Information Extraction & Structured Prediction,,event; classes; lora; decoder; performance long; f1 scores; macro f1; macro; long; detection
1207-FIND,Think Hard Only When Needed: A Hybrid Best-of-N and Beam Search for Efficient Test-Time Compute,"Test-time compute has emerged as a promising paradigm that enables small language models (SLMs) to achieve large language model (LLM)-level capabilities by allocating additional compute for explicit reasoning during inference. Two common approaches are beam search and Best-of-N sampling. Beam search improves reasoning quality by scoring and optimizing token sequences using Process Reward Models (PRMs), but can incur non-trivial computational overhead and latency. In contrast, Best-of-N executes all reasoning trajectories without PRM guidance, often wasting compute on low-quality trajectories that may have gone astray early in the generation process. To address both inefficiencies, we propose THROW (THink haRd Only When needed)вЂ”a hybrid inference pipeline that combines the diversity of Best-of-N with the reasoning trajectory optimization of beam search. THROW introduces a selective branch truncation and expansion mechanism: it generates shorter initial trajectories than Best-of-N and evaluates them using PRMs to classify each query as ""easy"" or ""hard."" Based on this classification, THROW applies branch truncation for easy queries, mimicking Best-of-N, and PRM-guided branch expansion for hard ones, similar to beam search. Evaluations on MATH500, AMC23, and AIME24 demonstrate that THROW achieves 1.54Г— and 14.38Г— latency speedups and 35.7% and 80.4% token reductions on average while preserving high reasoning accuracy compared to Best-of-N and Beam Search.",Hyewon Suh; Chaojian Li; Cheng-Jhih Shih; Zheng Wang; Kejing Xia; Yonggan Fu; Yingyan Celine Lin,Hyewon Suh,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",beam search; beam; best; search; hard; branch; compute; trajectories; prm; prms
1215-FIND,Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models,"Access control is a cornerstone of secure computing, yet large language models often blur role boundaries by producing unrestricted responses. We study role-conditioned refusals, focusing on the LLM's ability to adhere to access control policies by answering when authorized and refusing when not. To evaluate this behavior, we created a novel dataset that extends the Spider and BIRD text-to-SQL datasets, both of which have been modified with realistic PostgreSQL role-based policies at the table and column levels. We compare three designs: (i) zero or few-shot prompting, (ii) a two-step generator-verifier pipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that learn permission awareness directly. Across multiple model families, explicit verification (the two-step framework) improves refusal precision and lowers false permits. At the same time, fine-tuning achieves a stronger balance between safety and utility (i.e., when considering execution accuracy). Longer and more complex policies consistently reduce the reliability of all systems. We release RBAC-augmented datasets and code.",ДђorД‘e Klisura; Joseph Khoury; Ashish Kundu; RAM KRISHNAN; Anthony Rios,Anthony Rios,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Trustworthy, Safety, Privacy & Fairness",policies; role; access; control; refusals; sql; conditioned; step; policies consistently; role based
1218-FIND,NL2Logic: AST-Guided Translation of Natural Language into First-Order Logic with Large Language Models,"Automated reasoning is critical in domains such as law and governance, where verifying claims against facts in documents requires both accuracy and interpretability. Recent work has adopted a structured reasoning paradigm that parses first-order logic (FOL) rules from natural language and delegates inference to automated solvers. With the rise of large language models (LLMs), methods such as GCD and CODE4LOGIC leverage their reasoning and code generation capabilities to enhance logic parsing. However, these approaches suffer from (1) fragile syntax control, due to weak enforcement of global grammar consistency, and (2) low semantic faithfulness, as they lack fine-grained clause-level semantic understanding. To address these challenges, we propose NL2Logic, a framework that uses the Abstract Syntax Tree (AST) as an intermediate reasoning layer to translate natural language into logic expressions with both semantic fidelity and syntactic precision. It consists of a recursive LLM-based semantic parser that constructs a precise AST capturing clause-level semantics, and an AST-guided generator that deterministically compiles the parsed logic into solver-ready code through a two-pass algorithm. On the FOLIO and LogicNLI benchmarks, NL2Logic attains 99% syntactic accuracy and improves semantic correctness by 30% over state-of-the-art baselines, establishing a promising pathway for verifiable natural language reasoning.",Rizky Ramadhana Putra; Raihan Sultan Pasha Basuki; Yutong Cheng; Peng Gao,Rizky Ramadhana Putra Kusnaryanto,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics",,ast; logic; semantic; natural language; natural; order logic; clause; reasoning; syntax; syntactic
1225-FIND,Coding Agents with Multimodal Browsing are General-Purpose Problem Solvers,"Modern human labor is characterized by specialization; we train for years and develop particular tools that allow us to perform well across a variety of tasks. Similarly, specialized AI agents with task-specific tools or architectures often fail to generalize beyond their intended scope. In this work, we ask: *what is the minimal set of general tools for achieving generalizability across diverse domains?* We propose OpenHands-Versa, a single-agent system with a modest number of general tools like code execution, search engine, web browser and multimodal file viewer, for three practical domains: software engineering, deep research, and web browsing. Notably, OpenHands-Versa demonstrates superior or competitive performance over task-specific specialized agents on three challenging benchmarks: SWE-Bench Multimodal, GAIA, and The Agent Company, with absolute improvements in success rate of **9.1**, **1.3**, and **9.1 points**, respectively. Thus, our minimal *single-agent* system can achieve strong generalization indicating that specialist agents provide no practical benefit. Furthermore, we find that specialist multi-agent systems do not generalize beyond their intended scope. These findings establish OpenHands-Versa as a strong baseline for future research.",Aditya Bharat Soni; Boxuan Li; Xingyao Wang; Valerie Chen; Graham Neubig,Aditya Bharat Soni,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Multimodal & Speech/Audio,versa; tools; agent; agents; single agent; intended; scope; general; generalize; multimodal
1230-FIND,Quantifying Data Contamination in Psychometric Evaluations of LLMs,"Recent studies apply psychometric questionnaires to Large Language Models (LLMs) to assess high-level psychological constructs such as values, personality, moral foundations, and dark traits. Although prior work has raised concerns about possible data contamination from psychometric inventories, which may threaten the reliability of such evaluations, there has been no systematic attempt to quantify the extent of this contamination. To address this gap, we propose a framework to systematically measure data contamination in psychometric evaluations of LLMs, evaluating three aspects: (1) item memorization, (2) evaluation memorization, and (3) target score matching. Applying this framework to 21 models from major families and four widely used psychometric inventories, we provide evidence that popular inventories such as the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40) exhibit strong contamination, where models not only memorize items but can also adjust their responses to achieve specific target scores.",Jongwook Han; Woojung Song; Jonggeun Lee; Yohan Jo,Jongwook Han,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",psychometric; contamination; data contamination; evaluations llms; evaluations; memorization; values; target; models major; major families
1237-FIND,Task-aware Block Pruning with Output Distribution Signals for Large Language Models,"Large language models (LLMs) provide excellent performance, but their practical deployment is limited by significant inference costs. While block pruning effectively reduces latency with structural coherence, existing methods typically rely on representation similarity or costly sensitivity analyses, thus neglecting task-aware model behavior. Here, we introduce a novel block-pruning method that quantifies block-level uncertainty by measuring statistics on each block's early-exited output distribution on a calibration dataset, in order to identify prunable blocks. Extensive experiments validate the effectiveness of the proposed method, demonstrating substantial efficiency gains without compromising downstream task performance, while avoiding cost-heavy sensitivity analysis.",Song-ha Jo; Youngrok Ko; Sang-goo Lee; Jinseok Seol,Song-ha Jo,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Efficiency, Scaling & NLP Systems",Interpretability & Model Analysis,block; block pruning; pruning; output distribution; task aware; sensitivity; output; distribution; order identify; sensitivity analyses
1248-FIND,LARA: LLM-based Agile Power Distribution Network Restoration from Disastrous Events,"Restoring power distribution networks after disruptions demands rapid, reliable coordination across repair crews, mobile power sources, and switching actions under strict constraints. Classical optimization yields high-quality plans but can be slow, while reinforcement learning often requires feeder-specific training and careful reward shaping. We recast restoration as language-conditioned planning: a large language model generates high-level restoration plans over a compact pre-validated catalogue of feasible actions. This constrained generation design makes decisions reliably, scalably, and interpretably, and allows for real-time human-in-the-loop decision-making while requiring no topology-specific setup or retraining. Our method achieves near-mixed-integer-linear programming performance on the IEEE 13-node standard power distribution feeder and outperforms a time-capped MILP solver on the IEEE 33-node standard feeder by around 13\%, while using less than 1\% of its wall-clock runtime.",Jishnu Warrier; Heqing Huang; Yuzhang Lin; Sai Qian Zhang,Jishnu Warrier,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Reasoning, Planning & Agents",Summarization & Generation,power; distribution; plans; node; actions; standard; clock; interpretably; strict constraints; yields high
1253-FIND,Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Benchmark,"We introduce ChemComp, the first chemistry-focused benchmark for evaluating compositional multi-hop reasoning in large language models (LLMs). Our automated pipeline constructs benchmarks from proprietary or public data by integrating generative reasoning models, chemical named-entity recognition, and external knowledge bases to build knowledge graphs. Applied to recent chemistry literature, this approach minimizes overlap with LLM pretraining data. The resulting dataset comprises 1,188 multi-hop questions, refined through domain-expert feedback and robust evaluation protocols. Using ChemComp, we systematically compare LLM performance with and without retrieval augmentation, including an idealized gold-context scenario. Our results show that even state-of-the-art models struggle with compositional reasoning: retrieval significantly improves accuracy, yet reasoning errors persist even under perfect retrieval. These findings highlight the limitations of current LLMs and the critical role of retrieval-augmented methods in scientific reasoning. Furthermore, our pipeline is generalizable with fine-tuning, enabling the creation of challenging multi-hop reasoning benchmarks across domains and proprietary datasets.",Mohammad Khodadad; Ali Shiraee Kasmaee; Mahdi Astaraki; Nicholas Sherck; Hamidreza Mahyar; Soheila Samiee,Soheila Samiee,Poster,In-person,POSTER HALL,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"LLM Evaluation, Benchmarks & Metrics",chemistry; hop; multi hop; multi hop reasoning; hop reasoning; reasoning; retrieval; reasoning large language; reasoning large; multi
1256-FIND,SD-E2: Semantic Exploration for Reasoning Under Token Budgets,"Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity вЂ“ ExplorationвЂ“Exploitation (SD-E2), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E2 assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a -scoreвЂ“normalized multi-objective objective that stabilizes training. On GSM8K, SD-E2 surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +26.0, +6.0, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. These results indicate that rewarding semantic novelty yields a more compute-efficient explorationвЂ“exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation (adjusting the reasoning process structure rather than per-token computation), SD-E2 offers a complementary path to efficiency gains in resource-constrained models.",Kshitij Mishra; Nils Lukas; Salem Lahlou,Kshitij Mishra,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",grpo; diversity; exploration; novelty; semantic; slms; exploitation; budgets; reasoning; compute
1257-FIND,Risk Assessment of Power Outages as Rare Events with Learning Models and LLMs,"Large Language Models (LLMs) are increasingly being considered for high-stakes decision-making, yet their application in statistical risk analysis remains largely underexplored. A central challenge in this domain is enabling LLMs to effectively leverage historical data. To address this, we propose novel methods for extracting key information from raw data and translating it into structured contextual input within the LLM prompt. Applying our methods to a case study of power outage risk assessment, we demonstrate that this contextualization strategy significantly improves the LLM's performance in risk assessment tasks. While the LLM's numerical predictions still do not match those of a standard machine learning model, the LLM-based approach offers distinct advantages in versatility and interpretability. These findings demonstrate a new paradigm for contextualizing data to support risk assessment.",Haiyun Huang; Yukun Li; Marco A Pretell; Jacob Naroian; Ebadah Khan; Liping Liu,Daniel Fein,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Interpretability & Model Analysis,,risk; assessment; power; llm; address propose novel; raw data; contextualization; historical data; domain enabling; numerical predictions
1270-FIND,Thinking Beyond the Local: Multi-View Instructed Adaptive Reasoning in KG-Enhanced LLMs,"Knowledge Graph-enhanced Large Language Models (KG-Enhanced LLMs) integrate the linguistic capabilities of LLMs with the structured semantics of Knowledge Graphs (KGs), showing strong potential in knowledge-intensive reasoning tasks. However, existing methods typically adopt query-driven iterative reasoning from a local perspective, which limits their ability to capture semantically distant but crucial information, leading to dual bottlenecks in efficiency and accuracy for complex multi-hop tasks. To address this issue, we propose MIAoG, a multi-view instructed adaptive reasoning of LLM on KG, which is designed to overcome the limitations of local exploration by enabling LLMs to plan, evaluate, and adapt reasoning paths from a global perspective. Instead of query-anchored exploration, MIAoG first prompts the LLM to generate a multi-view instruction set that outlines diverse potential reasoning paths and explicitly specifies global reasoning intentions to guide the model toward coherent and targeted reasoning. During reasoning, MIAoG integrates a real-time introspection mechanism that evaluates the alignment between the current path and the instructions, adaptively pruning inconsistent trajectories to enhance global consistency while maintaining efficiency. Extensive experiments on multiple public datasets show that MIAoG achieves state-of-the-art performance in KG-enhanced LLM reasoning, particularly excelling in complex multi-hop scenarios.",Minghan Zhang; Shu Zhao; Zhen Yang; Hongsheng Wu; Yongxing Lin; Haodong Zou; Jie Chen; Zhen Duan,Minghan Zhang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics","Reasoning, Planning & Agents",reasoning; enhanced; view; local; global; multi; instructed; adaptive reasoning; reasoning paths; complex multi
1277-FIND,DAMASHA: Detecting AI in Mixed Adversarial Texts via Segmentation with Human-interpretable Attribution,"In the age of advanced large language models (LLMs), the boundaries between human and AI-generated text are becoming increasingly blurred. We address the challenge of segmenting mixed-authorship text, that is identifying transition points in text where authorship shifts from human to AI or vice-versa, a problem with critical implications for authenticity, trust, and human oversight. We introduce a novel framework, called Info-Mask for mixed authorship detection that integrates stylometric cues, perplexity-driven signals, and structured boundary modeling to accurately segment collaborative human-AI content. To evaluate the robustness of our system against adversarial perturbations, we construct and release an adversarial benchmark dataset Mixed-text Adversarial setting for Segmentation (MAS), designed to probe the limits of existing detectors. Beyond segmentation accuracy, we introduce Human-Interpretable Attribution (HIA) overlays that highlight how stylometric features inform boundary predictions, and we conduct a small-scale human study assessing their usefulness. Across multiple architectures, Info-Mask significantly improves span-level robustness under adversarial conditions, establishing new baselines while revealing remaining challenges. Our findings highlight both the promise and limitations of adversarially robust, interpretable mixed-authorship detection, with implications for trust and oversight in human-AI co-authorship.",L D M S Sai Teja; N Siva Gopala Krishna; Ufaq Khan; Muhammad Haris Khan; Partha Pakray; Atul Mishra,L D M S Sai Teja,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Dialogue, Conversational & Interactive NLP","Trustworthy, Safety, Privacy & Fairness",authorship; mixed; human ai; adversarial; human; segmentation; stylometric; human interpretable; robustness adversarial; oversight
1281-FIND,Improving LLM Responses to Sensitive Topics Through Fine-Grained Evaluation,"Large Language Models (LLMs) often default to overly cautious and vague responses when handling sensitive topics, sacrificing helpfulness for safety. Existing evaluation frameworks lack systematic methods to identify and address specific weaknesses in responses to sensitive topics, making it difficult to improve both safety and helpfulness simultaneously. To address this, we introduce FINEST, a FINE-grained response evaluation taxonomy for Sensitive Topics, which breaks down helpfulness and harmlessness into errors across three main categories: Content, Logic, and Appropriateness. Experiments on a Korean-sensitive question dataset demonstrate that our score- and error-based improvement pipeline, guided by FINEST, significantly improves the model responses across all three categories, outperforming refinement without guidance. Notably, score-based improvement---providing category-specific scores and justifications---yields the most significant gains, reducing the error sentence ratio for Appropriateness by up to 33.09%. This work lays the foundation for a more explainable and comprehensive evaluation and improvement of LLM responses to sensitive questions.",Juhyun Oh; Nayeon Lee; Chani Jung; Jiho Jin; Junho Myung; Jongwon Lee; Taieui Song; Alice Oh,Juhyun Oh,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",sensitive topics; sensitive; topics; helpfulness; responses; appropriateness; improvement; llm responses; evaluation; categories
1282-FIND,Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs,"Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",Junbo Li; Peng Zhou; Rui Meng; Meet P. Vadera; Lihong Li; Yang Li,Meet P. Vadera,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP","Reasoning, Planning & Agents",turn; ppo; multi turn; advantage estimation; mdp; policy optimization; advantage; grpo; multi; estimation
1294-FIND,Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation,"Time series data is ubiquitous across various domains, including manufacturing, finance, and healthcare. High-quality annotations are essential for effectively understanding time series and facilitating downstream tasks. However, obtaining such annotations is challenging, particularly in mission-critical domains. In this paper, we propose TESSA, a multi-agent system designed to automatically generate both general and domain-specific annotations for time series data. TESSA introduces two agents: a general annotation agent and a domain-specific annotation agent. The general agent captures common patterns and knowledge across multiple source domains, leveraging both time-series-wise and text-wise features to generate general annotations. Meanwhile, the domain-specific agent utilizes limited annotations from the target domain to learn domain-specific terminology and generate targeted annotations. Extensive experiments on multiple synthetic and real-world datasets demonstrate that TESSA effectively generates high-quality annotations, outperforming existing methods. Our code and data are available at https://anonymous.4open.science/r/TESSA-8B7D.",Minhua Lin; Zhengzhang Chen; Yanchi Liu; Xujiang Zhao; Zongyu Wu; Junxiang Wang; Xiang Zhang; Suhang Wang; Haifeng Chen,Minhua Lin,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Domain NLP (Biomedical/Clinical/Legal/Scientific),time series; annotations; series; agent; domain; time; domain specific; general; series data; time series data
1296-FIND,Multi-Hall-SA: A Cross-lingual Benchmark for Multi-Type Hallucination Detection in Low-Resource South African Languages,"Hallucinations generated by Large Language Models (LLMs) pose significant challenges for their application to low-resource languages. We present Multi-Hall-SA, a cross-lingual benchmark for hallucination detection spanning English and four low-resource South African languages: isiZulu, isiXhosa, Sepedi, and Sesotho. Derived from government texts, this benchmark categorizes hallucinations into four types aligned with established taxonomies of factual errors: temporal shifts, entity errors, numerical inaccuracies, and location mistakes. Human validation confirms the quality and cross-lingual alignment of our synthetically generated hallucinations. Our cross-lingual alignment methodology enables direct performance comparison between high-resource and low-resource languages, revealing notable gaps in detection capabilities. Evaluation across four state-of-the-art models shows they detect up to 23.6% fewer hallucinations in South African languages compared to English. Knowledge augmentation reduces this disparity, decreasing cross-lingual performance gaps by 59.4% on average. Beyond introducing a validated resource for low-resource languages, Multi-Hall-SA provides a framework for evaluating and improving factual reliability across linguistic boundaries, advancing more inclusive and equitable AI development.",Sello Ralethe; Jan Buys,Sello Ralethe,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multilinguality & Low-Resource NLP,"LLM Evaluation, Benchmarks & Metrics",resource; cross lingual; lingual; south; low resource; languages; african languages; african; hallucinations; cross
1298-FIND,ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laymen,"Identifying legal statutes given a situation is one of the most fundamental tasks in Legal NLP, and traditionally has been modeled using facts from court judgments as input, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laymen, or non-professionals. This has led to the development of some layman LSI datasets, although there has been little research to explore the differences between court and layman data for LSI. In this work, we create ILSIC, a corpus of layman queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court cases to enable researchers to effectively compare between court and layman data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the layman dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. When comparing the different types of data, we observe that models trained purely on court queries are ineffective during test time on layman queries, while transfer learning from court to layman data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.",Shounak Paul; Raghav Dogra; Pawan Goyal; Saptarshi Ghosh,Raghav Dogra,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Domain NLP (Biomedical/Clinical/Legal/Scientific),Summarization & Generation,court; statutes; queries; legal; legal statutes; corpus; indian; conducted; identifying; input
1307-FIND,Query4Regex: Verifiable Regex Transformation through Formal Operations from NL and DSL Queries,"While large language models (LLMs) excel at generating structured data, such as code, their ability to precisely manipulating it based on instructions remains relatively under-explored. Regular expressions (regexes), critical in practice, are challenging to manipulate. Crucially, the correctness of transformations can be mathematically verified, making them exceptionally well-suited for measuring the symbolic reasoning of LLMs. We introduce Query4Regex, a new benchmark for evaluating verifiable transformations on regexes. Our benchmark tests two query formats: natural language instructions and a program-like domain-specific language (DSL) that specifies the sequence of operations. We evaluate a range of LLMs, verifying semantic correctness through rigorous deterministic finite automata (DFA) equivalence testing. Our empirical studies reveal: 1) the formal DSL significantly outperforms natural language, achieving up to 6.74%p average accuracy gains. 2) Performance for both formats degrades sharply as compositional complexity increases, highlighting a core challenge in multi-step reasoning. 3) Models often generate syntactically valid but semantically incorrect regexes, making errors difficult to detect without formal verification. Query4Regex provides a robust framework for analyzing the gap between LLMs' linguistic fluency and their symbolic reasoning, paving the way for more reliable and verifiable manipulation of formal languages.",Joonghyuk Hahn; Yo-Sub Han,Joonghyuk Hahn,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",regexes; formal; verifiable; symbolic reasoning; transformations; operations; formats; symbolic; instructions; correctness
1308-FIND,SrcMix: Mixing of Related Source Languages Benefits Extremely Low-resource Machine Translation,"Multilingual models are widely used for machine translation (MT). However, their effectiveness for extremely low-resource languages (ELRLs) depends critically on how related languages are incorporated during fine-tuning. In this work, we study the role of language mixing directionality, linguistic relatedness, and script compatibility in ELRL translation. We propose SrcMix, a simple source-side mixing strategy that combines related ELRLs during fine-tuning while constraining the decoder to a single target language. Compared to its target-side counterpart TgtMix, SrcMix improves performance by +3 ChrF++ and +5 BLEU in high-resource to ELRL translations, and by +5 ChrF++ and +12 BLEU in mid-resource to ELRL translations. We also release the first Angika MT dataset and provide a systematic comparison of LLM (Aya-101) and NMT (mT5-Large) models under ELRL settings, highlighting the importance of directional mixing and linguistic compatibility.",Sanjeev Kumar; Preethi Jyothi; Pushpak Bhattacharyya,Sanjeev Kumar,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Machine Translation,Multilinguality & Low-Resource NLP,mixing; chrf; extremely low resource; extremely low; resource; related; translation; compatibility; extremely; bleu
1309-FIND,IMRNNs: An Efficient Method for Interpretable Dense Retrieval via Embedding Modulation,"Interpretability in black-box dense retrievers remains a central challenge in Retrieval-Augmented Generation (RAG). Understanding how queries and documents semantically interact is critical for diagnosing retrieval behavior and improving model design. However, existing dense retrievers rely on static embeddings for both queries and documents, which obscures this bidirectional relationship. Post-hoc approaches such as re-rankers are computationally expensive, add inference latency, and still fail to reveal the underlying semantic alignment. To address these limitations, we propose Interpretable Modular Retrieval Neural Networks (IMRNNs), a lightweight framework that augments any dense retriever with dynamic, bidirectional modulation at inference time. IMRNNs employ two independent adapters: one conditions document embeddings on the current query, while the other refines the query embedding using corpus-level feedback from initially retrieved documents. This iterative modulation process enables the model to adapt representations dynamically and expose interpretable semantic dependencies between queries and documents. Empirically, IMRNNs not only enhance interpretability but also improve retrieval effectiveness. Across seven benchmark datasets, applying our method to standard dense retrievers yields average gains of +6.35% nDCG, +7.14% recall, and +7.04% MRR over state-of-the-art baselines. These results demonstrate that incorporating interpretability-driven modulation can both explain and enhance retrieval in RAG systems.",Yash Saxena; Ankur Padia; Kalpa Gunaratna; Manas Gaur,Yash Saxena,Poster,In-person,POSTER HALL,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",dense; modulation; queries documents; retrieval; dense retrievers; documents; retrievers; interpretability; bidirectional; interpretable
1323-FIND,MMUIE: Massive Multi-Domain Universal Information Extraction for Long Documents,"We present **MMUIE**, a large-scale universal dataset for multi-domain, document-level information extraction (IE) from long texts. Existing IE systems predominantly operate at the sentence level or within narrow domains due to annotation constraints. MMUIE addresses this gap by introducing an automated annotation pipeline that integrates traditional knowledge bases with large language models to extract fine-grained entities, aliases, and relation triples across 34 domains. The dataset comprises a weakly-supervised training set and a manually verified test set, featuring 723 entity types and 456 relation types. Empirical evaluations reveal that existing sentence-level IE models and even advanced LLMs underperform on this task, highlighting the need for better domain-aware document-level models. To this end, we develop DocUIE, a universal IE model fine-tuned on MMUIE, which achieves strong generalization and transferability across domains. MMUIE lays the foundation for robust, scalable, and universal information extraction from long-form text in diverse real-world scenarios. All code and data will be made publicly available.",Shuyi Zhang; Zhenbin Chen; Shuting Li; Kewei Tu; Li Jing; Zixia Jia; Zilong Zheng,Shuyi Zhang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Information Extraction & Structured Prediction,"Retrieval, Grounding & External Knowledge (RAG)",universal; information extraction; extraction; level; long; sentence level; multi domain; document level; relation; domains
1342-FIND,Learning to Judge: LLMs Designing and Applying Evaluation Rubrics,"Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and use their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMsвЂ”consistent within models but fragmented across themвЂ”and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.",Clemencia Siro; Pourya Aliannejadi; Mohammad Aliannejadi,Clemencia Siro,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,rubrics; evaluation; reliability; criteria; scoring; defined; human; applying; llms; consistent models
1348-FIND,PsyProbe: Proactive and Interpretable Dialogue through User State Modeling for Exploratory Counseling,"Recent advances in large language models have enabled mental health dialogue systems, yet existing approaches remain predominantly reactive, lacking systematic user state modeling for proactive therapeutic exploration. We introduce PsyProbe, a dialogue system designed for the exploration phase of counseling that systematically tracks user psychological states through the PPPPPI framework (Presenting, Predisposing, Precipitating, Perpetuating, Protective, Impact) augmented with cognitive error detection. PsyProbe combines State Builder for extracting structured psychological profiles, Memory Construction for tracking information gaps, Strategy Planner for Motivational Interviewing behavioral codes, and Response Generator with Question Ideation and Critic/Revision modules to generate contextually appropriate, proactive questions. We evaluate PsyProbe with 27 participants in real-world Korean counseling scenarios, including automatic evaluation across ablation modes, user evaluation, and expert evaluation by a certified counselor. The full PsyProbe model consistently outperforms baseline and ablation modes in automatic evaluation. User evaluation demonstrates significantly increased engagement intention and improved naturalness compared to baseline. Expert evaluation shows that PsyProbe substantially improves core issue understanding and achieves question rates comparable to professional counselors, validating the effectiveness of systematic state modeling and proactive questioning for therapeutic exploration.",Sohhyung Park; Hyunji Kang; Sungzoon Cho; Dongil Kim,Sohhyung Park,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Efficiency, Scaling & NLP Systems","Dialogue, Conversational & Interactive NLP",counseling; user; exploration; evaluation; expert evaluation; therapeutic; dialogue; modeling; state; automatic evaluation
1349-FIND,Learning from Child-directed Speech in Two-language Scenarios: A French-English Case-Study,"Research on developmentally plausible language models has so far centered on English, leaving open questions about multilingual settings. We present a systematic study of compact models by extending BabyBERTa to EnglishвЂ“French scenarios under strictly size-matched data conditions, addressing {\it monolingual}, {\it bilingual}, and {\it cross-lingual} settings. Our design contrasts two corpus types: (i) child-directed speech (2.5M tokens), following BabyBERTa and related work, and (ii) multi-domain corpora (10M tokens), extending the BabyLM framework to French. To support fair evaluation, we also introduce new resources: French versions of QAMR and QASRL, and an English and French multi-domain corpus. We evaluate the models on both syntax and semantics tasks, comparing with Wikipedia-only training. Results reveal context-dependent effects. Training on Wikipedia consistently favors semantic tasks, while child-directed speech improves grammatical judgments in monolingual settings. Bilingual pretraining yields notable gains for Textual Entailment, with French benefiting from curriculum learning. Importantly, the same relative patterns emerge across BabyBERTa, RoBERTa, and LTG-BERT, suggesting architecture-independent findings.",Liel Binyamin; Elior Sulem,Liel Binyamin,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Linguistics, Syntax & Semantics",Multilinguality & Low-Resource NLP,french; child directed; directed speech; child directed speech; directed; child; speech; wikipedia; multi domain; extending
1355-FIND,DeVisE: Towards the Behavioral Testing of Medical Large Language Models,"Large language models (LLMs) are increasingly applied in clinical decision support, yet current evaluations rarely reveal whether their outputs reflect genuine medical reasoning or superficial correlations. We introduce DeVisE (Demographics and Vital signs Evaluation), a behavioral testing framework that probe fine-grained clinical understanding through controlled counterfactuals. Using intensive care unit (ICU) discharge notes from MIMIC-IV, we construct both raw (real-world) and template-based (synthetic) variants with single-variable perturbations in demographic (age, gender, ethnicity) and vital sign attributes. We evaluate five LLMs, spanning general-purpose and medical variants, under zero-shot and fine-tuned settings. Model behavior is analyzed through (1) input-level sensitivity, capturing how counterfactuals alter likelihoods, and (2) downstream reasoning, measuring their effect on predicted ICU length-of-stay. Zero-shot models exhibit more coherent and clinically aligned reasoning patterns, whereas fine-tuned ones are stabler but less sensitive to meaningful changes. Demographic factors subtly yet consistently shape predictions, emphasizing the need for fairness-aware evaluation in medical LLMs.",Camila Zurdo Tagliabue; Heloisa Oss Boll; Aykut Erdem; Erkut Erdem; Iacer Calixto,Camila Zurdo Tagliabue,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Trustworthy, Safety, Privacy & Fairness",medical; devise; vital; behavioral; demographic; variants; testing; clinical; fine tuned; zero shot
1365-FIND,Improving Decoder-only Language Models for Sequence Labeling through Sequence Repetition,"Modern language models (LMs) are trained in an autoregressive manner, conditioned only on the prefix. In contrast, sequence labeling (SL) tasks assign labels to each individual input token, naturally benefiting from bidirectional context. This discrepancy has historically led SL to rely on inherently bidirectional encoder-only models. However, the rapid development of decoder-only models has raised the question of whether they can be adapted to SL. While causal mask removal has emerged as a viable technique for adapting decoder-only models to leverage the full context for SL, it requires considerable changes to the base model functionality. In this work, we explore sequence repetition (SR) as a less invasive alternative for enabling bidirectionality in decoder-only models. Through fine-tuning experiments, we show that SR inherently makes decoders bidirectional, improving the quality of token-level embeddings and surpassing encoders and unmasked decoders. Contrary to earlier claims, we find that increasing the number of repetitions does not degrade SL performance. Finally, we demonstrate that embeddings from intermediate layers are highly effective for SR, comparable to those from final layers, while being at least 1.33 times more efficient to compute. Our findings underscore that SR alleviates the structural limitations of decoders, enabling more efficient and adaptable LMs and broadening their applicability to other token-level tasks.",Matija Luka KukiД‡; Marko ДЊuljak; David DukiД‡; Martin Tutek; Jan Е najder,Marko Čuljak,Poster,In-person,POSTER HALL,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Information Extraction & Structured Prediction,,decoder models; decoders; decoder; sequence; bidirectional; repetition; sequence labeling; labeling; token; lms
1367-FIND,MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment,"Recent advancements in large vision-language models (VLMs) have primarily focused on English, with limited attention given to other languages. To address this gap, we introduce MEENA (also known as PersianMMMU), the first dataset designed to evaluate Persian VLMs across scientific, reasoning, and human-level understanding tasks. Our dataset comprises approximately 7,500 Persian and 3,000 English questions, covering a wide range of topics such as reasoning, mathematics, physics, diagrams, charts, and Persian art and literature. Key features of MEENA include: (1) diverse subject coverage spanning various educational levels, from primary to upper secondary school, (2) rich metadata, including difficulty levels and descriptive answers, (3) original Persian data that preserves cultural nuances, (4) a bilingual structure to assess cross-linguistic performance, and (5) a series of diverse experiments assessing various capabilities, including overall performance, the modelвЂ™s ability to attend to images, and its tendency to generate hallucinations. We hope this benchmark contributes to enhancing VLM capabilities beyond English.",Omid Ghahroodi; Arshia Hemmat; Marzia Nouri; Seyed Mohammad Hadi Hosseini; Doratossadat Dastgheib; Mohammad Vali Sanian; Alireza Sahebi; Reihaneh Zohrabi; Mohammad Hossein Rohban; Ehsaneddin Asgari; Mahdieh Soleymani Baghshah,Ehsanaldin Asgary,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multimodal & Speech/Audio,,persian; educational; english; vlms; levels; various; capabilities english; vlms primarily; english questions; assessment recent
1368-FIND,Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pretrained Models,"Tokenizer adaptation plays an important role in transferring pretrained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, a simple alternative that extends an existing tokenizer by continuing its merge operations on new data instead of retraining from scratch. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, and we release an open-source implementation supporting both Byte-level and SentencePiece BPE tokenizers.",Taido Purason; Pavel Chizhov; Ivan P. Yamshchikov; Mark Fishel,Taido Purason,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Efficiency, Scaling & NLP Systems",,vocabulary; tokenizer; bpe; new; tokenizers; extension; tokens; pruning; pretrained; adaptation
1369-FIND,AGIC: Attention-Guided Image Captioning to Improve Caption Relevance,"Despite significant progress in image captioning, generating accurate and descriptive captions remains a long-standing challenge. In this study, we propose Attention-Guided Image Captioning (AGIC), which amplifies salient visual regions directly in the feature space to guide caption generation. We further introduce a hybrid decoding strategy that combines deterministic and probabilistic sampling to balance fluency and diversity. To evaluate AGIC, we conduct extensive experiments on the Flickr8k and Flickr30k datasets. The results\footnote{We plan to release the code and data.} show that AGIC matches or surpasses several state-of-the-art models while achieving faster inference. Moreover, AGIC demonstrates strong performance across multiple evaluation metrics, offering a scalable and interpretable solution for image captioning.",L D M S Sai Teja; Ashok Urlana; Pruthwik Mishra,L D M S Sai Teja,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Multimodal & Speech/Audio,Summarization & Generation,image captioning; captioning; image; guided image; caption; attention; guided; matches surpasses; metrics offering; conduct extensive experiments
1372-FIND,VisualвЂ“Linguistic Abductive Reasoning with LLMs for Knowledge-based Visual Question Answering,"Recent attempts to leverage large language models (LLMs) for reasoning and pre-trained knowledge in multi-modal reasoning focus on two main approaches: aligning image features with linguistic space, and converting images into textual cues to exploit the implicit reasoning capabilities of LLMs. Although they integrate visual information into the reasoning pipeline, they often treat visual perception and language reasoning as separate processes, limiting the potential for fully unified multi-modal reasoning. In this paper, we propose a novel method, VisualвЂ“Linguistic Abductive Reasoning (ViLA), inspired by human abductive reasoning processes. ViLA hypothesizes a plausible answer, generates the corresponding visual and textual premises, and employs fuzzy scoring to select the most coherent combination, thus deriving the final inference. This process integrates visual and linguistic modalities into interpretable abductive reasoning chains, enabling unified multi-modal reasoning. Without fine-tuning LLMs or retrieving external knowledge, ViLA improves performance by 2.31% on AOKVQA, 1.7% on OKVQA, and 1.7% on GQA over previous state-of-the-art models, while also improving interpretability and stability.",Jieun Kim; Yujin Jeong; Sung-Bae Cho,Jieun Kim,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","Linguistics, Syntax & Semantics",reasoning; multi modal reasoning; modal reasoning; visual; multi modal; modal; unified multi; linguistic; visualвђ; processes
1384-FIND,FactAppeal: Identifying Epistemic Factual Appeals in News Media,"Detecting how factual claims are supported by appeals to authority, expertise, or evidence is critical for assessing credibility in public discourse. We propose the novel task of Epistemic Appeal Identification, which not only detects whether a statement conveys a factual claim but also reveals how it is anchored by external sources or evidence. To advance research on this task, we present FactAppeal, a manually annotated dataset of 3,226 English-language news sentences capturing both claim factuality and the nuanced epistemic structures underlying these claims. Unlike prior resources that focus solely on claim detection and verification, FactAppeal provides theory-driven, fine-grained annotations of source attribution, quotation method (direct or indirect), and appeal type (e.g., expert testimony, official statements, direct evidence). Our experiments show that generative models consistently outperform encoder-based baselines, underscoring both the complexity of modeling epistemic appeals and the promise of large-scale generative architectures for advancing factuality detection in news media.",Guy Mor-Lan; Tamir Sheafer; Shaul R. Shenhav,Guy Mor-Lan,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Retrieval, Grounding & External Knowledge (RAG)",Interpretability & Model Analysis,epistemic; appeals; claim; news; appeal; factual; factuality; evidence; claims; media
1387-FIND,Vietnamese Automatic Speech Recognition: A Revisit,"Automatic Speech Recognition (ASR) performance is heavily dependent on the availability of large-scale, high-quality datasets. For low-resource languages, existing open-source ASR datasets often suffer from insufficient quality and inconsistent annotation, hindering the development of robust models. To address these challenges, we propose a novel and generalizable data aggregation and preprocessing pipeline designed to construct high-quality ASR datasets from diverse, potentially noisy, open-source sources. Our pipeline incorporates rigorous processing steps to ensure data diversity, balance, and the inclusion of crucial features like word-level timestamps. We demonstrate the effectiveness of our methodology by applying it to Vietnamese, resulting in a unified, high-quality 500-hour dataset that provides a foundation for training and evaluating state-of-the-art Vietnamese ASR systems.",Thi Vu; Linh The Nguyen; Dat Quoc Nguyen,Thi Vu,Poster,In-person,POSTER HALL,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,Multimodal & Speech/Audio,Multilinguality & Low-Resource NLP,asr; vietnamese; high quality; automatic speech recognition; automatic speech; quality; speech recognition; recognition; high; speech
1388-FIND,MapCoder-Lite: Distilling Multi-Agent Coding into a Single Small LLM,"Large language models (LLMs) have advanced code generation from single-function tasks to competitive-programming problems, but existing multi-agent solutions either rely on costly large-scale ($> 30 B$) models or collapse when downsized to small open-source models. We present MapCoder-Lite, a framework for distilling the complex reasoning of large, multi-agent coding systems into a single 7B model. Our contribution is a novel, three-pillar methodology that synergistically generates, refines, and encodes multi-agent knowledge: (i) pass-based trajectory distillation from strong LLMs fixes format fragility in retrieval and debugging, (ii) supervisor-guided correction strengthens planning and coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests shows that MapCoder-Lite more than doubles xCodeEval accuracy (13.2% в†’ 28.3%), eliminates all format failures, while reducing GPU memory and token-generation time by $4\times$ compared to a 32B model. It also achieves over 10% gains on simpler coding benchmarks, demonstrating broad improvements beyond competitive programming. These results demonstrate that careful agent-wise fine-tuning unleashes high-quality multi-agent coding on a small language model.",Woongkyu Lee; Junhee Cho; Jungwook Choi,Woongkyu Lee,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",coding; agent; multi agent; lite; multi; distilling; programming; small; single; format
1403-FIND,When Do Language Models Endorse Limitations on Human Rights Principles?,"As Large Language Models (LLMs) increasingly mediate global information access with the potential to shape public discourse, their alignment with universal human rights principles becomes important to ensure that these rights are abided by in high stakes AI-mediated interactions. In this paper, we evaluate how LLMs navigate trade-offs involving the Universal Declaration of Human Rights (UDHR), leveraging 1,152 synthetically generated scenarios across 24 rights articles in eight languages. Our analysis of eleven major LLMs reveals systematic biases where models: (1) accept limiting Economic, Social, and Cultural rights more often than Political and Civil rights, (2) demonstrate significant cross-linguistic variation with elevated endorsement rates of rights-limiting actions in Chinese and Hindi compared to English or Romanian, (3) show substantial susceptibility to prompt-based steering, and (4) exhibit noticeable differences between Likert and open-ended responses, highlighting critical challenges in LLM preference assessment.",Keenan Samway; Miu Nicole Takagi; Rada Mihalcea; Bernhard SchГ¶lkopf; Ilias Chalkidis; Daniel Hershcovich; Zhijing Jin,Keenan Samway,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Interpretability & Model Analysis,"Linguistics, Syntax & Semantics",rights; human rights; principles; universal; limiting; human; open ended responses; reveals systematic; mediated; critical challenges
1406-FIND,Abstractive Summarization of Bengali Academic Videos Based on Audio Subtitles,"The rapid growth of academic video content makes it difficult for students and educators to find relevant information efficiently. This is especially challenging for low-resource languages like Bengali due to the lack of a video summarization tool. This paper presents the first end-to-end pipeline for the abstractive summarization of Bengali academic videos. The proposed system preprocesses audio to improve transcription quality and converts speech to text using GoogleвЂ™s Speech Recognition API. The text is segmented using a smart chunking method to be compatible with the model's context window. For summarization, we fine-tuned the $\textit{BanglaT5}$ model on a new benchmark dataset of 10,029 text-summary pairs obtained from educational videos. To generate relevant titles, we fine-tuned the $\textit{mT5-multilingual-XLSum}$ model on our curated dataset of 1,005 summary-title pairs. Our fine-tuned summarization model shows strong performance, achieving F1 scores of 0.8793 (BERTScore), 0.3894 (ROUGE-1), and 0.2557 (ROUGE-L), outperforming other models. Our title generation model achieved ROUGE-1 and ROUGE-L F1 scores of 0.4476 and 0.3720, respectively. The summaries include timestamps for easy video navigation. This work aims to improve the accessibility of educational content in Bengali. It also contributes valuable datasets and a robust baseline system that demonstrates strong zero-shot capabilities on other spoken contents.",Lamisa Bintee Mizan Deya; Farhatun Shama; Abdul Aziz; Md Kaykobad Reza; Md Shahidul Salim,Abdul Aziz,Poster,In-person,POSTER HALL,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Summarization & Generation,Multimodal & Speech/Audio,bengali; rouge; summarization; videos; academic; video; title; abstractive summarization; fine tuned; abstractive
1412-FIND,Active Learning with Non-Uniform Costs for African Natural Language Processing,"Labeling datasets for African languages presents substantial challenges due to the diverse settings in which annotations are collected, resulting in highly variable labeling costs. These costs can vary with task complexity, annotator expertise, and data availability. Yet, most active learning (AL) frameworks assume uniform annotation costs, limiting their applicability in real-world, resource-constrained scenarios. To address this, we introduce KnapsackBALD, a novel cost-aware active learning method that integrates the BatchBALD acquisition strategy with a 0-1 Knapsack optimization objective to select informative and budget-efficient samples. We evaluate KnapsackBALD on the MasakhaNEWS dataset, a multilingual news classification benchmark covering 11 African languages. Our method consistently outperforms seven strong active learning baselines, including BALD, BatchBALD, and stochastic sampling variants such as PowerBALD and Softmax-BALD, across all three cost scenarios. The performance gap widens as annotation cost imbalances become more extreme, demonstrating the robustness of KnapsackBALD under practical constraints. These findings underscore the need for cost-sensitive acquisition in AL pipelines for African language NLP and beyond.",Bonaventure F. P. Dossou; Ines Arous; Audrey Durand; Jackie Chi Kit Cheung,Bonaventure F. P. Dossou,Poster,In-person,POSTER HALL,Session 5: Oral/Posters D,Thur. Mar 26,09:00-10:30,Multilinguality & Low-Resource NLP,"LLM Evaluation, Benchmarks & Metrics",active learning; african; active; costs; cost; african languages; acquisition; uniform; learning; labeling
15-IND,Iterative Structured Pruning for Large Language Models with Multi-Domain Calibration,"Large Language Models (LLMs) have achieved remarkable success across a wide spectrum of natural language processing tasks. However, their ever-growing scale introduces significant barriers to real-world deployment, including substantial computational overhead, memory footprint, and inference latency. While model pruning presents a viable solution to these challenges, existing unstructured pruning techniques often yield irregular sparsity patterns that necessitate specialized hardware or software support. In this work, we explore structured pruning, which eliminates entire architectural components and maintains compatibility with standard hardware accelerators. We introduce a novel structured pruning framework that leverages a hybrid multi-domain calibration set and an iterative calibration strategy to effectively identify and remove redundant channels. Extensive experiments on various models across diverse downstream tasks show that our approach achieves significant compression with minimal performance degradation.","Guangxin Wu, Hao Zhang, Zhang Zhibin, Jiafeng Guo, Xueqi Cheng",Guangxin Wu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",,pruning; calibration; hardware; multi domain; structured; iterative; channels; memory footprint; success wide; compression minimal performance
19-IND,SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System,"Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script–Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean scam cases, we built 22,712 structured scammer-sequence training instances. Experimental results show that the 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. Moreover, in phone scam simulation experiments, it significantly enhanced and sustained users’ suspicion levels, improving their cognitive awareness of scams. ScriptMind represents a step toward human-centered, cognitively adaptive LLMs for scam defense.","Heedou Kim, changsik Kim, Sanghwa Shin, Jaewoo Kang",Heedou Kim,Oral,In-person,SALLE Le Chellah,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",cognitive; script; engineering; detection; scams; social; defense; simulation; inference; llm based
22-IND,From Paper to Structured JSON: An Agentic AI Workflow for Compliant BMR Digital Transformation,"Agentic AI workflow converts noisy pharmaceutical batch records into validated JSON using hybrid OCR, vision–language and schema-guided LLMs, cutting QA review from hours to minutes while preserving GMP-critical structure.","Bhavik Agarwal, Nidhi Bendre, Viktoria Rojkova",Bhavik Agarwal,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",,agentic ai; json; workflow; agentic; pharmaceutical; compliant; llms cutting; hours minutes; schema guided; converts
23-IND,Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports,"Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization.","Nikita Neveditsin, Pawan Lingras, Salil Patil, Swarup Patil, Vijay Kumar Mago",Dr. Pawan Lingras,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Efficiency, Scaling & NLP Systems",ocr; reports; compact; digitization; multimodal language models; multimodal language; alternatives; multimodal; healthcare; noisy
28-IND,PersonaTrace: Synthesizing Realistic Digital Footprints with LLM Agents,"Digital footprints—records of individuals’ interactions with digital systems—are essential for studying behavior, developing personalized applications, and training machine learning models. However, research in this area is often hindered by the scarcity of diverse and accessible data. To address this limitation, we propose a novel method for synthesizing realistic digital footprints using large language model (LLM) agents. Starting from a structured user profile, our approach generates diverse and plausible sequences of user events, ultimately producing corresponding digital artifacts such as emails, messages, calendar entries, reminders, etc. Intrinsic evaluation results demonstrate that the generated dataset is more diverse and realistic than existing baselines. Moreover, models fine-tuned on our synthetic data outperform those trained on other synthetic datasets when evaluated on real-world out-of-distribution tasks.","Minjia Wang, Yunfeng Wang, Xiao Ma, Dexin Lv, Qifan Guo, Lynn Zheng, Benliang Wang, Lei Wang, Jiannan Li, Yongwei Xing, Junzhe Xu, Zheng Sun",Minjia Wang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Efficiency, Scaling & NLP Systems",digital; footprints; realistic; synthesizing; llm agents; diverse; synthetic; user; agents; calendar
32-IND,Evaluating the Pre-Consultation Ability of LLMs using Diagnostic Guidelines,"We introduce EPAG, a benchmark dataset and framework designed for evaluating the pre-consultation ability of LLMs using diagnostic guidelines. LLMs are evaluated directly through HPI-diagnostic guideline comparison and indirectly through disease diagnosis. In our experiments, we observe that small open-source models fine-tuned with a well-curated, task-specific dataset can outperform frontier LLMs in pre-consultation. Additionally, we find that increased amount of HPI (History of Present Illness) does not necessarily lead to improved diagnostic performance. Further experiments reveal that the language of pre-consultation influences the characteristics of the dialogue. By open-sourcing our dataset and evaluation pipeline on https://github.com/seemdog/EPAG, we aim to contribute to the evaluation and further development of LLM applications in real-world clinical settings.","Jean Seo, Gibaeg Kim, Kihun Shin, Seungseop Lim, Hyunkyung Lee, Wooseok Han, Jongwon Lee, Eunho Yang",Gibaeg Kim,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Dialogue, Conversational & Interactive NLP",diagnostic; pre; ability llms; guidelines; llms using; dataset; llms; indirectly; observe small; does necessarily
34-IND,SELENE: Selective and Evidence-Weighted LLM Debating for Efficient and Reliable Reasoning,"Multi-Agent Debate (MAD) frameworks improve factual reliability in large language models (LLMs) by allowing agents to critique and refine one another’s reasoning. Yet, existing MAD systems are computationally expensive and prone to degradation under prolonged debates due to redundant exchanges and unstable judging. We propose a lightweight,industry-deployable alternative that unifies Selective Debate Initiation (SDI) with Evidence Weighted Self-Consistency (EWSC) for adaptive, debate-on-demand reasoning. SDI dynamically predicts when debate is necessary by detecting confidence-likelihood misalignment and semantic disagreement, skipping well-aligned queries to conserve computation. EWSC replaces a single-judge verdict with a variance-aware, evidence-weighted aggregation across paraphrased evaluations, yielding more stable factual judgments. Combined, SDI and EWSC reduce token consumption by nearly 50% while improving both accuracy and calibration. Evaluated on BoolQ, CosmosQA, and an internal QnA benchmark, our framework achieves higher factual robustness and efficiency, demonstrating that scalable, epistemically reliable multi-agent reasoning is practical for real-world LLM deployments. ","Akshay Verma, Swapnil Gupta, Deepak Gupta, Prateek Sircar, Siddharth Pillai",Akshay Verma,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",debate; weighted; mad; factual; evidence; selective; reasoning; multi agent; reliable; agent
38-IND,SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code,"We introduce SymPyBench, a large-scale synthetic benchmark of 15K university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. In addition to standard accuracy, we introduce three new metrics: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems.","Shima Imani, Seungwhan Moon, Adel Ahmadyan, Lu Zhang, Ahmed Kirmani, Babak Damavandi",Shima Imani,Oral,In-person,SALLE Le Chellah,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics",,executable python; executable python code; python code; options; scientific reasoning; reasoning; executable; python; numerical; symbolic
39-IND,KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference,"Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78\% total memory reduction with minimal (1-3\%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference.","Sai Gokhale, Devleena Das, Rajeev Patwari, Ashish Sirasao, Elliott Delaye",Sai Gokhale,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Efficiency, Scaling & NLP Systems",,quantization; cache; kv cache; optimization; long context; optimization techniques; optimal configurations; memory; context; configurations
45-IND,MizanQA: A Benchmark for Multi-Answer Moroccan Legal QA,"We present MizanQA, a benchmark for assessing LLMs on Moroccan legal MCQs, many with multiple correct answers. Covering 1,776 expert-verified questions in Modern Standard Arabic enriched with Moroccan idioms, the dataset reflects influences from Maliki jurisprudence, customary law, and French legal traditions. Unlike single-answer settings, MizanQA features variable option counts, creating added difficulty. We evaluate multilingual and Arabic-centric models in zero-shot, native-Arabic prompts, measuring accuracy, a precision-penalized F1-like score, and calibration errors. Results show large performance gaps and miscalibration, particularly under stricter penalties. By scoping this benchmark to parametric knowledge only, we provide a baseline for future retrieval-augmented and rationale-focused setups.","Adil Bahaj, Mounir Ghogho",Adil Bahaj,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics",Domain NLP (Biomedical/Clinical/Legal/Scientific),arabic; legal; benchmark; answer; mcqs; counts; traditions; dataset reflects; benchmark assessing; single answer
46-IND,Router-Suggest: A Router-based Framework for Auto-Completions in Visually-Grounded Conversations,"Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.","SANDEEP MISHRA, Devichand Budagam, Anubhab Mandal, Bishal Santra, Pawan Goyal, Manish Gupta",Devichand Budagam,Oral,In-person,SALLE Le Chellah,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,"Dialogue, Conversational & Interactive NLP",router; auto; user; auto completion; completions; completion; multimodal; vlms; suggest; context
50-IND,"Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS","Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance. This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.","Mahta Fetrat Qharabagh, Donya Navabi, Zahra Dehghanian, Morteza Abolghasemi, Hamid R. Rabiee",Mahta Fetrat Qharabagh,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",Multimodal & Speech/Audio,tts; real time; time; context aware; real; service; lightweight; context; oriented; aware
52-IND,Retrieval Enhancements for RAG: Insights from a Deployed Customer Support Chatbot,"Retrieval-Augmented Generation (RAG) systems depend critically on retrieval quality to enable accurate, contextually relevant LLM responses. While LLMs excel at synthesis, their RAG performance is bottlenecked by document relevance. We evaluate advanced retrieval techniques including embedding model comparison, Reciprocal Rank Fusion (RRF), embedding concatenation and list-wise and adaptive LLM-based re-ranking, demonstrating that zero-shot LLMs outperform traditional cross-encoders in identifying high-relevance passages. We also explore context-aware embeddings, diverse chunking strategies, and model fine-tuning. All methods are rigorously evaluated on a proprietary dataset powering our deployed production chatbot, with validation on three public benchmarks: FiQA, HotpotQA, and SciDocs. Results show consistent gains in Recall@10, closing the gap with Recall@50 and yielding actionable pipeline recommendations. By prioritizing retrieval enhancements, we significantly elevate downstream LLM response quality in real-world, customer-facing applications.","Daniel González Juclà, Mohit Tuteja, Marcos Esteve Casademunt, Keshav Unnikrishnan, Yasir Usmani, Arvind Roshaan",Marcos Esteve Casademunt,Oral,In-person,SALLE Le Chellah,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,retrieval; enhancements; chatbot; rag; customer; deployed; recall; relevance; embedding; adaptive llm based
53-IND,Scaling Intent Understanding: A Framework for Classification with Clarification using Lightweight LLMs,"Despite extensive research in intent classification, most task-oriented dialogue systems still rigidly assign intents to user utterances without addressing ambiguity, often leading to misrouted requests, irrelevant responses, and user frustration. Proprietary large-language models (LLMs) can generate effective clarifying questions but are too costly for large-scale deployment. Smaller open-source LLMs are more economical, but struggle to ask appropriate clarifying questions. This paper introduces a domain-agnostic framework that equips lightweight, production-ready open-source LLMs with the ability to perform intent classification alongside precise ambiguity resolution via clarifying questions. We validate our framework on both proprietary and public intent classification datasets, demonstrating its ability to perform intent classification as well as generate clarification questions in case of ambiguity. To compare models, those trained with our framework and external baselines, we also propose an evaluation methodology that jointly assesses the accuracy of intent classification and the timing and quality of clarifying questions. Our instruction-tuned models achieve performance comparable to leading proprietary LLMs while offering an 8X reduction in inference cost, enabling broader, cost-efficient deployment. When deployed in the customer-care system of an e-commerce enterprise, our model reduced the misrouting rate by 8%, resulting in a significant improvement in automation rates, which potentially translates in dollar savings by reducing escalations to human agents.","Subhadip Nandi, Tanishka Agarwal, Anshika Singh, Priyanka Bhatt",Tanishka Agarwal,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","Dialogue, Conversational & Interactive NLP",intent classification; intent; classification; questions; ambiguity; proprietary; ability perform; open source llms; source llms; framework
56-IND,Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence,"Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.","Sumanth Balaji, Piyush Mishra, Aashraya Sachdeva, Suraj Agrawal",Sumanth Balaji,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",customer support; policy; customer; support; ivr; adherence; agent; like gpt; business; agents
57-IND,HotelQuEST: Balancing Quality and Efficiency in Agentic Search,"Agentic search has emerged as a promising paradigm for adaptive retrieval systems powered by large language models (LLMs). However, existing benchmarks primarily focus on quality, overlooking efficiency factors that are critical for real-world deployment. Moreover, real-world user queries often contain underspecified preferences, a challenge that remains largely underexplored in current agentic search evaluation. As a result, many agentic search systems remain impractical despite their impressive performance. In this work, we introduce HotelQuEST, a benchmark comprising 214 hotel search queries that range from simple factual requests to complex queries, enabling evaluation across the full spectrum of query difficulty. We further address the challenge of evaluating underspecified user preferences by collecting clarifications that make annotators' implicit preferences explicit for evaluation. We find that LLM-based agents achieve higher accuracy than traditional retrievers, but at substantially higher costs due to redundant tool calls and suboptimal routing that fails to match query complexity to model capability. Our analysis exposes inefficiencies in current agentic search systems and demonstrates substantial potential for cost-aware optimization.","Guy Hadad, Shadi Iskander, Sofia Tolmach, Oren Kalinsky, Haggai Roitman, Ran Levy",Shadi Iskander,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Efficiency, Scaling & NLP Systems","LLM Evaluation, Benchmarks & Metrics",search; agentic; preferences; search systems; underspecified; queries; systems; higher; query; efficiency
59-IND,TASER: Table Agents for Schema-guided Extraction and Recommendation,"Real-world financial filings report critical information about an entity's investment holdings, essential for assessing that entity's risk, profitability, and relationship profile. Yet, these details are often buried in messy, multi-page, fragmented tables that are difficult to parse, hindering downstream QA and data normalization. Specifically, 99.4% of the tables in our financial table dataset lack bounding boxes, with the largest table spanning 44 pages. To address this, we present TASER (Table Agents for Schema-guided Extraction and Recommendation), a continuously learning, agentic table extraction system that converts highly unstructured, multi-page, heterogeneous tables into normalized, schema-conforming outputs. Guided by an initial portfolio schema, TASER executes table detection, classification, extraction, and recommendations in a single pipeline. Our Recommender Agent reviews unmatched outputs and proposes schema revisions, enabling TASER to outperform vision-based table detection models such as Table Transformer by 10.1%. Within this continuous learning process, larger batch sizes yield a 104.3% increase in useful schema recommendations and a 9.8% increase in total extractions. To train TASER, we manually labeled 22,584 pages and 3,213 tables covering $731.7 billion in holdings, culminating in TASERTab to facilitate research on real-world financial tables and structured outputs. Our results highlight the promise of continuously learning agents for robust extractions from complex tabular data.","Nicole Cho, Kirsty Fielding, William Watson, Sumitra Ganesh, Manuela Veloso",Nicole Cho,Oral,In-person,SALLE Le Chellah,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Reasoning, Planning & Agents",,table; schema; tables; extraction; financial; multi page; extractions; schema guided; recommendation; page
64-IND,TAGQuant: Token-Aware Clustering for Group-Wise Quantization,"Grouping, e.g., grouping channels, which is widely used in current integer-based quantization, has become essential for the emerging MXFP4 format. Ideally, each group should contain channels with similar quantization scales. To guide such groups, existing work clusters the channels using scalar proxy, ignoring the token dimension, which we find suboptimal. In this paper, we propose TAGQuant, a simple yet powerful enhancement for such ``group-wise'' quantization. By strategically shuffling channels to group those with similar token-wise activation distributions, TAGQuant ensures better clustering of large- and small-range values. This shuffle operation is hardware-efficient, and seamlessly integrated into the quantization process with only 0.01x latency overhead. TAGQuant reduces relative GSM8K error in both INT4 and MXFP4 formats, by up to 86\% in Llama-3.1-8B-Instruct compared to baselines, validating the effectiveness of our channel shuffling approach for group-wise quantization. Code is publicly available.","Jaeseong Lee, seung-won hwang, Aurick Qiao, Zhewei Yao, Yuxiong He",Jaeseong Lee,Oral,In-person,SALLE Le Chellah,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Efficiency, Scaling & NLP Systems","Trustworthy, Safety, Privacy & Fairness",quantization; channels; group; group wise; wise; grouping; clustering; token; similar; seamlessly integrated
65-IND,Beyond Grid Search: Leveraging Bayesian Optimization for Accelerating RAG Pipeline Optimization,"Finding optimal configurations for Retrieval-Augmented Generation (RAG) pipelines via grid search is computationally prohibitive, limiting real-world scalability. We investigate Bayesian Optimization (BO) as an efficient alternative, systematically comparing seven BO strategies combining four surrogate models and two multi-fidelity methods across FiQA, SciFact, and HotpotQA datasets. Our framework explores both global pipeline and local component-wise optimization, targeting final RAG performance and resource efficiency. Our results show that BO reduces optimization time by up to 84\% compared to grid search while maintaining comparable accuracy, with local optimization offering the most practical balance for deployment. Notably, performance gains plateau with larger evaluation budgets, suggesting that moderate resource investments suffice for effective RAG tuning. We provide actionable guidelines that empower industry practitioners to efficiently configure and deploy high-performing RAG systems under real-world constraints.","Anum Afzal, Xueru Zheng, Florian Matthes",Anum Afzal,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",optimization; rag; grid; grid search; bayesian optimization; bayesian; search; local; pipeline; limiting real
66-IND,BornoDrishti: Leveraging Vision Encoders and Domain-Adaptive Learning for Bangla OCR on Diverse Documents,"OCR for Bangla scripts remains a challenging problem, with existing solutions limited to single-domain processing. Current approaches lack a unified vision encoder that can understand diverse Bangla script variations, hindering practical deployment. We present BornoDrishti, the first unified OCR system based on the vision transformer that accurately recognizes both printed and handwritten Bangla scripts within a single model. Our approach introduces a novel domain objective that enables the model to learn domain-invariant representations while preserving script-specific features, eliminating the need for separate domain experts. BornoDrishti achieves competitive accuracy across both domains, setting state-of-the-art performance for printed scripts and demonstrating that a single unified model can match or exceed specialized uni-domain systems. We evaluate our model against state-of-the-art domain-specific and cross-domain OCR systems. This work establishes a foundation for advancing practical applications by using a unified multi-domain OCR system for complex Bangla scripts.","S M Jishanul Islam, Md Mehedi Hasan, Masbul Haider Ovi, AKM SHAHARIAR AZAD RABBY, Fuad Rahman",S M Jishanul Islam,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",,ocr; domain; scripts; unified; printed; vision; script; single; practical; limited single
73-IND,MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation,"Generative agents offer promising capabilities for simulating realistic urban behaviors. However, existing methods often rely on static profiles, oversimplified behavioral logic, and synchronous inference pipelines that hinder scalability. We present MobileCity, a lightweight generative-agent framework for city-scale simulation powered by cognitively-grounded generative agents. Each agent acts based on its needs, habits, and obligations, evolving over time. Agents are initialized from survey-based demographic data and navigate a realistic multimodal transportation network spanning multiple types of vehicles. To achieve scalability, we introduce asynchronous batched LLM inference during action selection and a low-token communication mechanism. Experiments with 4,000 agents demonstrate that MobileCity generates more human-like urban dynamics than baselines while maintaining high computational efficiency. Our code is publicly available at https://github.com/Tony-Yip/MobileCity.","Xiaotong Ye, Nicolas Bougie, Toshihiko Yamasaki, Narimawa Watanabe",Xiaotong Ye,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",urban; agents; generative agents; generative; simulation; scalability; realistic; agent; generative agent; token communication
74-IND,Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks,"When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations (micro domains). A previous study shows micro domain-adaptive pre-training (mDAPT) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) eliciting facts relevant to questions from an LLM's own knowledge, (2) reasoning over the facts to obtain conclusions, and (3) composing long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90\%), emphasizing the need to enhance reasoning capability.","Masaya Tsunokake, Yuta Koreeda, Terufumi Morishita, Koichi Nagatsuka, Hikaru Tomonari, Yasuhiro Sogawa",Masaya Tsunokake,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",operations; micro; bottlenecks; domain adaptive; elicitation; real world; conclusions; subtasks; world; real
80-IND,A Compliance-Preserving Retrieval System for Aircraft MRO Task Search,"Aircraft Maintenance Technicians (AMTs) spend up to 30\% of work time searching manuals—a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90\% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9\% top-10 success rate and 95\% reduction in lookup time—from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.",Byungho Jo,Byungho Jo,Oral,In-person,SALLE Le Chellah,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Retrieval, Grounding & External Knowledge (RAG)",,certified; viewers; retrieval; compliance; search; preserving; operate strict; controlled studies; lookup; 90 10
81-IND,No Label? No Problem: Unsupervised Continual Learning for Adaptive Medical ASR,"Automatic Speech Recognition (ASR) plays an important role in healthcare but faces unique challenges. Medical audio often contains specialized terminology, such as medication names, which existing ASR systems struggle to transcribe accurately. High error rates arise from pronunciation variability, the continual introduction of new terms, and the scarcity of high-quality labeled data—whose collection is costly and requires medical expertise. Although synthetic datasets partially alleviate this problem, they fail to capture the noise and variability of real-world recordings. Moreover, ASR models trained in controlled environments are highly sensitive to noise, leading to degraded performance in clinical settings. To address these limitations, we propose an unsupervised continual learning ASR framework that adapts to new data while preserving prior knowledge. This enables efficient domain adaptation without extensive retraining. Experiments on real-world medical audio demonstrate significant improvements over state-of-the-art baselines.","Meizhu Liu, Tao Sheng",Meizhu Liu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,Domain NLP (Biomedical/Clinical/Legal/Scientific),asr; medical; continual; variability; continual learning; unsupervised; noise; audio; problem; introduction new terms
85-IND,EduPulse: A Practical LLM-Enhanced Opinion Mining System for Vietnamese Student Feedback in Educational Platforms,"Opinion mining from real-world student feedback presents significant practical challenges, such as handling linguistic noise (slang, teencode) and the need for scalable and maintainable systems, which are often overlooked in academic research. This paper introduces EduPulse, a practical opinion mining system designed specifically to analyze student feedback in Vietnamese. Our application performs four opinion analysis tasks, including Sentiment Classification, Category-based Sentiment Classification, Suggestion Detection, and Opinion Summarization. We design the hybrid architecture that strategically balances performance, cost, and maintainability. This architecture leverages the robustness of Large Language Models (LLMs) for complex, noise-sensitive tasks as sentiment classification and suggestion detection, while employing a specialized, lightweight neural model for high-throughput, low-cost solutions. Our experiments show that applying the LLM-based approach achieves high robustness, justifying its operational cost by eliminating expensive retraining cycles. Furthermore, we demonstrate that our collaborative modular architecture significantly improves task performance (+7.6\%) compared to traditional approaches, offering a practical design for industry-focused Natural Language Processing applications.","Nguyen Xuan Phuc, Phi Nguyen Xuan, Vinh-Tiep Nguyen, Thìn Đặng Văn, Ngan Luu-Thuy Nguyen",Nguyen Xuan Phuc,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems","Dialogue, Conversational & Interactive NLP",opinion; sentiment classification; mining; sentiment; student; practical; architecture; suggestion; feedback; vietnamese
87-IND,When Speed Meets Intelligence: Scalable Conversational NER in an Ever-evolving World,"Modern conversational AI systems require sophisticated Named Entity Recognition (NER) capabilities that can handle complex, contextual dialogue patterns. While Large Language Models (LLMs) excel at understanding conversational semantics, their inference latency and inability to efficiently incorporate emerging entities make them impractical for production deployment. Moreover, the scarcity of conversational NER data creates a critical bottleneck for developing effective models. We address these challenges through two main contributions. First, we introduce an automated pipeline for generating multilingual conversational NER datasets with minimal human validation, producing 4,082 English and 3,925 Spanish utterances. Second, we present a scalable framework that leverages LLMs as semantic filters combined with catalog-based entity grounding to label live traffic data, enabling knowledge distillation into faster, production-ready models. On internal conversational datasets, our teacher model demonstrates 39.55% relative F1-score improvement in English and 44.93% in Spanish compared to production systems. On public benchmarks, we achieve 97.12% F1-score on CoNLL-2003 and 83.09% on OntoNotes 5.0, outperforming prior state-of-the-art by 24.82 and 8.19 percentage points, respectively. Finally, student models distilled from our teacher approach achieve 13.84% relative improvement on English conversational data, bridging the gap between LLM capabilities and real-world deployment constraints.","Karim Ghonim, Antonio Roberto, Davide Bernardi",Karim Ghonim,Oral,In-person,SALLE Le Chellah,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,"Efficiency, Scaling & NLP Systems","Dialogue, Conversational & Interactive NLP",conversational; ner; production; spanish; teacher; f1 score; english; relative; deployment; improvement
91-IND,ReflectiveRAG: Rethinking Adaptivity in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) systems degrade sharply under extreme noise,where irrelevant or redundant passages dominate. Current methods-fixed top-k retrieval, cross-encoder reranking, or policy based iteration-depend on static heuristics or costly reinforcement learning, failing to assess evidence sufficiency, detect subtle mismatches, or reduce redundancy, leading to hallucinations and poor grounding. We introduce ReflectiveRAG, a lightweight yet reasoning-driven architecture that enhances factual grounding through two complementary mechanisms: Self-Reflective Retrieval (SRR) and Contrastive Noise Removal (NR). SRR employs small language model as a decision controller that iteratively evaluates evidence sufficiency, enabling adaptive query reformulation without fixed schedules or policy training. NR further refines retrieved content via embedding-based contrastive filtering, enforcing semantic sparsity and removing redundant or tangential passages. Evaluated on WebQuestions, HotpotQA (distractor setting) and InternalQA with 50M Common Crawl distractors, ReflectiveRAG achieves substantial gains over strong baselines-including DeepRAG-improving EM by +2.7 pp and F1 by +2.5 pp, while reducing evidence redundancy by 30.88% with only 18 ms additional latency. Ablation studies confirm that SRR and NR jointly drive both factual accuracy and efficiency, validating our central claim that retrieval reasoning and contrastive filtering can outperform large-scale policy optimization in RAG.","Akshay Verma, Swapnil Gupta, Siddharth Pillai, Prateek Sircar, Deepak Gupta",Akshay Verma,Oral,In-person,SALLE Le Chellah,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",retrieval; policy; contrastive; sufficiency; redundancy; evidence; passages; redundant; fixed; filtering
92-IND,OCR or Not? Rethinking Document Information Extraction in the MLLMs Era with Real-World Large-Scale Datasets,"Multimodal Large Language Models (MLLMs) enhance the potential of natural language processing. However, their actual impact on document information extraction remains unclear. In particular, it is unclear whether an MLLM-only pipeline—while simpler—can truly match the performance of traditional OCR+MLLM setups. In this paper, we conduct a large-scale benchmarking study that evaluates various out-of-the-box MLLMs on business-document information extraction. To examine and explore failure modes, we propose an automated hierarchical error analysis framework that leverages large language models (LLMs) to diagnose error patterns systematically. Our findings suggest that OCR may not be necessary for powerful MLLMs, as image-only input can achieve comparable performance to OCR-enhanced approaches. Moreover, we demonstrate that carefully designed schema, exemplars, and instructions can further enhance MLLMs performance. We hope this work can offer practical guidance and valuable insight for advancing document information extraction.","Jiyuan SHEN, Yuan Peiyue, Atin Ghosh, Yifan Mai, Daniel Dahlmeier",Jiyuan Shen,Oral,In-person,SALLE Le Chellah,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Information Extraction & Structured Prediction,Multimodal & Speech/Audio,document information extraction; document information; mllms; information extraction; ocr; document; extraction; mllm; information; unclear
94-IND,PatentVision: A multimodal method for drafting patent applications,"Patent drafting is complex due to its need for detailed technical descriptions, legal compliance, and visual elements. Although Large Vision-Language Models (LVLMs) show promise across various tasks, their application in automating patent writing remains underexplored. In this paper, we present PatentVision, a multimodal framework that integrates textual and visual inputs—such as patent claims and drawings—to generate complete patent specifications. Built on advanced LVLMs, PatentVision enhances accuracy by combining fine-tuned vision-language models with domain-specific training tailored to patents. Experiments reveal it surpasses text-only methods, producing outputs with greater fidelity and alignment with human-written standards. Its incorporation of visual data allows it to better represent intricate design features and functional connections, leading to richer and more precise results. This study underscores the value of multimodal techniques in patent automation, providing a scalable tool to reduce manual workloads and improve consistency. PatentVision not only advances patent drafting but also lays groundwork for broader use of LVLMs in specialized areas, potentially transforming intellectual property management and innovation processes.","Ruo Yang, Sai Krishna Reddy Mudhiganti, Manali Sharma",Ruo Yang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,Domain NLP (Biomedical/Clinical/Legal/Scientific),drafting; lvlms; visual; multimodal; vision language models; vision language; vision; drawings; enhances accuracy; language models domain
95-IND,VideoMind: Thinking in Steps for Long Video Understanding,"Multimodal Large Language Models (MLLMs) struggle with Long Video Understanding (LVU) due to their limited context window and the distributed nature of salient information across many redundant frames. To address this, we present VideoMind, a novel training free framework for LVU designed to mimic a human reasoning process. The framework is orchestrated by an MLLM that breaks down a user's query into a series of simpler, actionable sub-queries. For each sub query, the MLLM reconfigures itself by invoking specialized `modes' that are instantiations of the same MLLM, but with appropriately tailored context for the given sub query to extract targeted evidence. After gathering this evidence, the model resumes its role as the orchestrator which evaluates the results and decides if an answer is complete or if it must refine its strategy by engaging further modes with new context. Our specialized operational modes include: 1) a Multi-Scale Temporal Search mode to identify and summarize relevant video sub-snippets at varying time scales, and 2) a Single-Frame Visual Detail mode for precise spatial localization of objects. This dynamic allocation of computation yields state-of-the-art results on the Video-MME, LongVideo, and MLVU benchmarks, achieving 77.6% performance on Video MME using Qwen 2.5 72B (4.8% enhancement) while also yielding a 5% improvement on Llama 4 Scout.","Shubhang Bhatnagar, Renxiong Wang, Kapil Krishnakumar, Adel Ahmadyan, Zhaojiang Lin, Lambert Mathias, Xin Luna Dong, Babak Damavandi, Narendra Ahuja, Seungwhan Moon",Shubhang Bhatnagar,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"Reasoning, Planning & Agents",video; sub; mllm; modes; sub query; video understanding; mme; query; mode; context
96-IND,RegNLI: Detecting Online Product Misbranding through Legal and Linguistic Alignment,"Misbranding of health-related products poses significant risks to public safety and regulatory compliance. Existing approaches to claim verification largely rely on keyword matching or generic text classification, failing to capture the nuanced reasoning required to align product claims with legal statutes. In this work, we introduce RegNLI, a novel framework that formulates misbranding detection as a inference task between product claims and regulatory provisions. Leveraging a curated dataset of FDA warning letters, we construct structured representations of claims and statutes. Our model integrates a regulation-aware gating mechanism with a contrastive alignment objective to jointly optimize misbranding classification and statute mapping. Experiments on the \textsc{FDA-Misbrand} dataset demonstrate that RegNLI significantly outperforms strong baselines across accuracy, F1-score, and regulation alignment metrics, while providing interpretable attention patterns that highlight critical linguistic cues. This work establishes a foundation for compliance-aware NLP systems and opens new directions for integrating formal reasoning with neural architectures in regulatory domains.","Diya Saha, Abhishek Bharadwaj Varanasi, Tirthankar Dasgupta, Manjira Sinha",Manjira Sinha,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Efficiency, Scaling & NLP Systems","Trustworthy, Safety, Privacy & Fairness",regulatory; product; claims; statutes; regulation; compliance; legal; alignment; classification; gating mechanism
97-IND,CASPER: Bridging Discrete and Continuous Prompt Optimization through Feedback-Guided Gradient Descent,"Workflow automation is critical for reducing manual efforts in industries, yet existing pipelines fail to handle generative tasks like summarization and extraction without pre-built tools, forcing human intervention. While LLM-based agents offer solutions, their creation depends heavily on prompt engineering—a resource-intensive process often yielding suboptimal results. Current automated approaches face a fundamental trade-off: discrete optimization produces overfitted prompts without convergence guarantees due to non-convex landscapes, while continuous gradient-based methods generate semantically incoherent prompts through embedding optimization. We propose CASPER, a framework bridging discrete and continuous prompt optimization through feedback-guided gradient descent in embedding space. CASPER employs a feedback module producing detailed error analyses that capture failure modes as optimization signals. These insights are projected with prompt tokens into embedding space to steer gradient descent. To preserve interpretability, we incorporate fluency regularization that penalizes incomprehensible tokens. We further accelerate convergence through synthetic data generation that oversamples failure cases, while also addressing data scarcity in industrial settings. We evaluate CASPER on WDC, DROP, GSM8K with F1 improvements of 2.3\%, 1.6\%, 2.3\% and VQA, internal benchmarks showing accuracy improvements of 1.1\%, 3\%, demonstrating cross-domain generalizability.","Aryan Jain, Pushpendu Ghosh, Promod Yenigalla",Aryan Jain,Oral,In-person,SALLE Le Chellah,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Summarization & Generation,"Dialogue, Conversational & Interactive NLP",gradient; descent; optimization; discrete; continuous; prompt; feedback guided; discrete continuous; embedding; embedding space
100-IND,Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement,"Enterprise AI agents must continuously adapt to maintain accuracy, reduce latency, and remain aligned with user needs. We present a practical implementation of a data flywheel in NVInfo AI, NVIDIA's Mixture-of-Experts (MoE) Knowledge Assistant serving over 30,000 employees. By operationalizing a MAPE-driven data flywheel, we built a closed-loop system that systematically addresses failures in retrieval-augmented generation (RAG) pipelines and enables continuous learning. Over a 3-month post-deployment period, we monitored feedback and collected 495 negative samples. Analysis revealed two major failure modes: routing errors (5.25\%) and query rephrasal errors (3.2\%). Using NVIDIA NeMo Microservices, we implemented targeted improvements through fine-tuning. For routing, we replaced a Llama 3.1 70B model with a fine-tuned 8B variant, achieving 96\% accuracy, a 10× reduction in model size, and 70\% latency improvement. For query rephrasal, fine-tuning yielded a 3.7\% gain in accuracy and a 40\% latency reduction. Our approach demonstrates how human-in-the-loop (HITL) feedback, when structured within a data flywheel, transforms enterprise AI agents into self-improving systems. Key learnings include approaches to ensure agent robustness despite limited user feedback, navigating privacy constraints, and executing staged rollouts in production. This work offers a repeatable blueprint for building robust, adaptive enterprise AI agents capable of learning from real-world usage at scale.","Aaditya Shukla, Sidney Knowles, Meenakshi Madugula, David Farris, Ryan Angilly, Santiago Pombo, Lu An, Anbang Xu, Abhinav Balasubramanian, Tan Yu, Jiaxiang Ren, Rama Akkiraju",Aaditya Shukla,Oral,In-person,SALLE Le Chellah,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Efficiency, Scaling & NLP Systems","Dialogue, Conversational & Interactive NLP",ai agents; enterprise; latency; feedback; agents; loop; routing; reduction; adaptive; errors
101-IND,"Medical Summarization in Practice: Design, Deployment, and Analysis of a Clinical Summarization System for a German Hospital","Through the course of hospital treatment, a large number of electronic health records (EHRs) are created for a patient, detailing aspects of care history such as lab results, physician notes, and treatments administered. At the conclusion of treatment, this collection of EHRs must be summarized into a discharge summary, describing the course or care clearly and cohesively. In this paper, we present the design and development of a clinical summarization system integrated into a live German hospital workflow to help with the generation of discharge summaries. We first describe the system, its components, and its context of use within a hospital, before performing a number of experiments to gain insights into how best to use and evaluate our system. We investigate summarization performance across multiple input encoding strategies, compare expert judgments against automatic evaluation of summaries, and analyze the consistency of model summaries across multiple text generations. This work not only acts as a case study to demonstrate the feasibility of LLM integration into healthcare infrastructure, but also provides actionable insights into the use and evaluation of such systems.","Moiz Rauf, Sean Papay",Moiz Rauf,Oral,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),Summarization & Generation,hospital; summarization; summaries; ehrs; treatment; discharge; care; course; german; use
102-IND,Feedback-Aware Prompt Optimization Framework for Generating Job Postings,"Job postings are critical for recruitment, yet large enterprises struggle with standardization and consistency, requiring significant time from hiring managers and recruiters. We present a feedback-aware prompt optimization framework that automates high-quality job posting generation through iterative human-in-the-loop refinement. Our system integrates multiple data sources: job metadata, competencies, organization's compliance guidelines, and organization brand statement, while incorporating human feedback to continuously improve prompt quality through multi-LLM validation. We evaluated our approach using LLM-as-a-judge on 1,056 job postings and human evaluation on a smaller subset across three dimensions: Standardization, Compliance, and User Perception. Our results demonstrate high compliance rates and strong satisfaction scores in both automated and human evaluation, validating the effectiveness of our feedback-aware approach for enterprise job posting generation.","Suraj Maharjan, Ainur Yessenalina, Srinivasan H. Sengamedu",Suraj Maharjan,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"LLM Evaluation, Benchmarks & Metrics","Dialogue, Conversational & Interactive NLP",job; feedback; compliance; job posting; standardization; posting; optimization framework; organization; prompt optimization; prompt
103-IND,Enhancing User Safety: Context-Aware Detection of Offensive Query-Ad Pairs in Multimodal Search Advertising,"The proliferation of multi-modal online advertisements necessitates robust content moderation to ensure user safety, as offensive ad content can cause user distress and erode platform trust. This paper addresses the detection of content that becomes offensive only when a user’s search query is paired with a specific ad, a context-dependent challenge that simple moderation often misses. Key challenges include the nuanced, multi-modal nature of ads, severe data scarcity and class imbalance due to the rarity of offensive content, and the high cost of human labeling. To overcome these limitations, we introduce a novel, context-aware detection framework centered on a large-scale, Multi-modal Teacher-Student Knowledge Distillation architecture. A powerful Gemini encoder-only “teacher” model distills its knowledge into a lightweight student model suitable for low-latency deployment. We enhance robustness using a novel graph mining technique to find rare offensive examples for training. For evaluation, we developed a highly accurate Automated Evaluation Model (AEM)—a separate, larger Gemini model utilizing Chain-of-Thought (CoT) reasoning—to rigorously assess performance in a live A/B test. Our results demonstrate that the proposed framework reduces the serving of offensive query-ad pairs by more than 80\% compared to the baseline, while maintaining the efficiency required for real-time advertising systems that operate at a scale of over $\approx100$ billion query-ad pairs per day.","Gaurav Kumar, Qiangjian Xi, Tanmaya Shekhar Dabral, Hooshang Ghasemi, Abishek Krishnamoorthy, Danqing Fu, Rui Min, Emilio Antunez, Zhongli Ding, Pradyumna Narayana",Gaurav Kumar,Oral,In-person,SALLE Le Chellah,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems","Reasoning, Planning & Agents",offensive; multi modal; query; modal; user; user safety; advertising; content; pairs; teacher
104-IND,SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models,"Large language models (LLMs) have achieved remarkable progress, yet their internal mechanisms remain largely opaque, posing a significant challenge to their safe and reliable deployment. Sparse autoencoders (SAEs) have emerged as a promising tool for decomposing LLM representations into more interpretable features, but explaining the features captured by SAEs remains a challenging task. In this work, we propose SAGE (SAE Agentic Explainer), an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanation-driven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.","Jiaojiao Han, Wujiang Xu, Mingyu Jin, Mengnan Du",Jiaojiao Han,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",Summarization & Generation,sage; saes; features; explanations; sae; agentic; feature; predictive accuracy; task work; saes emerged
105-IND,Adapting Vision-Language Models for E-commerce Understanding at Scale,"E-commerce product understanding demands by nature, strong multimodal comprehension from text, images, and structured attributes. General-purpose Vision–Language Models (VLMs) enable generalizable multimodal latent modelling, yet there is no documented, well-known strategy for adapting them to the attribute-centric, multi-image, and noisy nature of e-commerce data, without sacrificing general performance. In this work, we show through a large-scale experimental study, how targeted adaptation of general VLMs can substantially improve e-commerce performance while preserving broad multimodal capabilities. Furthermore, we propose a novel extensive evaluation suite covering deep product understanding, strict instruction following, and dynamic attribute extraction.","Matteo Nulli, Orshulevich Vladimir, Tala Bazazo, Christian Herold, Michael Kozielski, Marcin Mazur, Szymon Tuzel, Cees G. M. Snoek, Seyyed Hadi Hashemi, Omar Javed, Yannick Versley, Shahram Khadivi",Matteo Nulli,Oral,In-person,SALLE Le Chellah,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,"Dialogue, Conversational & Interactive NLP",commerce; product; general; adapting; multimodal; attribute; understanding; vlms; nature; vision language models
106-IND,"MedRiskEval: Medical Risk Evaluation Benchmark of Language Models, On the Importance of User Perspectives in Healthcare Settings","As the performance of large language models (LLMs) continues to advance, their adoption in the medical domain is increasing. However, most existing risk evaluations largely focused on general safety benchmarks. In the medical applications, LLMs may be used by a wide range of users, ranging from general users and patients to clinicians, with diverse levels of expertise and the model's outputs can have a direct impact on human health which raises serious safety concerns. In this paper, we introduce MedRiskEval, a medical risk evaluation benchmark tailored to the medical domain. To fill the gap in previous benchmarks that only focused on the clinician perspective, we introduce a new patient-oriented dataset called PatientSafetyBench containing 466 samples across 5 critical risk categories. Leveraging our new benchmark alongside existing datasets, we evaluate a variety of open- and closed-source LLMs. To the best of our knowledge, this work establishes an initial foundation for safer deployment of LLMs in healthcare.","Jean-Philippe Corbeil, Minseon Kim, Maxime Griot, Sheela Agarwal, Alessandro Sordoni, Francois Beaulieu, Paul Vozila",Jean-Philippe Corbeil,Oral,In-person,SALLE Le Chellah,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"LLM Evaluation, Benchmarks & Metrics",medical; risk; evaluation benchmark; medical domain; healthcare; users; focused; benchmark; general; safety
111-IND,Synthetic Doctor-Patient Dialogue Generation for Robust Medical ASR: A Scalable Pipeline for Vocabulary Expansion and Privacy Preservation,"Automatic Speech Recognition (ASR) is increasingly integral to healthcare services, where medical conversations present unique transcription challenges due to specialized terminology and frequent introduction of new terms. Existing ASR models, including widely used systems like Whisper, struggle with high word error rates (WER) on clinical vocabulary, especially medication names, primarily due to the scarcity of annotated audio-transcript data in the medical domain. This paper proposes and evaluates a novel synthetic data generation pipeline that produces comprehensive doctor-patient dialogues in both text and audio forms, specifically targeting a curated set of over 124,000 medical terms. The pipeline generated over 1 billion audios with ground truth transcriptions. Fine-tuning ASR models with this synthetic corpus significantly reduced overall WER and improved transcription accuracy on medical terms, marking a significant advance in healthcare ASR accuracy. Data generation code, dataset, and training and evaluation scripts are released.","Kefei Liu, Meizhu Liu",Meizhu Liu,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),Multimodal & Speech/Audio,asr; medical; doctor; terms; asr models; wer; transcription; synthetic; pipeline; data generation
112-IND,Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization,"Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across many domains. However, automatically generating high-quality summaries is challenging, as the ideal summary must satisfy a set of complex, multi-faceted requirements. While summarization has received immense attention in research, prior work has primarily utilized static datasets and benchmarks, a condition rare in practical scenarios where requirements inevitably evolve. In this work, we present an industry case study on developing an agentic system to summarize multi-party interactions. We share practical insights spanning the full development lifecycle to guide practitioners in building reliable, adaptable summarization systems, as well as to inform future research, covering: 1) robust methods for evaluation despite evolving requirements and task subjectivity, 2) component-wise optimization enabled by the task decomposition inherent in an agentic architecture, 3) the impact of upstream data bottlenecks, and 4) the realities of vendor lock-in due to the poor transferability of LLM prompts.","Kushal Chawla, Chenyang Zhu, Pengshan Cai, Sangwoo Cho, Scott Novotney, Ayushman Singh, Jonah Lewis, Keasha Safewright, Alfy Samuel, Erin Babinsky, Shi-Xiong Zhang, Sambit Sahu",Kushal Chawla,Oral,In-person,SALLE Le Chellah,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Summarization & Generation,"Dialogue, Conversational & Interactive NLP",summarization; requirements; party; multi party; adaptable; industry; agentic; multi; practical; methods evaluation
113-IND,LingVarBench: Benchmarking LLMs on Entity Recognitions and Linguistic Verbalization Patterns in Phone-Call Transcripts,"We study structured entity extraction from phone-call transcripts in customer-support and healthcare settings, where annotation is costly, and data access is limited by privacy and consent. Existing methods degrade under disfluencies, interruptions, and speaker overlap, yet large real-call corpora are rarely shareable. We introduce LingVarBench, a benchmark and semantic synthetic data generation pipeline that generates linguistically varied training data via (1) LLM-sampled entity values, (2) curated linguistic verbalization patterns covering diverse disfluencies and entity-specific readout styles, and (3) a value–transcript consistency filter. Using this dataset, DSPy's SIMBA automatically synthesizes and optimizes extraction prompts, reducing manual prompt engineering and targeting robustness to verbal variation. On real customer transcripts, prompts optimized solely on LingVarBench outperform zero-shot baselines and match or closely approach human-tuned prompts for structured entities such as ZIP code, date of birth, and name (F1 approximately 94-95 percent). For subjective questionnaire items, optimized prompts substantially improve over zero-shot performance and approach human-tuned prompts. LingVarBench offers a practical and cost-efficient path to deployment in a direct-answer setting, with real annotations later enabling additional refinement.","Seyedali Mohammadi, Manas Paldhe, Amit Chhabra, Youngseo Son, Vishal Seshagiri",Amit Chhabra,Oral,In-person,SALLE Le Chellah,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",prompts; transcripts; entity; verbalization; phone; customer; optimized; real; extraction; zero shot
114-IND,Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging,"Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.","Alphaeus Dmonte, Vidhi Gupta, Daniel J Perry, Mark Arehart",Alphaeus Dmonte,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Efficiency, Scaling & NLP Systems",Multilinguality & Low-Resource NLP,merging; maintenance; efficiency; multilingual; training; involves; updating; costs; terms; reduces
115-IND,The Subtle Art of Defection: Understanding Uncooperative Behaviors in LLM based Multi-Agent Systems,"This paper introduces a novel framework for simulating and analyzing how uncooperative behaviors can destabilize or collapse LLM-based multi-agent systems. Our framework includes two key components: (1) a game theory-based taxonomy of uncooperative agent behaviors, addressing a notable gap in the existing literature; and (2) a structured, multi-stage simulation pipeline that dynamically generates and refines uncooperative behaviors as agents' states evolve. We evaluate the framework via a collaborative resource management setting, measuring system stability using metrics such as survival time and resource overuse rate. Empirically, our framework achieves ~96.7% accuracy in generating realistic uncooperative behaviors, validated by human evaluations. Our results reveal a striking contrast: cooperative agents maintain perfect system stability (100% survival over 12 rounds with 0% resource overuse), while any uncooperative behavior can trigger rapid system collapse within 1–7 rounds. We also evaluate LLM-based defense methods, finding they detect some uncooperative behaviors, but some behaviors remain largely undetectable. These gaps highlight how uncooperative agents degrade collective outcomes and underscore the need for more resilient multi-agent systems.","Devang Kulshreshtha, Wanyu Du, Raghav Jain, Srikanth Doss, Hang Su, Sandesh Swamy, Yanjun Qi",Devang Kulshreshtha,Oral,In-person,SALLE Le Chellah,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",behaviors; multi agent systems; agent systems; agent; llm based multi; based multi agent; multi agent; based multi; rounds; stability
116-IND,Tailoring Rumor Debunking to You: Diversifying Chinese Rumor-Debunking Passages with an LLM-Driven Simulated Feedback-Enhanced Framework,"Social media platforms have become primary sources for news consumption due to their real-time and interactive nature, yet they have also facilitated the widespread proliferation of misinformation, negatively impacting public health, social cohesion, and market stability. While professional fact-checking is essential for debunking rumors, the process is time-consuming, necessitating automation to effectively combat fake news. Existing approaches, such as extractive methods, often lack coherence and context, whereas abstractive methods leveraging large language models (LLMs) can generate more readable and informative debunking passages. However, readability alone is insufficient for effective misinformation correction; user acceptance is critical. Recent advancements in LLMs offer new opportunities for personalized debunking, as these models can generate context-sensitive responses and adapt to user profiles. Building on this, we propose the MUti-round Refinement and Simulated fEedback-enhanced framework (MURSE), which generates Chinese user-specific debunking passages by iteratively refining outputs based on simulated user feedback. Specifically, MURSE-generated user-specific debunking passages were preferred twice as often as general debunking passages in most cases, highlighting its potential to improve misinformation correction and foster positive dissemination chains.","Xinle Pang, Danding Wang, Qiang Sheng, Yifan Sun, Beizhe Hu, Juan Cao",Xinle Pang,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Dialogue, Conversational & Interactive NLP",,passages; user; misinformation; simulated; user specific; enhanced framework; feedback; chinese; news; correction
120-IND,Synthetic Data Fine-Tuning for Effective Team Formation in Enterprises,"We evaluate the effectiveness of synthetic data fine-tuning for Semantic Search in a real-world Enterprise Team Formation problem scenario. In this problem, we aim to retrieve the best employee for a given task, given their information regarding abilities, experiences, and other aspects. We evaluate two synthetic data generation strategies: (1) augmenting real-world data with synthetic labels and (2) generating synthetic profiles for employees tailored to specific tasks. To measure the impact of these strategies, we fine-tune a pretrained text embedding model using LoRA and Rank Aggregation techniques. We evaluate the model performance against current SOTA algorithms on a human-curated dataset. Our experiments indicate that training a model that uses a combination of both Synthetic data generation strategies outperforms already established pre-trained models on the Team Formation task, improving the ranking metrics by an average of 30\% in comparison to the best-performing pre-trained model.","Guilherme Drummond Lima, Adriano Veloso",Guilherme Drummond Lima,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Summarization & Generation,"Reasoning, Planning & Agents",synthetic; formation; team; synthetic data; data fine tuning; data fine; synthetic data generation; data generation; strategies; pre trained
121-IND,Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents,"Multi-turn tool-calling LLMs — models capable of invoking external APIs or tools across several user turns — have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines *remains difficult for many safety-critical industries* due to ongoing concerns regarding model resilience. While standardized benchmarks, such as the Berkeley Function-Calling Leaderboard (BFCL), have underpinned confidence concerning advanced function-calling models (like Salesforce’s xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce **Assertion-Conditioned Compliance (A-CC)**, a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model’s behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.","Daud Waqas, Aaryamaan Golthi, Erika Hayashida, Huanzhi Mao",Daud Waqas,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Dialogue, Conversational & Interactive NLP",calling; multi turn; turn; function calling; function; compliance; assertion; multi; sycophancy; sourced
122-IND,PROBES : Performance and Relevance Observation for BEtter Search,"High-quality search is essential for the success of online platforms, spanning e-commerce, social media, shopping-focused applications, and broader search systems such as content discovery and enterprise web search. To ensure optimal user experience and drive business growth, continuous evaluation and improvement of search systems is crucial. This paper introduces PROBES, a novel multi-task system powered by Large Language Models (LLMs) designed for end-to-end evaluation of semantic search systems. PROBES identifies context-aware relevance using a fine-grained scale (exact, substitute, complement, irrelevant) by leveraging the query category, feature-level intent, and category-aware feature importance, enabling more precise and consistent judgments than relying solely on raw query text. This allows PROBES to provide differentiated relevance assessment across a diverse range of query categories. PROBES then dives deeper to understand the reason behind irrelevant results (Precision issues) by checking product content conflicts and inaccuracies. It also analyzes Missed Recall by leveraging retrieval and relevance models to determine whether a missed recall was due to a selection issue or a ranking/retrieval system issue. To evaluate PROBES, we introduce a new metric, the Actionable Error Rate (AER), defined as the proportion of actionable errors over all flagged errors. We observe that PROBES operates at an AER of 76%, generating actionable insights across 100 product categories.","Sejal Jain, Cyrus Andre DSouza, Jitenkumar Babubhai Rana, Aniket Joshi, Promod Yenigalla",Sejal Jain,Oral,In-person,SALLE Le Chellah,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics","Retrieval, Grounding & External Knowledge (RAG)",probes; search; search systems; relevance; actionable; query; product; category; irrelevant; recall
126-IND,Aligning Paralinguistic Understanding and Generation in Speech LLMs via Multi-Task Reinforcement Learning,"Speech large language models (LLMs) observe paralinguistic cues such as prosody, emotion, and non-verbal sounds—crucial for intent understanding. However, leveraging these cues faces challenges: limited training data, annotation difficulty, and models exploiting lexical shortcuts over paralinguistic signals. We propose multi-task reinforcement learning (RL) with chain-of-thought prompting that elicits explicit affective reasoning. To address data scarcity, we introduce a paralinguistics-aware speech LLM (PALLM) that jointly optimizes sentiment classification from audio and paralinguistics-aware response generation via a two-stage pipeline. Experiments demonstrate that our approach improves paralinguistics understanding over both supervised baselines and strong proprietary models (Gemini-2.5-Pro, GPT-4o-audio), by 8-12% on Expresso, IEMOCAP, and RAVDESS. The results show that modeling paralinguistic reasoning with multi-task RL is crucial for building emotionally intelligent speech LLMs.","Minseok Kim, Jingxiang Chen, Seong-Gyun Leem, Yin Huang, Rashi Rungta, Zhicheng Ouyang, Haibin Wu, Surya Teja Appini, Ankur Bansal, Yang Bai, Yue Liu, Florian Metze, Ahmed A Aly, Anuj Kumar, Ariya Rastrow, Zhaojiang Lin",Minseok Kim,Oral,In-person,SALLE Le Chellah,Session 10: Oral/Poster G,Fri. Mar 27,09:00-10:30,Multimodal & Speech/Audio,Summarization & Generation,paralinguistic; paralinguistics; multi task; speech; speech llms; understanding; audio; reinforcement learning; cues; reinforcement
128-IND,IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages,"Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce Indic Jailbreak Robustness (IJR) a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (~2.09B speakers), covering 45,216 prompts in JSON (contract-bound) and Free (naturalistic) tracks. IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed $0.92$ JSR, and in Free all models reach ~1.0 with refusals collapsing. (2) English→Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized/mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization ρ ≈ 0.28–0.32 indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.","Priyaranjan Pattnayak, Sanchari Chowdhuri",Sanchari Chowdhuri,Oral,In-person,SALLE Le Chellah,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",asian; contract; south; json; free; refusals; bound; jailbreak; english; judge
134-IND,Synthesizing question answering data from financial documents: An End-to-End Multi-Agent Approach,"Answering complex questions that require numerical reasoning over financial documents is challenging due to the diverse and scattered nature of relevant information. While large language models (LLMs) excel at financial reasoning, their enterprise deployment is often limited by cost and latency. Small language models (SLMs) present a cost-effective alternative but need to be fine-tuned with high-quality, domain-specific question-answer (QA) data. Acquiring such data requires manual expert annotation, presenting a bottleneck to the wider application of SLMs. This work introduces a modular, scalable end-to-end agentic pipeline that extracts and selects relevant content from unstructured financial documents and then generates QA pairs from the selected content for SLM fine-tuning. Compared to the same models trained on previous manually generated data for the task, one of the models trained on our pipeline-produced synthetic data achieved competitive in-distribution performance, and all tested models demonstrated superior generalization. The framework thus demonstrates considerable potential to accelerate the deployment of smaller, cost-effective models by reducing manual data creation efforts.","Chetan Harsha, Karmvir Singh Phogat, Sridhar Dasaratha, Shashishekar Ramakrishna",Dr. Sridhar Dasaratha,Oral,In-person,SALLE Le Chellah,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",financial; end; documents; slms; cost; cost effective; manual; models trained; end end; deployment
136-IND,Toward Automatic Delegation Extraction in Japanese Law,"The legal systems have a hierarchical structure, and a higher-level law often authorizes a lower-level law to implement detailed provisions, which is called delegation. When interpreting legal texts with delegation, readers must repeatedly consult the lower-level laws that stipulate the detailed provisions, imposing a substantial workload. Therefore, it is necessary to develop a system that enables readers to instantly refer to relevant laws in delegation. However, manually annotating delegation is difficult because it requires extensive legal expertise, careful reading of numerous legal texts, and continuous adaptation to newly enacted laws. In this study, we focus on Japanese law and develop a two-stage pipeline system for automatic delegation annotation. First, we extract keywords that indicate delegation using a named entity recognition approach. Second, we identify the delegated provision corresponding to each keyword as an entity disambiguation task. In our experiments, the proposed system demonstrates sufficient performance to assist manual annotation in practice.","Tsuyoshi Fujita, Yuya Sawada, Yusuke Sakai, Taro Watanabe",Tsuyoshi Fujita,Oral,In-person,SALLE Le Chellah,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Information Extraction & Structured Prediction,Domain NLP (Biomedical/Clinical/Legal/Scientific),laws; law; legal; lower level; provisions; readers; japanese; detailed; develop; texts
139-IND,DIALECTIC: A Multi-Agent System for Startup Evaluation,"Venture capital (VC) investors face a large number of investment opportunities but only invest in few of these, with even fewer ending up successful. Early-stage screening of opportunities is often limited by investor bandwidth, demanding tradeoffs between evaluation diligence and number of opportunities assessed. To ease this tradeoff, we introduce DIALECTIC, an LLM-based multi-agent system for startup evaluation. DIALECTIC first gathers factual knowledge about a startup and organizes these facts into a hierarchical question tree. It then synthesizes the facts into natural-language arguments for and against an investment and iteratively critiques and refines these arguments through a simulated debate, which surfaces only the most convincing arguments. Our system also produces numeric decision scores that allow investors to rank and thus efficiently prioritize opportunities. We evaluate DIALECTIC through backtesting on real investment opportunities aggregated from five VC funds, showing that DIALECTIC matches the precision of human VCs in predicting startup success.","Jae Yoon Bae, Simon Malberg, Joyce Ann Clarize Galang, Andre Retterath, Georg Groh",Jae Yoon Bae,Oral,In-person,SALLE Le Chellah,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics","Reasoning, Planning & Agents",dialectic; opportunities; investment; arguments; facts; number; multi agent; agent; evaluation; dialectic multi agent
141-IND,Long-Context Long-Form Question Answering for Legal Domain,"Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. By leveraging the expertise of professionals from fields such as law and corporate tax, we curate a QA dataset. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.","Anagha Kulkarni, Parin Rajesh Jhaveri, Prasha Shrestha, Yu Tong Han, Reza Amini, Behrouz Madahian",Parin Rajesh Jhaveri,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,"Linguistics, Syntax & Semantics",Domain NLP (Biomedical/Clinical/Legal/Scientific),long; domain specific vocabulary; specific vocabulary; legal; legal documents; documents; long form; question; vocabulary; long context
144-IND,ELO: Efficient Layer-Specific Optimization for Continual Pretraining of Multilingual LLMs,"We propose an efficient layer-specific optimization (ELO) method designed to enhance continual pretraining (CP) for specific languages in multilingual large language models (MLLMs). This approach addresses the common challenges of high computational cost and degradation of source language performance associated with traditional CP. The ELO method consists of two main stages: (1) ELO Pretraining, where a small subset of specific layers, identified in our experiments as the critically important first and last layers, are detached from the original MLLM and trained with the target language. This significantly reduces not only the number of trainable parameters but also the total parameters computed during the forward pass, minimizing GPU memory consumption and accelerating the training process. (2) Layer Alignment, where the newly trained layers are reintegrated into the original model, followed by a brief full fine-tuning step on a small dataset to align the parameters. Experimental results demonstrate that the ELO method achieves a training speedup of up to 6.46 times compared to existing methods, while improving target language performance by up to 6.2\% on qualitative benchmarks and effectively preserving source language (English) capabilities.","HanGyeol Yoo, ChangSu Choi, Minjun Kim, Seohyun Song, SeungWoo Song, Inho Won, Jongyoul Park, Cheoneum Park, KyungTae Lim",Hangyeol Yoo,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Multilinguality & Low-Resource NLP,"Efficiency, Scaling & NLP Systems",pretraining; efficient layer; continual pretraining; language performance; layer; layers; parameters; target language; specific; continual
145-IND,MIRAGE: Metadata-guided Image Retrieval and Answer Generation for E-commerce Troubleshooting,"Existing multimodal systems typically associate text and available images based on embedding similarity or simple co-location, but such approaches often fail to ensure that the linked image accurately depicts the specific product or component mentioned in a troubleshooting instruction. We introduce MIRAGE, a metadata-first paradigm that treats structured metadata, (not raw pixels), as a first-class modality for multimodal grounding. In MIRAGE, both text and images are projected through a shared semantic schema capturing product attributes, context, and visual aspects, enabling reasoning over interpretable attributes for troubleshooting rather than unstructured embeddings. MIRAGE comprises of three complementary modules: M-Link for schema-guided image–text linking, M-Gen for metadata-conditioned multimodal generation, and M-Eval for consistency evaluation in the same structured space. Experiments on large-scale enterprise e-commerce troubleshooting data across 10 product types on 100K text chunks and 35K images show that metadata-centric grounding achieves over 40% higher linking coverage of high-quality visual content and over 45% in linking and response quality than embedding-based baselines. MIRAGE demonstrates the potential of structured metadata in enabling scalable, fine-grained grounding in multimodal troubleshooting systems.","Rishav Sahay, Lavanya Sita Tekumalla, Anoop Saladi",Lavanya Sita Tekumalla,Oral,In-person,SALLE Le Chellah,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,"Retrieval, Grounding & External Knowledge (RAG)",metadata; mirage; product; linking; multimodal; guided image; images; grounding; image; commerce
150-IND,CODMAS: A Dialectic Multi-Agent Collaborative Framework for Structured RTL Optimization,"Optimizing Register Transfer Level (RTL) code is a critical step in Electronic Design Automation (EDA) for improving power, performance, and area (PPA). We present CODMAS (Collaborative Optimization via a Dialectic Multi-Agent System), a framework that combines structured dialectic reasoning with domain-aware code generation and deterministic evaluation to automate RTL optimization. At the core of CODMAS are two dialectic agents: the Articulator, inspired by rubber-duck debugging, which articulates stepwise transformation plans and exposes latent assumptions; and the Hypothesis Partner, which predicts outcomes and reconciles deviations between expected and actual behavior to guide targeted refinements. These agents direct a Domain-Specific Coding Agent (DCA) to generate architecture-aware Verilog edits and a Code Evaluation Agent (CEA) to verify syntax, functionality, and PPA metrics. We introduce RTLOPT, a benchmark of 120 Verilog triples (unoptimized, optimized, testbench) for pipelining and clock-gating transformations. Across proprietary and open LLMs, CODMAS achieves ~25% reduction in critical path delay for pipelining and ~22% power reduction for clock gating, while reducing functional and compilation failures compared to strong prompting and agentic baselines. These results demonstrate that structured multi-agent reasoning can significantly enhance automated RTL optimization and scale to more complex designs and broader optimization tasks.","Che-Ming Chang, Prashanth Vijayaraghavan, Ashutosh Jadhav, Charles Mackin, Hsinyu Tsai, Vandana Mukherjee, Ehsan Degan",Prashanth Vijayaraghavan,Oral,In-person,SALLE Le Chellah,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",dialectic; rtl; optimization; agent; clock; dialectic multi; dialectic multi agent; ppa; verilog; multi agent
155-IND,D3: Dynamic Docid Decoding for Multi-Intent Generative Retrieval,"Generative Retrieval (GR) maps queries to documents by generating discrete identifiers (DocIDs). However, offline DocID assignment and constrained decoding often prevent GR from capturing query-specific intent, especially when documents express multiple or unseen intents (i.e., intent misalignment). We introduce Dynamic Docid Decoding (D3), an inference-time mechanism that adaptively refines DocIDs through delayed, query-informed identifier expansion. D3 uses (a) verification to detect intent misalignment and (b) dynamic decoding to extend DocIDs with query-aligned tokens, even those absent from the pre-indexed vocabulary, enabling plug-and-play DocID expansion beyond the static vocabulary while adding minimal overhead. Experiments on NQ320k and MS-MARCO show that D3 consistently improves retrieval accuracy, especially on unseen and multi-intent documents, across various GR models, including a +2.4\%p nDCG@10 gain on the state-of-the-art model.","Jaeyoung Kim, Dohyeon Lee, Soona Hong, seung-won hwang",Seung-won Hwang,Oral,In-person,SALLE Le Chellah,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",intent; decoding; multi intent; generative retrieval; documents; dynamic; query; misalignment; expansion; vocabulary
160-IND,DisGraph-RP: Graph-Augmented Temporal Modeling with Aspect-Based Contrastive Encoding of Discharge Summary for Readmission Prediction,"Predicting hospital readmissions is a critical clinical task with substantial implications for patient outcomes and healthcare cost management. We propose DisGraph-RP, a graph-augmented temporal modeling framework that integrates structured discourse-aware text representation with cross-admission relational reasoning. Our approach introduces a Section-Aware Contrastive Encoder that leverages section segmentation and aspect-based supervision to produce fine-grained representations of discharge summaries. These representations are then composed over time using a Graph-Based temporal module that encodes inter-visit dependencies through learned edge relations, enabling the model to capture disease progression, treatment history, and recurrent risk signals. Experiments on multiple real-world datasets demonstrate that DisGraph-RP achieves significant improvements over strong baselines, including transformer-based clinical models and prompting-based LLM approaches. Our findings highlight the importance of combining discourse-informed text encoding with temporal graph reasoning for robust clinical outcome prediction.","Sudeshna Jana, Tirthankar Dasgupta, Manjira Sinha, Pabitra Mitra",Tirthankar Dasgupta,Poster,In-person,POSTER HALL,Session 8: Oral/Posters F,Thur. Mar 26,14:30-16:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),,temporal; graph; clinical; graph augmented; discharge; encoding; aspect; based; discourse; contrastive
161-IND,CareerPathKG: Knowledge Graph Integrated Framework for Career Intelligence,"The labor market is experiencing rapid and continual shifts in required skills and competencies, driven by technological advancement and evolving industry structures. Within this dynamic environment, candidates increasingly face challenges in orienting their career development, requiring them to continuously update their knowledge and capabilities to meet contemporary job requirements; this need is particularly necessary for new entrants to the labor market, who must cultivate a comprehensive understanding of current labor-market conditions. To address these issues, this study proposes an enterprise recruitment framework grounded in a career path knowledge graph, capturing occupations, skill requirements, and career transitions using standardized taxonomies enriched with job-posting data. The framework integrates transformer-based embeddings, large language models, and knowledge-graph reasoning to support efficient and reliable CV assessment, CV-JD matching and career guidance.","Ngoc-Quang Le, Duc Duong Hoang, Mai Vu Tran, Thi-Hai-Yen Vuong",Ngoc-Quang Le,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents",,labor market; market; labor; knowledge graph; job; graph; knowledge; requirements; models knowledge; technological
162-IND,A Hybrid Supervised-LLM Pipeline for Actionable Suggestion Mining in Unstructured Customer Reviews,"Extracting actionable suggestions from customer reviews is essential for operational decision-making, yet these directives are often embedded within mixed-intent, unstructured text. Existing approaches either classify suggestion-bearing sentences or generate high-level summaries, but rarely isolate the precise improvement instructions businesses need. We evaluate a hybrid pipeline combining a high-recall RoBERTa classifier trained with a precision–recall surrogate to reduce unrecoverable false negatives with a controlled, instruction-tuned LLM for suggestion extraction, categorization, clustering, and summarization. Across real-world hospitality and food datasets, the hybrid system outperforms prompt-only, rule-based, and classifier-only baselines in extraction accuracy and cluster coherence. Human evaluations further confirm that the resulting suggestions and summaries are clear, faithful, and interpretable. Overall, our results show that hybrid reasoning architectures achieve meaningful improvements fine-grained actionable suggestion mining while highlighting challenges in domain adaptation and efficient local deployment.","Aakash Trivedi, Aniket Upadhyay, Pratik Narang, Dhruv Kumar, Praveen Kumar",Aakash Trivedi,Oral,In-person,SALLE Le Chellah,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Summarization & Generation,"Reasoning, Planning & Agents",suggestion; hybrid; actionable; suggestions; customer; reviews; mining; classifier; summaries; unstructured
166-IND,ShopperBench: A Benchmark for Personalized Shopping with Persona-Guided Simulation,"Personalized shopping agents must adapt their decisions to different user personas, balancing efficiency, preference alignment, and goal success. Building upon the WebShop dataset and $\tau^2$-Bench environment, ShopperBench introduces a persona-guided benchmark for evaluating such adaptive behaviors. ShopperBench augments shopping trajectories with persona-conditioned goals, reasoning rationales, and preference cues, capturing how diverse shopper types---from price-conscious planners to trend-seeking explorers---navigate product search and selection. We further design a baseline of ShopperAgents that operate under persona guidance to simulate realistic, goal-oriented shopping interactions. To evaluate these agents, we propose new metrics including Persona Fidelity, Persona-Query Alignment, and Path Consistency. Together, Our ShopperBench provides a testbed for studying personalized and context-aware shopping intelligence, bridging the gap between human-centered e-commerce behavior and agent-based simulation.","Yuan Ling, Chunqing Yuan, Shujing Dong, Yongjian Yang, Nataraj Mocherla, Ayush Goyal",Yuan Ling,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"Reasoning, Planning & Agents","LLM Evaluation, Benchmarks & Metrics",shopping; persona; personalized; goal; simulation; preference; guided; agents; balancing efficiency; tau
167-IND,ARQA: A Benchmark for Grounded Table–Text QA in Enterprise Annual Reports,"Annual reports communicate corporate performance to stakeholders through dense tables and explanatory text, with rich grounding signals making automated reasoning challenging. Existing QA benchmarks focus on retrieval or single-modality reasoning and rarely require justification for answers with both textual and tabular evidence. We introduce ARQA (Annual Report QA), a benchmark of ~2.5K QA pairs spanning ten fiscal years of automotive enterprise annual reports and three reasoning families — Lookup, Arithmetic, and Insight. Data are produced via a planner–generator pipeline, deterministically verified and recomputed, and fully reviewed by domain experts. We evaluate state-of-the-art instruction-tuned language models on ARQA, showing strong factual retrieval but persistent weaknesses in grounded arithmetic and causal reasoning. We release ARQA and its evaluation toolkit to facilitate research on auditable, evidence-first reasoning over enterprise documents. (https://github.com/RuilongWang/ARQA-Benchmark/)","Ruilong Wang, Simone Balloccu",Ruilong Wang,Oral,In-person,SALLE Le Chellah,Session 3: Oral/Posters B,Wed. Mar 25,14:30-16:00,"Retrieval, Grounding & External Knowledge (RAG)","LLM Evaluation, Benchmarks & Metrics",reports; enterprise; reasoning; arithmetic; evidence; benchmark; grounded; evaluate state; evaluate state art; textual tabular
168-IND,Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning?,"Clinical Question-Answering (CQA) industry systems are increasingly rely on Large Language Models (LLMs), yet their deployment is often guided by the assumption that domain-specific fine-tuning is essential. Although specialised medical LLMs such as BioBERT, BioGPT, and PubMedBERT remain popular, they face practical limitations including narrow coverage, high retraining costs, and limited adaptability. Efforts based on Supervised Fine-Tuning (SFT) have attempted to address these assumptions but continue to reinforce what we term the SPECIALISATION FALLACY—the belief that specialised medical LLMs are inherently superior for CQA. To address this assumption, we introduce MEDASSESS-X, a deployment-industry-oriented CQA framework that applies alignment at inference time rather than through SFT. MEDASSESS-X uses lightweight steering vectors to guide model activations toward medically consistent reasoning without updating model weights or requiring domain-specific retraining. This inference-time alignment layer stabilises CQA performance across both general-purpose and specialised medical LLMs, thereby resolving the SPECIALISATION FALLACY. Empirically, MEDASSESS-X delivers consistent gains across all LLM families, improving Accuracy by up to +6%, Factual Consistency by +7%, and reducing Safety Error Rate by as much as 50%.","Sushant Kumar Ray, Gautam Siddharth Kashyap, Sahil Tripathi, Nipun Joshi, Vijay Govindarajan, Rafiq Ali, Jiechao Gao, Usman Naseem",Gautam Siddharth Kashyap,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Efficiency, Scaling & NLP Systems",cqa; specialised; medical llms; medical; specialisation; clinical question; fallacy; clinical question answering; industry; assumption
170-IND,SkiLLens: Recognising and Mapping Novel Skills from Millions of Job Ads Across Europe Using Language Models,"In a rapidly evolving labor market, detecting and addressing emerging skill needs is essential for shaping responsive education and workforce policies. Online job advertisements (OJAs) provide a real-time view of changing demands, but require first retrieving skill mentions from unstructured text and then solving the entity linking problem of connecting them to standardized skill taxonomies. To harness this potential, we present a multilingual human-in-the-loop (HITL) pipeline that operates in two steps: candidate skills are extracted from national OJA corpora using country-specific word embeddings, capturing terms that reflect each country’s labor market. These candidates are linked to ESCO using an encoder-based system and refined through a decoder large language models (LLMs) for accurate contextual alignment. Our approach is validated through both quantitative and qualitative evaluations, demonstrating that our method enables timely, multilingual monitoring of emerging skills, supporting agile policy-making and targeted training initiatives.","Alessia De Santo, Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, Navid Nobani",Lorenzo Malandri,Oral,In-person,SALLE Le Chellah,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Information Extraction & Structured Prediction,Multilinguality & Low-Resource NLP,skill; skills; labor market; country; job; market; emerging; labor; models rapidly; language models rapidly
173-IND,SYMDIREC: A Neuro-Symbolic Divide-Retrieve-Conquer Framework for Enhanced RTL Synthesis and Summarization,"Register-Transfer Level (RTL) synthesis and summarization are central to hardware design automation but remain challenging for Large Language Models (LLMs) due to rigid HDL syntax, limited supervision, and weak alignment with natural language. Existing prompting and retrieval-augmented generation (RAG) methods have not incorporated symbolic planning, limiting their structural precision. We introduce SYMDIREC, a neuro-symbolic framework that decomposes RTL tasks into symbolic subgoals, retrieves relevant code via a fine-tuned retriever, and assembles verified outputs through LLM reasoning. Supporting both Verilog and VHDL without LLM fine-tuning, SYMDIREC achieves ~20% higher Pass@1 rates for synthesis and 15–20% ROUGE-L improvements for summarization over prompting and RAG baselines, demonstrating the benefits of symbolic guidance in RTL tasks.","Prashanth Vijayaraghavan, Apoorva Nitsure, Luyao Shi, Charles Mackin, Ashutosh Jadhav, David Beymer, Ehsan Degan, Vandana Mukherjee",Prashanth Vijayaraghavan,Oral,In-person,SALLE Le Chellah,Session 11: Oral/Poster H,Fri. Mar 27,11:00-12:30,Summarization & Generation,"Retrieval, Grounding & External Knowledge (RAG)",rtl; symbolic; synthesis; summarization; neuro symbolic; neuro; rag; prompting; challenging large; language existing
176-IND,Benchmarking and Mitigating the Impact of Noisy User Prompts in Medical VLMs via Cross-Modal Reflection,"Medical vision-language models (Med-VLMs) offer a new and effective paradigm for digital health in tasks such as disease diagnosis using clinical images and text. In these tasks, an important but underexplored research question is how Med-VLMs interpret and respond to user-provided clinical information, especially when the prompts are noisy. For a systematic evaluation, we construct Med-CP, a large-scale visual question answering (VQA) benchmark designed to comprehensively evaluate the influence of clinical prompts across diverse modalities, anatomical regions, and diagnostic tasks. Our experiments reveal that existing Med-VLMs tend to follow user-provided prompts blindly, regardless of whether they are accurate or not, raising concerns about their reliability in real-world interactions. To address this problem, we introduce a novel supervised fine-tuning (SFT) approach for Med-VLMs based on cross-modal reflection chain-of-thought (CoT) across medical images and text. In our SFT method, the Med-VLM is trained to produce reasoning paths for the analysis of the medical image and the user-provided prompt. Then, the final answer is determined by conducting a reflection on the visual and textual information. Experimental results demonstrate that our method considerably enhances the robustness against noisy user-provided prompts for both in-domain and out-of-domain evaluation scenarios.","Zhiyu Xue, Reza Abbasi-Asl, Ramtin Pedarsani",Zhiyu Xue,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",med; med vlms; user provided; vlms; provided; prompts; medical; user; reflection; noisy
180-IND,Lightweight Domain-Specific Language Model for Real-Time Structuring of Medical Prescriptions,"Automated structuring of medical prescriptions is critical for downstream safety checks in pharmacies, yet remains challenging due to heterogeneous layouts, OCR noise, and dense clinical abbreviations in real-world documents. Existing language models either ignore layout information, rely on computationally expensive image-based architectures, or cannot operate under strict privacy and hardware constraints such as GDPR and HDS-certified environments. We present a lightweight (<10M parameters), privacy-preserving transformer specifically designed for Entity Extraction (EE) and Entity Linking (EL) in French medical prescriptions. The model uses only OCR text and normalized 2D word coordinates, enabling robust pseudonymisation and real-time CPU-level inference while preserving essential spatial cues. It is pretrained on a large corpus of pseudonymised OCR outputs using objectives tailored to prescription structure, including a novel Token-to-Line Alignment (TLA) task, and fine-tuned on the Rx-PAD dataset (Pattin Cottet et al., 2025). Empirical results show that our approach matches or surpasses larger document-understanding models and rivals multimodal LLMs on strict extraction metrics, while achieving sub-second latency suitable for operational deployment. The system is currently used in 230 pharmacies, demonstrating both scalability and practical relevance. These findings highlight the importance of specialized, domain-aware, lightweight models for safe, efficient, and legally compliant prescription verification.","Jonathan Pattin Cottet, Véronique Eglin, Alex Aussem",Jonathan Pattin Cottet,Oral,In-person,SALLE Le Chellah,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,"Trustworthy, Safety, Privacy & Fairness",Domain NLP (Biomedical/Clinical/Legal/Scientific),ocr; medical; lightweight; strict; real time; privacy; real; extraction; entity; preserving
182-IND,Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden’s J statistic,"Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden’s $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.","Stephane Collot, Colin Fraser, Justin Zhao, William F. Shen, Timon Willi, Ilias Leontiadis",Stéphane Collot,Find-Presentation,Virtual,ZOOM,Virtual TBA,,,"LLM Evaluation, Benchmarks & Metrics","Trustworthy, Safety, Privacy & Fairness",balanced accuracy; prevalence; classifier; judges; balanced; estimates; accuracy; class; choice; judge
184-IND,PharmaQA.IT: an Italian dataset for Q\&A in the pharmaceutical domain,"The growing use of Large Language Models (LLMs) for medical Question Answering (QA) requires reliable, evidence-grounded benchmarks beyond English. In Italy, Riassunti delle Caratteristiche del Prodotto (RCP) issued by the Italian Medicines Agency (AIFA) are the main regulatory source on medicines, yet no QA dataset exists on these documents, limiting the development and evaluation of trustworthy Italian QA systems. We introduce \textbf{PharmaQA.IT}, an Italian extractive QA dataset built from RCPs in PharmaER.IT. Using a semi-automatic pipeline, we (i) select informative pages from 1{,}077 leaflets, (ii) prompt a multimodal LLM on page images with professional personas to generate candidate question–answer pairs, and (iii) validate and normalise them with expert revision. The final dataset contains 861 high-quality question–answer pairs on indications, contraindications, dosage, warnings, interactions, and pharmacological properties. We frame PharmaQA.IT as an extractive QA benchmark with structured JSON outputs and evaluate a range of open and proprietary LLMs. Results show that open models approach closed-source performance under a chunking-and-retrieval setup. PharmaQA.IT, together with all code, prompts, and evaluation scripts, will be publicly released to support research on trustworthy Italian biomedical QA. PharmaQA.IT, together with all code, prompts, and evaluation scripts, is publicly \href{https://huggingface.co/datasets/VillanovaAI/PharmaQA.IT}{available on Hugging Face} to support research on trustworthy Italian biomedical QA.","Kamyar Zeinalipour, Andrea Zugarini, Asya Zanollo, Leonardo Rigutini",Kamyar Zeinalipour,Oral,In-person,SALLE Le Chellah,Session 4: Oral/Posters C,Wed. Mar 25,16:30-18:00,Domain NLP (Biomedical/Clinical/Legal/Scientific),"LLM Evaluation, Benchmarks & Metrics",italian; trustworthy; biomedical qa; extractive qa; support research; qa dataset; evaluation scripts; extractive; question answer pairs; answer pairs
185-IND,DIRECT: Directional Relevance in Conversational Trajectories,"Conversational Agents have become ubiquitous across application domains, such as, shopping assistants, medical diagnosis, autonomous task planning etc. Users interacting with these agents often fail to understand how to start a conversation or what to ask next to obtain the desired information. To enable seamless and hassle-free user-agent interactions, we introduce Next Question Suggestions (NQS), which are essentially highly relevant follow-up question recommendations that act as conversation starters or discover-ability tools to capture non-trivial user intents, leading to more engaging conversations. Relying on LLMs for both response as well as NQS generation is a costly ask in latency-constrained commercial settings, with an added risk of handling potentially unsafe or unanswerable generated queries. A key component of building an efficient low-latency NQS experience is, therefore, retrieval (or embedding) models that fetch the most-relevant candidate questions from an offline pre-curated Question Bank (QB). Off-the-shelf embedding models cannot capture domain-specific nuances and more importantly the directionality inherent in follow-up question recommendations. In this work, we propose an end-to-end retrieval system, DIRECT that is optimized to model directional relevance. Given a user query, it produces a ranked list of highly relevant follow-up question recommendations within 1 sec. Our system also contains an LLM-as-a-judge component, tuned on proprietary user-agent interaction logs, to evaluate the end-to-end performance in terms of CTR.","Rajdeep Mukherjee, Anshuman Mourya, Prerna Jolly, Vinayak S Puranik, Sivaramakrishnan R Kaveri",Anshuman Mourya,Oral,In-person,SALLE Le Chellah,Session 2: Oral/Posters A,Wed. Mar 25,11:30-13:00,"Dialogue, Conversational & Interactive NLP","Reasoning, Planning & Agents",follow question; follow; question; recommendations; end; user; highly relevant; user agent; directional; relevant
9-SRW,Voice Identification of 1960s Tamil Singers Using Transfer Learning for Preserving Cultural Heritage,Need,Sathiyakugan Balakrishnan; Uthayasanker Thayasivam,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multilinguality & Low-Resource NLP,"Trustworthy, Safety, Privacy & Fairness",cultural heritage; heritage; transfer learning; voice; identification; cultural; transfer; preserving; learning; need
13-SRW,Thesis Proposal: Efficient KV Cache Reuse for Multi-Document Retrieval-Augmented Generation,Need,Zhipeng Zhang; Dmitry Ilvovsky,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,augmented generation need; generation need; efficient kv; document retrieval; multi document; reuse; cache; kv cache; proposal; thesis proposal
14-SRW,Thesis proposal: COGNILENS: Analyzing Cognitive Decline in Language Models for Alzheimer's Monitoring,Need,Jonathan Guerne,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,,monitoring; thesis proposal; proposal; thesis; analyzing; cognitive; need; language models; language
17-SRW,Beyond One-Step Distillation: Bridging the Capacity Gap in Small Language Models via Multi-Step Knowledge Transfer,Need,Gaeun Yim; Nayoung Ko; Manasa Bharadwaj,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Reasoning, Planning & Agents","Efficiency, Scaling & NLP Systems",step; gap small language; gap small; transfer need; language models multi; knowledge transfer; models multi; bridging; small language models; small language
19-SRW,Thesis proposal: Are We Losing Textual Diversity to Natural Language Processing?,Need,Josef Jon; Ondřej Bojar,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,,thesis proposal; proposal; thesis; natural language processing; language processing; diversity; textual; processing; natural language; natural
21-SRW,Construction of an Evaluation Dataset for Hallucination Detection in Japanese Summarization Task,Need,Hikari Tanaka; Atsushi Keyaki; Mamoru Komachi,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Summarization & Generation,"LLM Evaluation, Benchmarks & Metrics",evaluation dataset; hallucination detection; japanese; construction; hallucination; summarization; detection; need; dataset; evaluation
23-SRW,Comparing Text Compression Capabilities of Large Language Models with Traditional Compression Algorithms,Need,Mehran Haddadi; William John Teahan,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems",,compression; capabilities large language; capabilities large; algorithms; comparing; traditional; capabilities; need; text; large language models
24-SRW,Comprehensive Comparison of RAG Methods Across Multi-Domain Conversational QA,Need,Klejda Alushi; Jan Strich; Chris Biemann; Martin Semmann,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Dialogue, Conversational & Interactive NLP","Retrieval, Grounding & External Knowledge (RAG)",methods multi; rag methods; multi domain; comparison; conversational; rag; comprehensive; domain; need; multi
25-SRW,LEMUR: Robust Fine-Tuning for Multilingual Embedding Models for Retrieval,Need,Narges Baba Ahmadi; Jan Strich; Martin Semmann; Chris Biemann,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",Multilinguality & Low-Resource NLP,retrieval need; multilingual embedding models; multilingual embedding; embedding models; embedding; multilingual; robust; retrieval; fine tuning; tuning
28-SRW,"Trainable, Multiword-aware Tokenization Using Modern Neural Networks",Need,Clara Boesenberg; Kilian Evang,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,multiword; modern neural networks; using modern; aware tokenization; modern neural; trainable; neural networks; networks; tokenization; neural
33-SRW,"Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors",Need,Adnan Al Ali; Jindřich Helcl; Jindřich Libovický,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness",,non native speakers; revisiting; native speakers; different; non native; detectors; native; speakers; gpt; bias
35-SRW,"Call, Reward, Repeat: Advancing Dialog State Tracking with GRPO and Function Calling",Need,Timur Ionov; Anna Marshalova; Valentin Malykh,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,function calling; repeat; dialog; tracking; calling; grpo; function; advancing; reward; need
36-SRW,Generalising LLM Routing using Past Performance Retrieval: A Few-Shot Router is Sufficient,Need,Clovis Varangot-Reille; Christophe Bouvard; Antoine Gourru,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)","Efficiency, Scaling & NLP Systems",routing using; llm routing; performance retrieval; router; past; sufficient; routing; shot; retrieval; need
38-SRW,CAPID: Context-Aware PII Detection for Question-Answering Systems,Need,Mariia Ponomarenko; Sepideh Abedini; Masoumeh Shafieinejad; D. B. Emerson; Shubhankar Mohapatra; Xi He,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems",,question answering systems; answering systems; pii; context aware; question answering; answering; detection; question; aware; context
39-SRW,Exploring the Semantic Space of Second Language Learners,Need,Trisha Godara; Rui He; Wolfram Hinzen; Yan Cong,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,,learners; second language; exploring; second; space; semantic; need; language
40-SRW,Kahaani: A Multimodal Co-Creative Storytelling System,Need,Samee Arif; Taimoor Arif; Muhammad Saad Haroon; Aamina Jamal Khan; Agha Ali Raza; Awais Athar,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,,storytelling; creative; multimodal; need
42-SRW,A Benchmark and Evaluation of Automated Language of Study Extraction from Computational Linguistics Publications,Need,Ashwin Kirubakaran; Henry Gagnier,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,publications; computational linguistics; benchmark evaluation; linguistics; extraction; automated; computational; need; study; benchmark
43-SRW,Who Plays Which Role? Protagonist Detection and Classification in Moral Discourse,Need,Mirko Sommer; Maria Becker,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Linguistics, Syntax & Semantics",,detection classification; moral; plays; discourse; role; classification; detection; need
44-SRW,Thesis Proposal: Multimodal Benchmark for Music Understanding in Large Language Models,Need,Tomáš Sourada,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,"LLM Evaluation, Benchmarks & Metrics",multimodal benchmark; music; understanding large language; understanding large; language models need; thesis proposal; proposal; thesis; models need; multimodal
45-SRW,Communication as a Complex System: Modeling the Feedback Dynamics of Trust and Credibility,Need,Swaptik Chowdhury; Samuel D. Allen; Jung Hee Hyun,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Dialogue, Conversational & Interactive NLP",,credibility; trust; dynamics; communication; feedback; modeling; complex; need
46-SRW,The Clinical Fingerprint: Comparing the Rhetorical Integrity and Epistemic Safety of Human Physicians and Large Language Models,Need,Bayram Ayadi,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Trustworthy, Safety, Privacy & Fairness",fingerprint; epistemic; physicians; rhetorical; language models need; integrity; models need; clinical; comparing; safety
49-SRW,Acceleration of Backpropagation in Linear Layers of Transformer Models Based on Gradient Structure,Need,Dmitrii Topchii; Alexander Panchenko; Viktoriia A. Chekalina,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,acceleration; transformer models; models based; gradient; linear; transformer; layers; structure; need; based
50-SRW,Chronocept: Instilling a Sense of Time in Machines,Need,Krish Goel; Sanskar Pandey; Mahadevan KS; Harsh Kumar; Vishesh Khadaria,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,instilling; sense time; machines; sense; time; need
52-SRW,When Prompt Optimization Becomes Jailbreaking: Adaptive Red-Teaming of Large Language Models,Need,Zafir Shamsi; Nikhil Chekuru; Zachary Guzman; Shivank Garg,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness",,red teaming; teaming; language models need; prompt optimization; red; jailbreaking; models need; adaptive; optimization; prompt
53-SRW,GraphRAG-Rad: Concept-Aware Radiology Report Generation via Latent Visual-Semantic Retrieval,Need,Faezeh Safari; Hang Dong; ZEYU FU; Aline Villavicencio,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Domain NLP (Biomedical/Clinical/Legal/Scientific),"Retrieval, Grounding & External Knowledge (RAG)",retrieval need; rad; radiology report; concept aware; radiology report generation; aware radiology; report generation; radiology; semantic retrieval; graphrag
55-SRW,Token Pruning for Improving Graph-Generating State Space Model Performance,Need,Monish Beegamudre; Jack Zheng; Margaret Capetz,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems",,performance need; state space; pruning; model performance; graph; space; generating; token; improving; need
58-SRW,Scale Is All You Need 🙄: Analyzing Modality Interaction and Speaker Intent Without Fine-Tuning,Need,Animesh Gurjar; Nikhil Krishnaswamy,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,scale need; need; speaker; modality; intent; interaction; analyzing; scale; fine tuning; tuning
59-SRW,Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget,Need,Zohaib Khan; Omer Tafveez; Zoha Hayat Bhatti,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Reasoning, Planning & Agents",,low rank adapters; rank adapters; micro; adapters; low rank; budget; rank; impact; low; need
64-SRW,In-Image Machine Translation. A Preliminary Modular Approach,Need,Sergio Gomez Gonzalez; Miguel Domingo; Francisco Casacuberta,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Machine Translation,Multimodal & Speech/Audio,modular approach; preliminary; modular; machine translation; image; translation; machine; need
65-SRW,Text-to-Text Automatic Story Generation: A Survey,Need,Yuan Ma; Hanna Suominen; Patrik Haslum; Richard Susilo,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Summarization & Generation,,story generation; text text; story; text; survey; automatic; need; generation
66-SRW,Probabilistic Bilingual Subword Segmentation with Latent Subword Alignment,Need,Shoto Nishida; Daiki Matsui; Takashi Ninomiya; Isao Goto; Akihiro Tamura,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Interpretability & Model Analysis,"Trustworthy, Safety, Privacy & Fairness",subword; subword segmentation; probabilistic; segmentation; bilingual; latent; alignment; need
68-SRW,Thesis Proposal: Development of End-to-End Speech Translation Models for Indian Languages,Need,Jamaluddin,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Machine Translation,Multimodal & Speech/Audio,end; indian languages; translation models; indian; thesis proposal; proposal; thesis; end end; speech; translation
73-SRW,Towards Singable Lyrics Translation Using Large Language Models,Need,Liu Hanze; Yusuke Sakai; Taro Watanabe,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Machine Translation,,translation using; language models need; models need; using large language; using large; translation; need; using; large language models; large language
74-SRW,Evaluating the Impact of SAE-based Language Steering on LLM Performance,Need,Sebastian Zwirner; Wentao Hu; Koshiro Aoki; Daisuke Kawahara,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Interpretability & Model Analysis,,performance need; sae based; evaluating impact; steering llm; based language; sae; llm performance; steering; impact; evaluating
76-SRW,Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework,Need,Grzegorz Statkiewicz; Alicja Dobrzeniecka; Karolina Seweryn; Aleksandra Krasnodębska; Karolina Piosek; Katarzyna Bogusz; Sebastian Cygert; Wojciech Kusa,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,,language using; vision language model; polish; llava; adaptation; annotation; vision language; vision; language model; efficient
78-SRW,A Computational Forensic Linguistic Analysis of Narrative and Question-Answer Structures in Italian Police Interrogation Transcripts,Need,Romane Werner; Thomas François; Sonja Bitzer,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Interpretability & Model Analysis,"Linguistics, Syntax & Semantics",interrogation; linguistic analysis; transcripts; question answer; italian; narrative; structures; computational; answer; linguistic
80-SRW,Thesis Proposal: A Multi-Agent System for Ontology-Based Perspective-Aware Knowledge Extraction,Need,Luiz do Valle Miranda; Grzegorz J. Nalepa,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Reasoning, Planning & Agents",,perspective aware; aware knowledge; ontology; knowledge extraction; thesis proposal; proposal; thesis; perspective; multi agent; extraction
84-SRW,Fake News Detection Strategies under Dataset Bias: Using Large-scale Coarse-grained Labels,Need,Yuki Kishi; Yuji Arima; Hitoshi Iyatomi,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness",,fake news; coarse grained; bias using; strategies dataset; fake; coarse; using large; news; labels; bias
86-SRW,DRAGOn: Designing RAG On Periodically Updated Corpus,Need,Fedor Chernogorskii; Sergei Averkiev; Liliya Kudraleeva; Zaven Martirosian; Maria Tikhonova; Valentin Malykh; Alena Fenogenova,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",,dragon; updated; designing; corpus; rag; need
92-SRW,Efficient Low-Resource Language Model Using Tokenizer Transfer,Need,Gustaf Gren; Murathan Kurfali,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multilinguality & Low-Resource NLP,,language model using; transfer need; efficient low; model using; low resource language; tokenizer; resource language; transfer; low resource; language model
93-SRW,Learning Nested Named Entity Recognition from Flat Annotations,Need,Igor Rozhkov; Natalia V Loukachevitch,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Information Extraction & Structured Prediction,,nested; flat; named entity recognition; entity recognition; named entity; named; recognition; entity; annotations; learning
94-SRW,LLMs Exhibit Performative Fairness When Generating Profiles with Complex Geopolitical Identities,Need,Maida Aizaz; Quang Minh Nguyen,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness",,geopolitical; identities; llms exhibit; profiles; fairness; exhibit; generating; complex; need; llms
95-SRW,Beyond Bias Scores: Unmasking Vacuous Neutrality in Small Language Models,Need,Sumanth Manduru; Carlotta Domeniconi,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness",,unmasking; neutrality; language models need; models need; small language models; small language; scores; bias; small; need
96-SRW,From Detection to Explanation: Modeling Fine-Grained Emotional Social Influence Techniques with LLMs and Human Preferences,Need,Maciej Markiewicz; Wiktoria Mieleszczenko-Kowszewicz; Beata Bajcar; Tomasz Adamczyk; Aleksander Szczęsny; Jolanta Babiak; Przemyslaw Kazienko,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Interpretability & Model Analysis,,social influence; fine grained emotional; grained emotional; llms human preferences; llms human; emotional; human preferences; explanation; preferences; influence
97-SRW,How Do Lexical Senses Correspond Between Spoken German and German Sign Language?,Need,Melis Çelikkol; Wei Zhao,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,,german; sign; senses; correspond; spoken; lexical; need; language
100-SRW,Evaluating Cost-Efficiency of LLMs in a RAG Setup on Polish Wikipedia: Quality vs. Energy Consumption,Need,Patrycja Smits; Tomasz Walkowiak,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems","Retrieval, Grounding & External Knowledge (RAG)",cost efficiency; llms rag; polish; energy; consumption; wikipedia; setup; rag; cost; efficiency
101-SRW,Thesis Proposal: Measuring Prejudice at Scale,Need,Zoran Fijavž; Senja Pollak; Veronika Bajt,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,scale need; thesis proposal; proposal; thesis; measuring; scale; need
112-SRW,Energy Matching based Preference Learning for Diffusion Langauge Models,Need,Shiv Shankar,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,preference learning; based preference; diffusion; energy; models need; matching; preference; learning; need; based
118-SRW,"Thesis Proposal: Stability-Aware, Evidence-Grounded Knowledge Graphs for Substance Use Disorders and Social Determinants of Health",Need,Gautham Vijay Kumar,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",,aware evidence; social determinants health; determinants health; social determinants; substance; determinants; evidence grounded; substance use; disorders; stability
122-SRW,Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation,Need,Julia Belikova; Danila Rozhevskii; Dennis Svirin; Konstantin Polev; Alexander Panchenko,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Retrieval, Grounding & External Knowledge (RAG)",Summarization & Generation,augmented generation need; generation need; overflow; token representations; retrieval augmented generation; detecting; augmented generation; retrieval augmented; representations; augmented
137-SRW,$\texttt{lrnn-lib}$: A library for Linear RNNs,Need,Karan Bania; Soham Kalburgi; Manit Tanwar; Dhruthi; Aditya Nagarsekar; Harshvardhan Mestha; Naman Chibber; Raj Deshmukh; Anish Sathyanarayanan; Aarush Rathore; Pratham Chheda,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,library; texttt; linear; need
143-SRW,Automatic Generation of a Compositional QA Benchmark for Geospatial Reasoning under Spatial and Entity Constraints,Need,Tetsuhisa Suizu; Shohei Higashiyama; Hiroyuki Shindo; Hiroki Ouchi; Sakriani Sakti,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Summarization & Generation,"LLM Evaluation, Benchmarks & Metrics",geospatial reasoning; geospatial; qa benchmark; spatial; compositional; constraints; entity; automatic; need; generation
146-SRW,Thesis Proposal: Comparing Human and Model Perception of Writing Style under Controlled Perturbations,Need,Ewelina Paulina Księżniak,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Summarization & Generation,,human model; thesis proposal; proposal; thesis; writing; perception; perturbations; comparing; style; controlled
147-SRW,"Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models",Need,Andrey Moskalenko; Daria Pugacheva; Denis Shepelev; Andrey Kuznetsov; Vlad Shakhuro; Elena Tutubalina,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Dialogue, Conversational & Interactive NLP",,embodied; commands; bring; models need; irrelevant; impact; context; need
148-SRW,An Evaluation of Classifiers for Mapping Generative LLM Responses to Answer Options of Multiple-choice Questionnaires,Need,Alisea Stroligo; Anna Shamray; Julian Schelb; Andreas Spitz,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,questionnaires; options; llm responses; classifiers; mapping; multiple choice; choice; generative; responses; answer
153-SRW,Beep boop: Bot Detection as a Preprocessing Step for Polish Reddit,Need,Karmela Matyjaszek,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Reasoning, Planning & Agents",,preprocessing step; bot; reddit; polish; preprocessing; step; detection; need
157-SRW,Emergent Misalignment: Tracking the Emergence and Evolution of Misaligned traits throughout Model Training,Need,Geunwoo Park; Pranay Chauhan; Haihao Liu,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Interpretability & Model Analysis,,misaligned; emergence; emergent; traits; evolution; tracking; model training; misalignment; need; training
158-SRW,FluffInjector: Diagnosing Logical Consistency Failures in Chain-of-Thought Reward Models,Need,Varshith Vijjapu; Krishiv Ray; Archana Vaidheeswaran,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,logical consistency; diagnosing; reward models; models need; logical; failures; reward; chain thought; chain; thought
160-SRW,"What the Router Sees Matters: Funnel Pooling for Fast, Content Driven Expert Routing",Need,Josef Pichlmeier; Sebastian Nicolas Mueller; Jakob Sturm; Josef Dräxl; Andre Luckow,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems",,expert routing; pooling; router; fast; matters; routing; driven; expert; content; need
161-SRW,TimeRes: A Turkish Benchmark For Evaluating Temporal Understanding of Large Language Models,Need,Habib Yağız Demir; Susan Üsküdarlı; Ümit Atlamaz,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,turkish; understanding large language; understanding large; language models need; models need; benchmark evaluating; temporal; evaluating; understanding; need
162-SRW,Hospitality-VQA: Decision-Oriented Informativeness Evaluation for Vision–Language Models,Need,Jeongwoo Lee; Baek Duhyeong; Eungyeol Han; Soyeon Shin; Gukin han; Seungduk Kim; Jaehyun Jeon; Taewoo Jeong,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,hospitality; decision oriented; informativeness; language models need; models need; vqa; oriented; decision; vision language models; vision language
167-SRW,Colorism in Large Vision-Language Models: An Empirical Exploration of Socioeconomic Linguistic Bias,Need,Raj Gaurav Maurya; Vaibhav Shukla; Sreedath Panat,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,"Linguistics, Syntax & Semantics",socioeconomic; linguistic bias; large vision language; large vision; exploration; empirical; vision language models; vision language; bias; vision
168-SRW,Pushing the Boundaries of Multiple Choice Evaluation to One Hundred Options,Need,Nahyun Lee; Guijin Son,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,pushing; options; boundaries; multiple choice; choice; need; multiple; evaluation
169-SRW,Broken Chains: The Cost of Incomplete Reasoning in LLMs,Need,Ian Su; Gaurav Purushothaman; Jey Narayan; Ruhika Goel; Kevin Zhu; Sunishchal Dev; Yash More; Maheep Chaudhary,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Reasoning, Planning & Agents",,llms need; broken; incomplete; chains; reasoning llms; cost; need; reasoning; llms
171-SRW,You Didn't Have to Say It Like That: Subliminal Learning from Faithful Paraphrases,Need,Isaia Gisler; Zhonghao He; Tianyi Alex Qiu,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"LLM Evaluation, Benchmarks & Metrics",,subliminal learning; subliminal; say; paraphrases; faithful; like; learning; need
172-SRW,Active Learning for Corpus Refinement: Cost-Effective Preprocessing to Improve Validity of Applied Quantitative Text Analysis,Need,Jakob Steglich; Stephan Poppe,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Interpretability & Model Analysis,,active learning; preprocessing; cost effective; active; validity; quantitative; refinement; applied; corpus; cost
176-SRW,"Understanding Subliminal Learning: Generality, Sensitivity, and Token-Level Explanations",Need,Yagnesh Veeraraghavan; Keanu Lim; Jacob Lipner; Saanvi Ibrahimpatnam; Kevin Zhu; Madhur Panwar,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Efficiency, Scaling & NLP Systems",,subliminal learning; subliminal; generality; token level; explanations; sensitivity; token; understanding; level; learning
178-SRW,From Sentences to Proof Trees: Leveraging Language Models for Structured Reasoning,Need,Aayushee Gupta,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Reasoning, Planning & Agents",,reasoning need; trees; proof; language models structured; models structured; structured reasoning; sentences; leveraging; structured; need
1002-SRW,Mask What Matters: Mitigating Object Hallucinations in Large Vision–Language Models with Object-Aligned Visual Contrastive Decoding,Need,Boqi Chen; Xudong Liu; Jianing Qiu,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,,object; visual contrastive decoding; contrastive decoding; visual contrastive; object hallucinations; mask; matters; large vision; large vision language; mitigating
1004-SRW,Domain Adaptation of Image Encoder for Multimodal Manga Translation,Need,Kota Manabe; Tomoyuki Kajiwara; Takashi Ninomiya; Isao Goto; Shonosuke Ishiwatari; Hiroshi Noji,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multimodal & Speech/Audio,Machine Translation,multimodal manga; manga; domain adaptation; adaptation; encoder; image; translation; multimodal; domain; need
1006-SRW,Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning,Need,Mahbub E Sobhani; Md. Faiyaz Abdullah Sayeedi; Mohammad Nehad Alam; Proma Hossain Progga; Swakkhar Shatabda,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Reasoning, Planning & Agents",,reasoning need; problem solving reasoning; solving reasoning; evaluating agentic; geometry problem solving; geometry problem; diagram; geometry; solve; problem solving
1009-SRW,PATCH Dataset: Empowering Traditional Chinese Safety Classifiers for Lightweight LLM,Need,Chi-Wei Chang; Chiung-Jui Chen; Richard Tzong-Han Tsai,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness",,traditional chinese; lightweight llm; patch; classifiers; chinese; lightweight; traditional; safety; need; dataset
1010-SRW,Luth: Efficient French Specialization for Small Language Models and Cross-Lingual Transfer,Need,Sinoué GAD; Maxence Lasbordes,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multilinguality & Low-Resource NLP,,models cross lingual; transfer need; models cross; cross lingual transfer; lingual transfer; specialization; french; small language models; small language; cross lingual
1012-SRW,Machine Translation for Low-Resource Languages through Monolingual Data and LLM: A Case Study of English-to-Basque,Need,Nam Luu; Aitor Soroa; German Rigau; Ondřej Bojar,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Machine Translation,Multilinguality & Low-Resource NLP,data llm; translation low resource; machine translation low; translation low; languages monolingual; monolingual data; case study english; study english; monolingual; case study
1013-SRW,"Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety",Need,Denis Janiak; Julia Moska; Dawid Motyka; Karolina Seweryn; Paweł Walkowiak; Bartosz Żuk; Arkadiusz Janz,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Trustworthy, Safety, Privacy & Fairness","LLM Evaluation, Benchmarks & Metrics",alignment methods; rethinking; diversity; insights; safety; alignment; need; methods; evaluation
1014-SRW,Modality Matching Matters: Calibrating Language Distances for Cross-Lingual Transfer in URIEL+,Need,York Hay Ng; Aditya Khan; Xiang Lu; Matteo Salloum; Michael Zhou; Phuong Hanh Hoang; A. Seza Doğruöz; En-Shiun Annie Lee,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,Multilinguality & Low-Resource NLP,,distances; calibrating; cross lingual transfer; lingual transfer; matters; modality; matching; cross lingual; lingual; transfer
1015-SRW,What Persona Are We Missing? Identifying Unknown Relevant Personas for Faithful User Simulation,Need,Weiwen SU; Yuhan Zhou; Zihan Wang; Naoki Yoshinaga; Masashi Toyoda,Need,Poster,Unknown,POSTER HALL,Session 6: Oral/Posters E,Thur. Mar 26,11:00-12:30,"Dialogue, Conversational & Interactive NLP",,user simulation; unknown; personas; missing; persona; simulation; faithful; identifying; relevant; user
